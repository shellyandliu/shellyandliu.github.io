<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="会飞的猪">





<link rel="stylesheet" href="/fonts/iconfont_new/iconfont.css">


<title>Lasso回归 | 会飞的猪</title>



    <link rel="icon" href="/image/longmao.jpg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">会飞的猪</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/categories">分类</a>
                
                    <a class="menu-item" href="/tags">标签</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">会飞的猪</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/categories">分类</a>
                
                    <a class="menu-item" href="/tags">标签</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">展开目录</a>
        <a onclick="go_top()">回到顶部</a>
        <a onclick="go_bottom()">到达底部</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Lasso回归</h1>
            
                <div class="post-meta">
                    
                        作者: <a itemprop="author" rel="author" href="/">会飞的猪</a>
                    

                    
                        <span class="post-time">
                        时间: <a href="#">四月 16, 2020&nbsp;&nbsp;17:54:43</a>
                        </span>
                    
                    
                        <span class="post-category">
                    类别:
                            
                                <a href="/categories/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">-线性回归</a>
                            
                        </span>
                    
                    
                        <span class="post-time">
                            <span class="post-meta-item-icon">
                              <i class="fa fa-keyboard-o"></i>
                              <span class="post-meta-item-text">  字数统计: </span>
                              <span class="post-count"><a href="#">2.2k字</a></span>
                            </span>
                        </span>
                        <span class="post-time">
                            <span class="post-meta-item-icon">
                            <i class="fa fa-hourglass-half"></i>
                            <span class="post-meta-item-text">  阅读时长: </span>
                            <span class="post-count"><a href="#">11分</a></span>
                            </span>
                        </span>
                    

                    
                </div>
            
        </header>

        <div class="post-content">
            <h2 id="Lasso-回归"><a href="#Lasso-回归" class="headerlink" title="Lasso 回归"></a>Lasso 回归</h2><p>LASSO回归相当于将岭回归的2范数换成了1范数，用拉格朗日对偶形式表示为：</p>
<blockquote>
<p>$\min _{\beta}\left\{\frac{1}{2 N}\left|y-X \beta\right|_{1}\right\},|\beta|_{1} \leq t$</p>
</blockquote>
<p>得到的解称为<strong>LASSO估计</strong>。对应的原始的优化形式中的 $\lambda$ 称为<strong>收缩算子</strong>。</p>
<p>相较于岭回归，LASSO回归的优势在于，它能够使有些变量迅速的趋于0，因此可以用作<strong>变量选择</strong>，下面作解释：</p>
<p>设 $\hat{\beta}$ 为普通最小二乘得到的参数估计,则</p>
<blockquote>
<p>$L(\beta)=\frac 1 {2N}|Y-X\beta|_2^2+\lambda|\beta|_1=\frac 1 {2N}|Y-X\hat{\beta}+X\hat{\beta}-X\beta|_2^2+\lambda|\beta|_1 =L(\hat{\beta})+\frac 1{2N}|X(\hat{\beta}-\beta)|_2^2+|\lambda|_1$</p>
<p>$=L(\hat{\beta})+\frac 1{2N}(\hat{\beta}-\beta)’X’X(\hat{\beta}-\beta)+\lambda|\beta|_1$</p>
</blockquote>
<p>则 $\min_{\beta}L(\beta)=\min_{\beta}\{\frac 1{2N}\}(\hat{\beta}-\beta)’X’X(\hat{\beta}-\beta)+\lambda|\beta|_1$，不妨设$X’X$为对角矩阵 $diag(\lambda_1,\dots,\lambda_p)$，可以通过主成分分析得到，则</p>
<blockquote>
<p>$\min_{\beta}L(\beta)=\min_{\beta_i}\frac1 {2N}\sum_{i=1}^p(\lambda_i(\hat{\beta}_i-\beta_i)^2+\lambda|\beta_i|)$</p>
</blockquote>
<p>则可以解的：$\beta_{i}=\operatorname{sign}\left(\hat{\beta}_{i}\right) \max \left\{\left|\hat{\beta}_{i}\right|-\frac{\lambda}{\lambda_i}, 0\right\}$</p>
<p>当 $\hat{\beta}_i &gt;0 $ 时：</p>
<ol>
<li><p>当 $\hat{\beta}_i\le \frac{\lambda}{\lambda_i}$，此时目标中 $\beta_i$ 的最优值为 $\beta_i=0$</p>
</li>
<li><p>$\hat{\beta}_i\ge \frac{\lambda}{\lambda_i}$，则正则化不会讲 $\beta_i$ 的最优值推向 $0$,而仅移动 $\frac{\lambda}{\lambda_i}$ 的距离。</p>
</li>
</ol>
<p>当 $\hat{\beta}_i &lt;0 $ 时与之类似。</p>
<p>在 $X’X$ 为对角矩阵时，可以通过上述方法得到闭式解。但由于一般 $(X’X)^{-1}$ 过于复杂，一般并不采用这种方法，由于$L-1$ 范数并不可微，因此传统的梯度下降法也将失效，作为替代一般采用以下算法。</p>
<h2 id="求解Lasso回归的数值算法"><a href="#求解Lasso回归的数值算法" class="headerlink" title="求解Lasso回归的数值算法"></a>求解Lasso回归的数值算法</h2><h3 id="坐标轴下降法求解"><a href="#坐标轴下降法求解" class="headerlink" title="坐标轴下降法求解"></a>坐标轴下降法求解</h3><h4 id="次梯度"><a href="#次梯度" class="headerlink" title="次梯度"></a>次梯度</h4><p>梯度是一个函数增长最快的方向，但是当函数在某点不可导时梯度就没法计算了，作为推广引入次梯度：</p>
<blockquote>
<p>定义：函数 $f$ 在某点 $x_0$ 的次梯度 $\nabla g,$ 满足：</p>
<p>$f(x)-f(x_0)\ge \nabla g^T(x-x_0)$, 对任意 $x$ 成立。</p>
</blockquote>
<p>次梯度不能保证是增长最快的方向，甚至有可能是降低的方向，故形成了一个不等式。并且次梯度不是一个确定的值，而是一个取值范围内中的一个点。如果在该点是可导的，则次梯度就是梯度。</p>
<p>若设 $\partial f(x)$ 为 $f$ 在点 $x$ 处的次微分，则由定义可知 $g(x^{(k)})\in \partial f(x^{(k)})$</p>
<p>次梯度方法的迭代更新形式为：</p>
<blockquote>
<p>$x^{k+1}=x^{k}-\alpha_k\nabla g(x^kk)$</p>
</blockquote>
<p>由于次梯度方向并不一定总是函数值下降的方向，因此很自然的在每一次迭代后，都取曾今最小的函数值：</p>
<blockquote>
<p>$f^{(k)}_{best}=\min\{f^{k-1}_{best},f(x^{(k)}\}$</p>
</blockquote>
<p>下面给出几种常用的步长选择方式：</p>
<blockquote>
<p>(1).  <em>Constant step size:</em> $\alpha_k=\alpha$ 是一个正常数。</p>
<p>(2).  <em>Constant step length:</em> $\alpha_k=\frac{\gamma}{|g^{(k)}|_2}$，其中 $\gamma&gt;0$ ，这就意味着 $|x^{(k+1)}-x^{(k)}|_2=\gamma$</p>
<p>(3).  <em>Square summable but not summable:</em>  $\alpha_k\ge0, \quad\sum_{k=1}^{\infty}\alpha_k^2&lt;\infty,\quad \sum_{k=1}^{\infty}\alpha_k=\infty$,一个典型的例子是 $\alpha_k=a/(b+k)$, 其中 $a&gt;0,b\ge0$</p>
<p>(4).  <em>diminishing step size rules:</em> $\alpha_k\ge0,\quad\lim_{k\to\infty}\alpha_k=0,\quad\sum_{k=1}^{\infty}\alpha_k=\infty$, 一个典型的例子是 $\alpha_k=a/\sqrt{k}$, 其中 $a&gt;0$.</p>
<p>(5).  <em>Nonsummable diminishing step lengths:</em> $\alpha_k=\frac{\gamma_k}{|g^{(k)}|_2}$, 其中 $\gamma_k\ge0,\quad\lim_{k\to\infty}\gamma_k=0,\quad\sum_{k=1}^{\infty}\gamma_k=\infty$.</p>
</blockquote>
<p>设 $x^{*}$ 为 $f$ 的极小值点；设迭代的初始点 $x^{(1)}$ 与极小值点 $x^{*}$ 之间满足：$|x^{(1)}-x^{*}|_2\le R$,  $R$ 为常数；设 $f$ 满足 Lipschitz 条件：</p>
<blockquote>
<p>对任意的 $u,v$ , 有 $|f(u)-f(v)|\le G|u-v|_2$</p>
</blockquote>
<p>下面给出上述步长的收敛性结果：</p>
<blockquote>
<p>定理1：<em>Constant step size:</em> $f_{\mathrm{best}}^{(k)}-f^{*} \leq \frac{R^{2}+G^{2} \alpha^{2} k}{2 \alpha k}$, 当 $k\to\infty$ 时, 收敛于 $G^2\alpha/2$.</p>
<p>定理2：<em>Constant step length:</em> $f_{\mathrm{best}}^{(k)}-f^{*} \leq \frac{R^{2}+G^{2} \alpha^{2} k}{2 \sum_{i=1}^k\alpha_i}\le\frac{R^2+\gamma^2k}{2\gamma k/G}$, 当 $k\to\infty$ 时,收敛于 $G\gamma/2$</p>
<p>定理3：<em>Square summable but not summable:</em>  $f_{\mathrm{best}}^{(k)}-f^{*} \leq \frac{R^{2}+G^{2} \sum_{k=1}^{\infty}\alpha_k^2}{2 \sum_{i=1}^k\alpha_i}$，当 $k\to\infty$ 时,收敛于 $0$.</p>
<p>定理4：<em>diminishing step size rules:</em>  $f_{\mathrm{best}}^{(k)}-f^{*} \leq \frac{R^{2}+G^{2} \sum_{i=1}^{k}\alpha_k^2}{2 \sum_{i=1}^k\alpha_i}$, 当 $k\to\infty$ 时,收敛于 $0$.</p>
<p>定理5：<em>Nonsummable diminishing step lengths:</em>  $f_{\mathrm{best}}^{(k)}-f^{*} \leq \frac{R^{2}+ \sum_{i=1}^{k}\gamma_k^2}{2 \sum_{i=1}^k\alpha_i}\le \frac{R^{2}+ \sum_{i=1}^{k}\gamma_k^2}{(2/G) \sum_{i=1}^k\gamma_i}$, 当 $k\to\infty$ 时,收敛于 $0$.</p>
</blockquote>
<p>下面给出证明：</p>
<p>由于 $f$ 是 Lipschitz 连续的，因此有 $|g|_2\le G$ , 则有：</p>
<script type="math/tex; mode=display">
\begin{aligned}\left\|x^{(k+1)}-x^{\star}\right\|_{2}^{2} &=\left\|x^{(k)}-\alpha_{k} g^{(k)}-x^{\star}\right\|_{2}^{2} \\ &=\left\|x^{(k)}-x^{\star}\right\|_{2}^{2}-2 \alpha_{k} g^{(k) T}\left(x^{(k)}-x^{\star}\right)+\alpha_{k}^{2}\left\|g^{(k)}\right\|_{2}^{2} \\ & \leq\left\|x^{(k)}-x^{\star}\right\|_{2}^{2}-2 \alpha_{k}\left(f\left(x^{(k)}\right)-f^{\star}\right)+\alpha_{k}^{2}\left\|g^{(k)}\right\|_{2}^{2} \end{aligned}</script><p>因此有：</p>
<blockquote>
<p>$\left|x^{(k+1)}-x^{\star}\right|_{2}^{2} \leq\left|x^{(1)}-x^{\star}\right|_{2}^{2}-2 \sum_{i=1}^{k} \alpha_{i}\left(f\left(x^{(i)}\right)-f^{\star}\right)+\sum_{i=1}^{k} \alpha_{i}^{2}\left|g^{(i)}\right|_{2}^{2}$</p>
</blockquote>
<p>由于 $|x^{(1)}-x^{*}|_2\le R$, $|x^{(k+1)}-x^{*}|_2^2\ge0$,则有</p>
<blockquote>
<p>$2\sum_{i=1}^{k} \alpha_{i}\left(f\left(x^{(i)}\right)-f^{\star}\right)\le R^2+\sum_{i=1}^{k} \alpha_{i}^{2}\left|g^{(i)}\right|_{2}^{2}$</p>
</blockquote>
<p>又因为 $\sum_{i=1}^{k} \alpha_{i}\left(f\left(x^{(i)}\right)-f^{\star}\right) \geq\left(\sum_{i=1}^{k} \alpha_{i}\right) \min _{i=1, \ldots, k}\left(f\left(x^{(i)}\right)-f^{\star}\right)=\left(\sum_{i=1}^{k} \alpha_{i}\right)\left(f_{\mathrm{best}}^{(k)}-f^{\star}\right)$:</p>
<p>因此：</p>
<blockquote>
<p>$f_{\text {best }}^{(k)}-f^{\star}=\min _{i=1, \ldots, k} f\left(x^{(i)}\right)-f^{\star} \leq \frac{R^{2}+\sum_{i=1}^{k} \alpha_{i}^{2}\left|g^{(i)}\right|_{2}^{2}}{2 \sum_{i=1}^{k} \alpha_{i}}\le\frac{R^{2}+G^2\sum_{i=1}^{k} \alpha_{i}^{2}}{2 \sum_{i=1}^{k} \alpha_{i}}$</p>
</blockquote>
<p>所以定理1，2，3，5显然可得.</p>
<p>定理4：对于任意的 $\epsilon&gt;0$,存在整数 $N_1$使得 $\alpha_i\le \epsilon/G^2$, 对任意的 $i&gt;N_1$ 成立。同样的也存在整数 $N_2$， 使得 $\sum_{i=1}^{N_2}\ge\frac 1{\epsilon}\left(R^2+G^2\sum_{i=1}^{N_1}\alpha_i^2\right)$, 由于 $\sum_{i=1}^{\infty}\alpha_i=\infty,$ 令 $N=\max\{N_1,N_2\}$，则对于 $k&gt;N$, 我们有:</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned} \frac{R^{2}+G^{2} \sum_{i=1}^{k} \alpha_{i}^{2}}{2 \sum_{i=1}^{k} \alpha_{i}} & \leq \frac{R^{2}+G^{2} \sum_{i=1}^{N_{1}} \alpha_{i}^{2}}{2 \sum_{i=1}^{k} \alpha_{i}}+\frac{G^{2} \sum_{i=N_{1}+1}^{k} \alpha_{i}^{2}}{2 \sum_{i=1}^{N_{1}} \alpha_{i}+2 \sum_{i=N_{1}+1}^{k} \alpha_{i}} \\ & \leq \frac{R^{2}+G^{2} \sum_{i=1}^{N_{1}} \alpha_{i}^{2}}{(2 / \epsilon)\left(R^{2}+G^{2} \sum_{i=1}^{N_{1}} \alpha_{i}^{2}\right)}+\frac{G^{2} \sum_{i=N_{1}+1}^{k}\left(\epsilon \alpha_{i} / G^{2}\right)}{2 \sum_{i=N_{1}+1}^{k} \alpha_{i}} \\ &=\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon \end{aligned}</script></blockquote>
<h4 id="算法与收敛性"><a href="#算法与收敛性" class="headerlink" title="算法与收敛性"></a>算法与收敛性</h4><p>坐标轴下降法是一种迭代算法，在每个坐标轴上搜索函数的最小值，不断迭代得到收敛值。</p>
<p>设损失函数为 $J(\theta)$，其中 $\theta$ 是 $p$ 维向量。则坐标轴下降法的步骤为：</p>
<blockquote>
<p>(1).  初始位置点 $\theta^{(0)}=(\theta_1^{(0)},\theta_2^{(0)},\dots,\theta_p^{(0)})$</p>
<p>(2).  第 $k$ 次迭代，从 $\theta_1^{(k)}$ 开始，固定后面的 $p-1$ 个参数，计算使 $J(\theta_1|\theta_2^{(k-1)},\dots,\theta_p^{(k-1)})$ 达到最小的 $\theta_1$, 记为 $\theta_1^{(k)}$，然后依次往后计算，则一共执行 $p$ 次运算：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\theta_{1}^{(k)}=\arg \min _{\theta_{1}} J\left(\theta_{1}| \theta_{2}^{(k-1)}, \theta_{3}^{(k-1)}, \ldots, \theta_{p}^{(k-1)}\right)\\
&\theta_{2}^{(k)}=\arg \min _{\theta_{2}} J\left(\theta_{2}|\theta_{1}^{(k)}, \theta_{3}^{(k-1)}, \ldots, \theta_{p}^{(k-1)}\right)\\
&\theta_{3}^{(k)}=\arg \min _{\theta_{3}} J\left(\theta_{3}|\theta_{1}^{(k)}, \theta_{2}^{(k)}, \ldots, \theta_{p}^{(k-1)}\right)\\
&\quad\ \ \ \cdots\\
&\theta_{p}^{(k)}=\arg \min _{\theta_{p}} J\left( \theta_{p}|\theta_{1}^{(k)}, \theta_{2}^{(k)}, \theta_{3}^{(k)}, \ldots\theta_{p-1}^{(k)}\right)
\end{aligned}</script><p>(3).  不断迭代，直到所有的 $\theta_i$ 的变化都小于预先设定的阈值 $\epsilon$.</p>
</blockquote>
<p>收敛性结果：见文献 [7]</p>
<h4 id="坐标轴下降法求解-Lasso-回归"><a href="#坐标轴下降法求解-Lasso-回归" class="headerlink" title="坐标轴下降法求解 Lasso 回归"></a>坐标轴下降法求解 Lasso 回归</h4><p>Lasso回归的损失函数为：</p>
<blockquote>
<p>$L(\beta)=\frac{1}{2n}\left(|y-X\beta|_2^2+\lambda|\beta|_1\right)=\frac{1}{2n}\left(\sum_{i=1}^n(y_i-x_i\beta)^2+\sum_{j=1}^p|\beta_j|\right)$</p>
<p>$=\frac{1}{2n}\left(\sum_{i=1}^n(y_i-\sum_{j=1}^px_ij\beta_j)^2+\sum_{j=1}^p|\beta_j|\right)$</p>
</blockquote>
<p>因此对第 $k$ 个变量做优化，求偏导得：</p>
<script type="math/tex; mode=display">
\begin{aligned}\frac{\partial L}{\partial \beta_{k}}&=\sum_{i=1}^{n}-2 x_{i k}\left(y_{i}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)+\left\{\begin{array}{cc}\lambda & \beta_{k}>0 \\{[-\lambda, \lambda]} & \beta_{k}=0 \\-\lambda & \beta_{k}<0
\end{array}\right.\\&=\sum_{i=1}^{n}-2 x_{i k}\left(y_{i}-\sum_{j \neq k}^{p} x_{i j} \beta_{j}-x_{i k} \beta_{k}\right)+\left\{\begin{array}{cc}\lambda & \beta_{k}>0 \\{[-\lambda, \lambda]} & \beta_{k}=0 \\-\lambda & \beta_{k}<0
\end{array}\right.  \\&=-2 \sum_{i=1}^{n} x_{i k}\left(y_{i}-\sum_{j \neq k}^{p} x_{i j} \beta_{j}\right)+2 \beta_{k} \sum_{i=1}^{n} x_{i k}^{2}+ \left\{\begin{array}{cc}\lambda & \beta_{k}>0 \\{[-\lambda, \lambda]} & \beta_{k}=0 \\-\lambda & \beta_{k}<0
\end{array}\right.\\&=-p_{k}+m_{k} \beta_{k}+\left\{\begin{array}{cc}\lambda & \beta_{k}>0 \\{[-\lambda, \lambda]} & \beta_{k}=0 \\-\lambda & \beta_{k}<0
\end{array}\right.\end{aligned}</script><p>令其偏导为 $0$, 可以得到:</p>
<blockquote>
<script type="math/tex; mode=display">
\beta_{k}^{*}=\left\{\begin{array}{ll}-\frac{1}{m_{k}}\left(p_{k}-\lambda\right) & p_{k}>\lambda \\ -\frac{1}{m_{k}}\left(p_{k}+\lambda\right) & p_{k}<-\lambda \\ 0 & -\lambda \leq p_{k} \leq \lambda\end{array}\right.</script></blockquote>
<h3 id="近端梯度下降法"><a href="#近端梯度下降法" class="headerlink" title="近端梯度下降法"></a>近端梯度下降法</h3><h4 id="算法与收敛性-1"><a href="#算法与收敛性-1" class="headerlink" title="算法与收敛性"></a>算法与收敛性</h4><p>设 $f(x)$ 表示为： $f(x)=g(x)+h(x)$, 其中 $g(x)$ 是一个凸的可微函数， $h(x)$ 是一个凸得非可微函数</p>
<p>首先，若 $f(x)$ 可微时，梯度下降公式 $x^{(k)}=x^{(k-1)}-t\nabla f(x^{(k-1)}) $ 可以通过下列公式导出:</p>
<blockquote>
<p>$x^{k}=\underset{x}{\arg \min } f\left(x^{k-1}\right)+\nabla f\left(x^{k-1}\right)^{\top}\left(x-x^{k-1}\right)+\frac{1}{2 t}\left|x-x^{k-1}\right|_{2}^{2}$</p>
</blockquote>
<p>现在 $h(x)$ 不可微 ，仍按照上述方法，可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned} x^{k} &=\underset{x}{\arg \min } g\left(x^{k-1}\right)+\nabla g\left(x^{k-1}\right)^{\top}\left(x-x^{k-1}\right)+\frac{1}{2 t}\left\|x-x^{k-1}\right\|_{2}^{2}+h(x) \\ &=\underset{x}{\arg \min } \frac{1}{2 t}\left\|x-\left(x^{k-1}-t \nabla g\left(x^{k-1}\right)\right)\right\|_{2}^{2}+h(x) \end{aligned}</script></blockquote>
<p>定义近端算子：</p>
<blockquote>
<p>$prox_{h,t}(x)=\underset{z}{\arg \min } \frac{1}{2 t}\left|x-z\right|_{2}^{2}+h(z)$</p>
</blockquote>
<p>所以迭代关系式为 $x^{(k)}=prox_{h,t}(x^{(k-1)}-t\nabla g(x^{(k-1)}) )$ .</p>
<p>下面证明迭代得收敛性：</p>
<p>设 $f$ 关于初始点 $x^{(0)}$ 的下水平集为 $S=\{x\in dom f|f(x)\le f(x^{(0)})$, 则 $S$ 为闭集。定义 $G_t(x)=\frac 1t (x-prox_{h,t}(x-t\nabla g(x)))$。</p>
<p>由于 $g(x)$ 是凸函数，则存在常数 $M$，使得 $\nabla^2f(x)\le MI$, 所以有 :</p>
<blockquote>
<p>$g(y)\le g(x)+\nabla g(x)^T(y-x)+\frac M2|y-x|^2_2$, 对任意 $x,y$ 成立。</p>
</blockquote>
<p>令 $y=x-tG_t(x)$, 且 $0&lt;t\le1/$M, 有 $g(x-tG_t(x))\le g(x)+t\nabla g(x)^TG_t(x)+\frac t2|G_t(x)|^2_2$</p>
<p>利用函数 $g,h$ 的凸性，即 $g(z)-g(x)\ge\nabla g(x)^T(z-x),h(z)-h(x)\ge \partial h(x)^T(z-x)$, 所以可以得到：</p>
<blockquote>
<p>$f(x-tG_t(x))\le g(x)+t\nabla g(x)^TG_t(x)+\frac t2|G_t(x)|^2_2+h(x-tG_t(x))$</p>
<p>$\le g(z)-\nabla g(x)^T(z-x)g(x)+t\nabla g(x)^TG_t(x)+\frac t2|G_t(x)|^2_2+h(z)-\partial h(x-tG_t(x))^T(z-(x-tG_t(x)))$</p>
</blockquote>
<p>由次梯度的定义 $G_t(x)-\nabla g(x)\in \partial h(x-tG_t(x))$ 。因此：</p>
<blockquote>
<p>$f(x-tG_t(x))\le g(z)+h(z)+G_t(x)^T(x-z)-\frac t2|G_t(x)|_2^2=f(z)+G_t(x)^T(x-z)-\frac t2|G_t(x)|^2_2$</p>
</blockquote>
<p>由于 $G_t(x)$ 定义，取 $x=x^{(k-1)}$ ,  $x^{(k)}=x^{(k-1)}-tG_t(x^{(k-1)})$,  $z=x^{(k-1)}$:</p>
<blockquote>
<p>$f(x^{(k)})\le f(x^{(k-1)})-\frac t2|G_t(x^{(k-1)})|^2_2$</p>
</blockquote>
<p>则可知这是一个下降的方法。</p>
<p>又令 $z=x^*$, 为极小值点，则可以得到：</p>
<blockquote>
<p>$f(x^{(k)})-f(x^{*})\le G_t(x^{(k-1)})^T(x^{(k-1)}-x^{*})-\frac t2|G_t(x^{(k-1)})|_2^2=\frac 1{2t}\left(|x^{(k-1)}-x^{*}|_2^2-|x^{(k-1)}-x^*-tG_t(x^{(k-1)})|^2_2\right)$</p>
<p>$=\frac 1{2t}\left(|x^{(k-1)}-x^{*}|_2^2-|x^{(k)}-x^{*}|_2^2\right)$</p>
</blockquote>
<p>因为 $f(x^{(k)})-f(x^{*})\ge0$,所以 $|x^{(k-1)}-x^{*}|_2^2\ge|x^{(k)}-x^{*}|_2^2$, 即迭代使得 $x,f(x)$ 均不断向着最优值靠近。</p>
<blockquote>
<p>$f(x^{(k)})-f(x^{*})\le \frac 1k\sum_{i=1}^kf(x^{(i)})-f(x^{*})\le\frac 1{2tk}\left(|x^{(0)}-x^{*}|^2_2-|x^{(k)}-x^{*}|_2^2\right)\le\frac 1{2tk}\left(|x^{(0)}-x^{*}|_2^2\right)$</p>
</blockquote>
<p>收敛性证毕，并且得到收敛速度为 $O(1/k)$</p>
<h4 id="近端梯度下降法求解-Lasso-回归"><a href="#近端梯度下降法求解-Lasso-回归" class="headerlink" title="近端梯度下降法求解 Lasso 回归"></a>近端梯度下降法求解 Lasso 回归</h4><p>在Lasso回归中 $g(\beta)=|y-X\beta|_2^2, h(\beta)=\lambda|\beta|_1=\lambda\sum_{j=1}^p|\beta_j|$。因此</p>
<blockquote>
<p>$\beta^k=prox_{h,t}(\beta^{(k-1)}-t\nabla g(\beta^{(k-1)}) )=prox_{h,t}(\beta^{(k-1)}+tX^Ty-tX^TX\beta^{(k-1)})$</p>
<p>其中 $h(\beta)=\lambda\sum_{j=1}^p|\beta_j|$</p>
</blockquote>
<p>该迭代公式通常也被称为<strong>迭代软阈值算法</strong></p>
<p>下面对其进行求解，令 $z^{(k-1)}=\beta^{(k-1)}+tX^Ty-tX^TX\beta^{(k-1)}$, 因此</p>
<blockquote>
<p>$\beta^{(k)}=\arg\min_{\beta}\frac 1{2t}|\beta-z^{(k-1)}|_2^2+\lambda\sum_{j=1}^p|\beta_j|$</p>
</blockquote>
<p>则可以解的：</p>
<script type="math/tex; mode=display">
\beta^{(k)}_{i}=\left\{\begin{array}{ll}z^{(k-1)}_{i}-\lambda t, & \lambda t<z^{(k-1)}_{i} \\ 0, & \left|z^{(k-1)}_{i}\right| \leqslant \lambda t\\ z^{(k-1)}_{i}+\lambda t, & z^{(k-1)}_{i}<-\lambda t\end{array}\right.</script><h3 id="最小角回归方法"><a href="#最小角回归方法" class="headerlink" title="最小角回归方法"></a>最小角回归方法</h3><p>最小角回归是一种变量选择方法，是由前向选择算法所演化而来的。</p>
<h4 id="Forward-Selection-算法"><a href="#Forward-Selection-算法" class="headerlink" title="Forward Selection 算法"></a>Forward Selection 算法</h4><p> 在这里考虑选择标准为：每一步都选择能最小化残差变量。</p>
<p>下面说明在每一步中选择最小化残差向量的特征等价于选择与上一步残差最为相关的特征：</p>
<p>设候选集中已有 $k$ 个特征，不妨将 $X$ 的前 $k$ 列就设为候选集中的特征。则可将划分成两部分： $X=[X_1\quad X_2]$, $X_1$ 为候选集中的特征，对应的划分 $Q$ ,$R$ 。由 $QR$ 分解可知， $\hat{y}=Q_1Q_1^Ty$, 残差 $r=\hat{y}-y=(I-Q_1Q_1^T)y$, 设第 $k+1$ 个添加的特征为 $x_{k+1}$,则：</p>
<blockquote>
<p>$z_{k+1}=x_{k+1}-\sum_{i=1}^k\gamma_{i(k+1)}x_i$</p>
<p>其中：$\hat{\gamma}_{i(k+1)}=\langle z_i,x_{k+1}\rangle/\langle z_i,z_i\rangle, i=1,\cdots,k$</p>
</blockquote>
<p>则 $z_{k+1}=(I-Q_1Q_1^T)x_{k+1}$, 令 $Q_1^{*}=[Q_1\quad \frac{z_{k+1}}{|z_{k+1}|^2}]=[Q_1\quad q]$, 则 $\hat{y}^{*}=Q_1^{*}(Q_1^{*})^Ty=Q_1Q_1^Ty+qq^Ty=Q_1Q_1^Ty+(q^Ty)q$, 因此残差为：</p>
<blockquote>
<p>$r^{*}=\hat{y}^{*}-y=r+(q^Ty)q$</p>
</blockquote>
<p>则 $|r^*|^2=r^Tr+2(q^Ty)r^Tq+(q^Ty)^2$</p>
<p>所以最小化 $|r^*|^2$ 等价于最小化 $q^Tyr^Tq$,  经过简单的计算 $r^Tq=\frac{y^T(I-Q_1Q_1^T)x_{k+1}}{|z_{k+1}|^2}=q^Ty$.</p>
<p>因此等价于最小化 $|r^Tq|^2=\frac{|r^Tq|}{|z_{k+1}|^2}=\frac{|r^Tq|}{|q|^2}$ .</p>
<p>综上所述算法的具体步骤为：</p>
<blockquote>
<p>(1).  对于中心化的 $p$ 个特征，分别计算与 $y$ 之间的相关系数，选择最为相关的特征 $k$ 加入候选集, 即 $k=\arg\max_j\frac{|x_j^Ty|}{|x_j|^2}$。</p>
<p>(2).  然后利用候选集中的特征求解回归问题，若候选集中的特征有 $1,\cdots,j$, 则可以计算残差为：$r_j=y-\beta_1^{*}x_1-\beta_2^{*}x_2-\cdots-\beta_j^{*}x_j$</p>
<p>(3).  在 $x_{j+1},\cdots,x_p$ 中选择与残差 $r_j$ 相关系数最大的特征，不断进行下去。</p>
</blockquote>
<h4 id="Forward-Stragewise-算法"><a href="#Forward-Stragewise-算法" class="headerlink" title="Forward Stragewise 算法"></a>Forward Stragewise 算法</h4><p>该算法是以一个步长 $\alpha$ 来迭代计算 $\beta$. 具体过程如下：</p>
<blockquote>
<p>(1). 初始化 $r=y,\beta_j=0(1\le j\le p)$</p>
<p>(2). 计算与 $y$ 最为相关的特征 $k$, 即 $k=\arg\max_j|x_i^Ty|$.</p>
<p>(3). 更新 $\beta_k=\beta_k+\alpha*sign(x_k^Tr)$.</p>
<p>(4). 更新残差 $r=r-\alpha*sign(x_k^Tr)*x_k$.</p>
<p>(5). 重复上述过程一定轮数。</p>
</blockquote>
<p>一般来说，对于很小的步长，使用前向逐渐选择算法可以产生类似Lasso的稀疏的效果。所以有时可以利用前向逐渐选择算法的结果来当作Lasso的结果，但是有三个问题：第一，步长很难确定；第二，需要迭代的次数很难确定。第三，数据运算量很大。</p>
<h4 id="最小角回归算法"><a href="#最小角回归算法" class="headerlink" title="最小角回归算法"></a><strong>最小角回归算法</strong></h4><p> 该算法中和了Forward Selection 算法和Forward Stragewise 算法，每次选取的步长为角平分线：</p>
<blockquote>
<p>(1).  令 $ x_i$ 为经过标准初始化的特征，初始化 $r=y$, $\beta_j=0,j=1\cdots,p$ .</p>
<p>(2).  寻找与 $r$ 相关性的最大的 $x_j$.</p>
<p>(3).  沿着 $sign(corr(r,x_j))$ 的方向增加 $\beta_j$, 直到存在一个其他的特征 $x_k$ 与当前残差有相同的相关性,并将 $x_k$ 加入候选集。</p>
<p>(4).  再计算当前残差 $r$ 在 $(x_j,x_k)$ 的联合最小二乘系数方向上移动来增加 $\beta_j,\beta_k$，直到存在其它的预测变量 $x_l$ 与当前残差的相关性和当前残差与 $(x_j,x_k)$ 的相关性相等。</p>
<p>(5).  这种方式继续直到所有的 $p$ 个预测变量加入到模型中．经过 $\min(n-1,p)$ 步，可以达到全最小二乘的解．</p>
</blockquote>
<p>下面理论推导迭代方向和补偿选取，特征的选取。</p>
<p>先证残差假设已经进行了第 $k$ 次特征选择，当前候选集包含特征 $x_1,x_2,\cdots,x_k$, 当前残差为 $r_k$, 假设 $x_j,j=1,\cdots,k$ 与 $r_k$ 之间的相关性一样，即 $\frac{|\langle x_j,r_k\rangle|}{|x_j|\cdot|r_k|}=\lambda,j=1,\cdots,p$, 由步骤三: 移动方向为联合最小二乘系数方向，设前 $k$ 个特征构成的特征矩阵为 $X_1$ , $X_1$ 关于 $r_k$ 的最小二乘系数为 $\hat{\beta}$, 即当前参数的移动方向为 $\hat{\beta}$, 设移动步长为 $\alpha$ . 令 $u(\alpha)=\alpha X_1\hat{\beta}$ ，则 $r_{k+1}=r_k-u(\alpha)$。</p>
<blockquote>
<p>$\frac{|\langle x_j,r_k-u(\alpha)\rangle|}{|x_j|\cdot|r_k-u(\alpha)|}=\frac{1}{|r_k-u(\alpha)|}\cdot\frac{|\langle x_j,\alpha r_k+(1-\alpha)r_k-u(\alpha)\rangle|}{|x_j|}$</p>
<p>$=\frac{1}{|r_k-u(\alpha)|}\cdot\frac{|\langle x_j,\alpha(r_k-X_1\hat{\beta})+(1-\alpha)r_k\rangle |}{|x_j|}$</p>
<p>$=\frac{(1-\alpha)\lambda|r_k|}{|r_k-u(\alpha)|}$</p>
</blockquote>
<p>其中倒数第二个等式是由于 $\langle x_j,\alpha(r_k-X_1\hat{\beta})\rangle=x_j^T(r_k-X\hat{\beta})=0$.</p>
<p>即对任意 $j,l=1,\cdots,k, $ 有 $\frac{|\langle x_j,r_k-u(\alpha)\rangle|}{|x_j|\cdot|r_k-u(\alpha)|}=\frac{|\langle x_l,r_k-u(\alpha)\rangle|}{|x_l|\cdot|r_k-u(\alpha)|}$，即相关性相同。</p>
<p>由于 $\hat{\beta}=(X_1^TX_1)^{-1}X_1^Tr_k$，因此 $u(\alpha)=\alpha X_1(X_1^TX_1)^{-1}X_1^Tr_k$，则 $X_1^Tu(\alpha)=X_1^Tr_k=\lambda\mathbf{1}$ . 即每次特征选择后，候选集里的所有特征与残差的相关性相等，也就是说 $u(\alpha)$ 为对角线方向，这也是为什么称为最小角回归的原因。</p>
<p>下面计算步长 $\alpha$ :</p>
<p>仍考虑候选集有 $k$ 个特征，令$\mathcal{A}={1,\cdots,k},$ 由于任意候选集里的特征与残差的相关系数相等，设为 $\lambda$, 则有 $\frac{|\langle x_j,r_{k+1}\rangle|}{|x_j|\cdot|r_{k+1}|}=\frac{(1-\alpha)\lambda|r_k|}{|r_{k+1}|}$,对任意 $j=1,\cdots,p$, 由于 $x_j$ 经过中心标准化，则 $|x_j|=1$，所以 $\max_{j\in\mathcal{A}^c}|\langle x_j,r_{k}-u(\alpha)\rangle |=(1-\alpha)\lambda|r_k|$, 因此$|corr(x_j,r_k)-\alpha corr(x_j,X_1\hat{\beta})|=(1-\alpha)\lambda$, 记 $C_j=corr(x_j,r_k),D_j=corr(x_j,X_1\hat{\beta})$, 则可以解的：</p>
<blockquote>
<p>$\alpha=\min _{j \in A^{c}}^{+}\left\{\frac{\lambda-C_{j}}{\lambda-D_{j}}, \frac{\lambda+C_{j}}{\lambda+D_{j}}\right\}$</p>
</blockquote>
<p>若令 $|\langle x_j,r_k\rangle|=\lambda|r_k|=\hat{C}$, $|\langle x_j,r_k\rangle|=C_j$, $|\langle x_j,X_1\hat{\beta}\rangle|=D_j,$ 则可以得到和文献 [8] 中一致的表达方式。</p>
<hr>
<p>最小角回归和Lasso的关系有空再补。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><hr>
<p>[1] T. Hastie, R.J. Tibshirani, Friedman J.H. The Elements of Statistical Learning: Springer[J]. Elements, 2009, 1(3):267-268.</p>
<p>[2] 周志华. 机器学习. 清华大学出版社，2016.1</p>
<p>[3] Tibshirani, Robert. Regression Shrinkage and Selection Via the Lasso[J]. Journal of the Royal Statistical Society, 58(1):267-288.</p>
<p>[4] Stephen Boyd, Lieven Vandenberghe. Convex Optimization[M]. 世界图书出版公司, 2004.</p>
<p>[5] Boyd, Stephen, Lin Xiao, and Almir Mutapcic. “Subgradient methods.” <em>lecture notes of EE392o, Stanford University, Autumn Quarter</em> 2004 (2003): 2004-2005.</p>
<p>[6] Nedic, Angelia, and Dimitri P. Bertsekas. “Incremental subgradient methods for nondifferentiable optimization.” <em>SIAM Journal on Optimization</em> 12.1 (2001): 109-138.</p>
<p>[7] Tseng, Paul. “Convergence of a block coordinate descent method for nondifferentiable minimization.” <em>Journal of optimization theory and applications</em> 109.3 (2001): 475-494.</p>
<p>[8] Efron, Bradley, et al. “Least angle regression.” <em>The Annals of statistics</em> 32.2 (2004): 407-499.</p>

        </div>
        
            <div style="text-align:center;color: rgb(125,125,125);font-size:14px;">
               -------------本文结束 感谢您的阅读-------------
            </div>
        
        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>文章作者:</span>
                        <span>会飞的猪</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>文章链接:</span>
                        <span><a href="http://shellyandliu.github.io/2020/04/16/Lasso%E5%9B%9E%E5%BD%92/">http://shellyandliu.github.io/2020/04/16/Lasso%E5%9B%9E%E5%BD%92/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>版权声明:</span>
                        <span>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC-BY-NC-4.0</a> ， 转载请注明来自 会飞的猪！</span>
                    </p>
                
            </section>
        
        <section class="post-tags">
            <div>
                <span>标签:</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"> <i class="iconfont icon-tags"></i> 机器学习</a>
                    
                        <a href="/tags/%E7%BB%9F%E8%AE%A1/"> <i class="iconfont icon-tags"></i> 统计</a>
                    
                        <a href="/tags/%E4%BC%98%E5%8C%96/"> <i class="iconfont icon-tags"></i> 优化</a>
                    
                        <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"> <i class="iconfont icon-tags"></i> 线性回归</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">返回</a>
                <span> /  </span>
                <a href="/">首页</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/05/08/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">梯度下降法</a>
            
            
            <a class="next" rel="next" href="/2020/04/16/%E4%B8%BB%E6%88%90%E5%88%86%E5%9B%9E%E5%BD%92%E5%92%8C%E5%81%8F%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E5%9B%9E%E5%BD%92/">主成分回归和偏最小二乘回归</a>
            
        </section>
        <div class="post-content">
            <section style="font-size: 20px;font-weight: 700;margin-bottom: 10px;margin-top: 20px;">
                <i class="iconfont icon-comments"></i>
                <span>
                    评论
                </span>
            </section>
        </div>
        
            <section id="comments" class="comments">
              <style>
                .comments{margin:0px;padding:10px;background:#fff}
                @media screen and (max-width:800px){.comments{margin:auto;padding:10px;background:#fff}}
              </style>
              <div class="valine_comment"></div>
<!--����js����</body>֮ǰ���뼴��-->
<!--Leancloud ������:-->
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<!--Valine �ĺ��Ĵ����-->
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
  new Valine({
      el: '.valine_comment',
      app_id: 'bCvyxhtL2V59mmqAp81lAGob-gzGzoHsz#Leancloud应用的appId',
      app_key: 'J4zVKQfUjhToGJwBfLcoOHOv',
      placeholder: '欢迎大家评论交流~',
      notify: 'true',
      verify: 'true',
    });
</script>
            </section>
        
        

    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 会飞的猪 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>

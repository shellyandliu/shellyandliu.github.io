<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Lasso回归 | 会飞的猪</title><meta name="description" content="本文介绍了一种经典回归方法--Lasso回归,并介绍了几种求解该问题的经典算法。"><meta name="keywords" content="机器学习,统计,优化,线性回归"><meta name="author" content="会飞的猪"><meta name="copyright" content="会飞的猪"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Lasso回归"><meta name="twitter:description" content="本文介绍了一种经典回归方法--Lasso回归,并介绍了几种求解该问题的经典算法。"><meta name="twitter:image" content="http://shellyandliu.github.io/img/post.jpg"><meta property="og:type" content="article"><meta property="og:title" content="Lasso回归"><meta property="og:url" content="http://shellyandliu.github.io/2020/04/16/Lasso%E5%9B%9E%E5%BD%92/"><meta property="og:site_name" content="会飞的猪"><meta property="og:description" content="本文介绍了一种经典回归方法--Lasso回归,并介绍了几种求解该问题的经典算法。"><meta property="og:image" content="http://shellyandliu.github.io/img/post.jpg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://shellyandliu.github.io/2020/04/16/Lasso%E5%9B%9E%E5%BD%92/"><link rel="next" title="主成分回归和偏最小二乘回归" href="http://shellyandliu.github.io/2020/04/16/%E4%B8%BB%E6%88%90%E5%88%86%E5%9B%9E%E5%BD%92%E5%92%8C%E5%81%8F%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E5%9B%9E%E5%BD%92/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/longmao.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">8</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">4</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">1</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Lasso-回归"><span class="toc-number">1.</span> <span class="toc-text">Lasso 回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#求解Lasso回归的数值算法"><span class="toc-number">2.</span> <span class="toc-text">求解Lasso回归的数值算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#坐标轴下降法求解"><span class="toc-number">2.1.</span> <span class="toc-text">坐标轴下降法求解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#次梯度"><span class="toc-number">2.1.1.</span> <span class="toc-text">次梯度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#算法与收敛性"><span class="toc-number">2.1.2.</span> <span class="toc-text">算法与收敛性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#坐标轴下降法求解-Lasso-回归"><span class="toc-number">2.1.3.</span> <span class="toc-text">坐标轴下降法求解 Lasso 回归</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#近端梯度下降法"><span class="toc-number">2.2.</span> <span class="toc-text">近端梯度下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#算法与收敛性-1"><span class="toc-number">2.2.1.</span> <span class="toc-text">算法与收敛性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#近端梯度下降法求解-Lasso-回归"><span class="toc-number">2.2.2.</span> <span class="toc-text">近端梯度下降法求解 Lasso 回归</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最小角回归方法"><span class="toc-number">2.3.</span> <span class="toc-text">最小角回归方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Forward-Selection-算法"><span class="toc-number">2.3.1.</span> <span class="toc-text">Forward Selection 算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Forward-Stragewise-算法"><span class="toc-number">2.3.2.</span> <span class="toc-text">Forward Stragewise 算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#最小角回归算法"><span class="toc-number">2.3.3.</span> <span class="toc-text">最小角回归算法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文献"><span class="toc-number">3.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(/img/post.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">会飞的猪</a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">Lasso回归</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-04-16 17:54:43"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-04-16</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-04-25 16:51:47"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-04-25</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">-线性回归</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><h2 id="Lasso-回归"><a href="#Lasso-回归" class="headerlink" title="Lasso 回归"></a>Lasso 回归</h2><p>LASSO回归相当于将岭回归的2范数换成了1范数，用拉格朗日对偶形式表示为：</p>
<blockquote>
<p>$\min _{\beta}\left\{\frac{1}{2 N}\left|y-X \beta\right|_{1}\right\},|\beta|_{1} \leq t$</p>
</blockquote>
<p>得到的解称为<strong>LASSO估计</strong>。对应的原始的优化形式中的 $\lambda$ 称为<strong>收缩算子</strong>。</p>
<p>相较于岭回归，LASSO回归的优势在于，它能够使有些变量迅速的趋于0，因此可以用作<strong>变量选择</strong>，下面作解释：</p>
<p>设 $\hat{\beta}$ 为普通最小二乘得到的参数估计,则</p>
<blockquote>
<p>$L(\beta)=\frac 1 {2N}|Y-X\beta|_2^2+\lambda|\beta|_1=\frac 1 {2N}|Y-X\hat{\beta}+X\hat{\beta}-X\beta|_2^2+\lambda|\beta|_1 =L(\hat{\beta})+\frac 1{2N}|X(\hat{\beta}-\beta)|_2^2+|\lambda|_1$</p>
<p>$=L(\hat{\beta})+\frac 1{2N}(\hat{\beta}-\beta)’X’X(\hat{\beta}-\beta)+\lambda|\beta|_1$</p>
</blockquote>
<p>则 $\min_{\beta}L(\beta)=\min_{\beta}\{\frac 1{2N}\}(\hat{\beta}-\beta)’X’X(\hat{\beta}-\beta)+\lambda|\beta|_1$，不妨设$X’X$为对角矩阵 $diag(\lambda_1,\dots,\lambda_p)$，可以通过主成分分析得到，则</p>
<blockquote>
<p>$\min_{\beta}L(\beta)=\min_{\beta_i}\frac1 {2N}\sum_{i=1}^p(\lambda_i(\hat{\beta}_i-\beta_i)^2+\lambda|\beta_i|)$</p>
</blockquote>
<p>则可以解的：$\beta_{i}=\operatorname{sign}\left(\hat{\beta}_{i}\right) \max \left\{\left|\hat{\beta}_{i}\right|-\frac{\lambda}{\lambda_i}, 0\right\}$</p>
<p>当 $\hat{\beta}_i &gt;0 $ 时：</p>
<ol>
<li><p>当 $\hat{\beta}_i\le \frac{\lambda}{\lambda_i}$，此时目标中 $\beta_i$ 的最优值为 $\beta_i=0$</p>
</li>
<li><p>$\hat{\beta}_i\ge \frac{\lambda}{\lambda_i}$，则正则化不会讲 $\beta_i$ 的最优值推向 $0$,而仅移动 $\frac{\lambda}{\lambda_i}$ 的距离。</p>
</li>
</ol>
<p>当 $\hat{\beta}_i &lt;0 $ 时与之类似。</p>
<p>在 $X’X$ 为对角矩阵时，可以通过上述方法得到闭式解。但由于一般 $(X’X)^{-1}$ 过于复杂，一般并不采用这种方法，由于$L-1$ 范数并不可微，因此传统的梯度下降法也将失效，作为替代一般采用以下算法。</p>
<h2 id="求解Lasso回归的数值算法"><a href="#求解Lasso回归的数值算法" class="headerlink" title="求解Lasso回归的数值算法"></a>求解Lasso回归的数值算法</h2><h3 id="坐标轴下降法求解"><a href="#坐标轴下降法求解" class="headerlink" title="坐标轴下降法求解"></a>坐标轴下降法求解</h3><h4 id="次梯度"><a href="#次梯度" class="headerlink" title="次梯度"></a>次梯度</h4><p>梯度是一个函数增长最快的方向，但是当函数在某点不可导时梯度就没法计算了，作为推广引入次梯度：</p>
<blockquote>
<p>定义：函数 $f$ 在某点 $x_0$ 的次梯度 $\nabla g,$ 满足：</p>
<p>$f(x)-f(x_0)\ge \nabla g^T(x-x_0)$, 对任意 $x$ 成立。</p>
</blockquote>
<p>次梯度不能保证是增长最快的方向，甚至有可能是降低的方向，故形成了一个不等式。并且次梯度不是一个确定的值，而是一个取值范围内中的一个点。如果在该点是可导的，则次梯度就是梯度。</p>
<p>若设 $\partial f(x)$ 为 $f$ 在点 $x$ 处的次微分，则由定义可知 $g(x^{(k)})\in \partial f(x^{(k)})$</p>
<p>次梯度方法的迭代更新形式为：</p>
<blockquote>
<p>$x^{k+1}=x^{k}-\alpha_k\nabla g(x^kk)$</p>
</blockquote>
<p>由于次梯度方向并不一定总是函数值下降的方向，因此很自然的在每一次迭代后，都取曾今最小的函数值：</p>
<blockquote>
<p>$f^{(k)}_{best}=\min\{f^{k-1}_{best},f(x^{(k)}\}$</p>
</blockquote>
<p>下面给出几种常用的步长选择方式：</p>
<blockquote>
<p>(1).  <em>Constant step size:</em> $\alpha_k=\alpha$ 是一个正常数。</p>
<p>(2).  <em>Constant step length:</em> $\alpha_k=\frac{\gamma}{|g^{(k)}|_2}$，其中 $\gamma&gt;0$ ，这就意味着 $|x^{(k+1)}-x^{(k)}|_2=\gamma$</p>
<p>(3).  <em>Square summable but not summable:</em>  $\alpha_k\ge0, \quad\sum_{k=1}^{\infty}\alpha_k^2&lt;\infty,\quad \sum_{k=1}^{\infty}\alpha_k=\infty$,一个典型的例子是 $\alpha_k=a/(b+k)$, 其中 $a&gt;0,b\ge0$</p>
<p>(4).  <em>diminishing step size rules:</em> $\alpha_k\ge0,\quad\lim_{k\to\infty}\alpha_k=0,\quad\sum_{k=1}^{\infty}\alpha_k=\infty$, 一个典型的例子是 $\alpha_k=a/\sqrt{k}$, 其中 $a&gt;0$.</p>
<p>(5).  <em>Nonsummable diminishing step lengths:</em> $\alpha_k=\frac{\gamma_k}{|g^{(k)}|_2}$, 其中 $\gamma_k\ge0,\quad\lim_{k\to\infty}\gamma_k=0,\quad\sum_{k=1}^{\infty}\gamma_k=\infty$.</p>
</blockquote>
<p>设 $x^{*}$ 为 $f$ 的极小值点；设迭代的初始点 $x^{(1)}$ 与极小值点 $x^{*}$ 之间满足：$|x^{(1)}-x^{*}|_2\le R$,  $R$ 为常数；设 $f$ 满足 Lipschitz 条件：</p>
<blockquote>
<p>对任意的 $u,v$ , 有 $|f(u)-f(v)|\le G|u-v|_2$</p>
</blockquote>
<p>下面给出上述步长的收敛性结果：</p>
<blockquote>
<p>定理1：<em>Constant step size:</em> $f_{\mathrm{best}}^{(k)}-f^{*} \leq \frac{R^{2}+G^{2} \alpha^{2} k}{2 \alpha k}$, 当 $k\to\infty$ 时, 收敛于 $G^2\alpha/2$.</p>
<p>定理2：<em>Constant step length:</em> $f_{\mathrm{best}}^{(k)}-f^{*} \leq \frac{R^{2}+G^{2} \alpha^{2} k}{2 \sum_{i=1}^k\alpha_i}\le\frac{R^2+\gamma^2k}{2\gamma k/G}$, 当 $k\to\infty$ 时,收敛于 $G\gamma/2$</p>
<p>定理3：<em>Square summable but not summable:</em>  $f_{\mathrm{best}}^{(k)}-f^{*} \leq \frac{R^{2}+G^{2} \sum_{k=1}^{\infty}\alpha_k^2}{2 \sum_{i=1}^k\alpha_i}$，当 $k\to\infty$ 时,收敛于 $0$.</p>
<p>定理4：<em>diminishing step size rules:</em>  $f_{\mathrm{best}}^{(k)}-f^{*} \leq \frac{R^{2}+G^{2} \sum_{i=1}^{k}\alpha_k^2}{2 \sum_{i=1}^k\alpha_i}$, 当 $k\to\infty$ 时,收敛于 $0$.</p>
<p>定理5：<em>Nonsummable diminishing step lengths:</em>  $f_{\mathrm{best}}^{(k)}-f^{*} \leq \frac{R^{2}+ \sum_{i=1}^{k}\gamma_k^2}{2 \sum_{i=1}^k\alpha_i}\le \frac{R^{2}+ \sum_{i=1}^{k}\gamma_k^2}{(2/G) \sum_{i=1}^k\gamma_i}$, 当 $k\to\infty$ 时,收敛于 $0$.</p>
</blockquote>
<p>下面给出证明：</p>
<p>由于 $f$ 是 Lipschitz 连续的，因此有 $|g|_2\le G$ , 则有：</p>
<script type="math/tex; mode=display">
\begin{aligned}\left\|x^{(k+1)}-x^{\star}\right\|_{2}^{2} &=\left\|x^{(k)}-\alpha_{k} g^{(k)}-x^{\star}\right\|_{2}^{2} \\ &=\left\|x^{(k)}-x^{\star}\right\|_{2}^{2}-2 \alpha_{k} g^{(k) T}\left(x^{(k)}-x^{\star}\right)+\alpha_{k}^{2}\left\|g^{(k)}\right\|_{2}^{2} \\ & \leq\left\|x^{(k)}-x^{\star}\right\|_{2}^{2}-2 \alpha_{k}\left(f\left(x^{(k)}\right)-f^{\star}\right)+\alpha_{k}^{2}\left\|g^{(k)}\right\|_{2}^{2} \end{aligned}</script><p>因此有：</p>
<blockquote>
<p>$\left|x^{(k+1)}-x^{\star}\right|_{2}^{2} \leq\left|x^{(1)}-x^{\star}\right|_{2}^{2}-2 \sum_{i=1}^{k} \alpha_{i}\left(f\left(x^{(i)}\right)-f^{\star}\right)+\sum_{i=1}^{k} \alpha_{i}^{2}\left|g^{(i)}\right|_{2}^{2}$</p>
</blockquote>
<p>由于 $|x^{(1)}-x^{*}|_2\le R$, $|x^{(k+1)}-x^{*}|_2^2\ge0$,则有</p>
<blockquote>
<p>$2\sum_{i=1}^{k} \alpha_{i}\left(f\left(x^{(i)}\right)-f^{\star}\right)\le R^2+\sum_{i=1}^{k} \alpha_{i}^{2}\left|g^{(i)}\right|_{2}^{2}$</p>
</blockquote>
<p>又因为 $\sum_{i=1}^{k} \alpha_{i}\left(f\left(x^{(i)}\right)-f^{\star}\right) \geq\left(\sum_{i=1}^{k} \alpha_{i}\right) \min _{i=1, \ldots, k}\left(f\left(x^{(i)}\right)-f^{\star}\right)=\left(\sum_{i=1}^{k} \alpha_{i}\right)\left(f_{\mathrm{best}}^{(k)}-f^{\star}\right)$:</p>
<p>因此：</p>
<blockquote>
<p>$f_{\text {best }}^{(k)}-f^{\star}=\min _{i=1, \ldots, k} f\left(x^{(i)}\right)-f^{\star} \leq \frac{R^{2}+\sum_{i=1}^{k} \alpha_{i}^{2}\left|g^{(i)}\right|_{2}^{2}}{2 \sum_{i=1}^{k} \alpha_{i}}\le\frac{R^{2}+G^2\sum_{i=1}^{k} \alpha_{i}^{2}}{2 \sum_{i=1}^{k} \alpha_{i}}$</p>
</blockquote>
<p>所以定理1，2，3，5显然可得.</p>
<p>定理4：对于任意的 $\epsilon&gt;0$,存在整数 $N_1$使得 $\alpha_i\le \epsilon/G^2$, 对任意的 $i&gt;N_1$ 成立。同样的也存在整数 $N_2$， 使得 $\sum_{i=1}^{N_2}\ge\frac 1{\epsilon}\left(R^2+G^2\sum_{i=1}^{N_1}\alpha_i^2\right)$, 由于 $\sum_{i=1}^{\infty}\alpha_i=\infty,$ 令 $N=\max\{N_1,N_2\}$，则对于 $k&gt;N$, 我们有:</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned} \frac{R^{2}+G^{2} \sum_{i=1}^{k} \alpha_{i}^{2}}{2 \sum_{i=1}^{k} \alpha_{i}} & \leq \frac{R^{2}+G^{2} \sum_{i=1}^{N_{1}} \alpha_{i}^{2}}{2 \sum_{i=1}^{k} \alpha_{i}}+\frac{G^{2} \sum_{i=N_{1}+1}^{k} \alpha_{i}^{2}}{2 \sum_{i=1}^{N_{1}} \alpha_{i}+2 \sum_{i=N_{1}+1}^{k} \alpha_{i}} \\ & \leq \frac{R^{2}+G^{2} \sum_{i=1}^{N_{1}} \alpha_{i}^{2}}{(2 / \epsilon)\left(R^{2}+G^{2} \sum_{i=1}^{N_{1}} \alpha_{i}^{2}\right)}+\frac{G^{2} \sum_{i=N_{1}+1}^{k}\left(\epsilon \alpha_{i} / G^{2}\right)}{2 \sum_{i=N_{1}+1}^{k} \alpha_{i}} \\ &=\frac{\epsilon}{2}+\frac{\epsilon}{2}=\epsilon \end{aligned}</script></blockquote>
<h4 id="算法与收敛性"><a href="#算法与收敛性" class="headerlink" title="算法与收敛性"></a>算法与收敛性</h4><p>坐标轴下降法是一种迭代算法，在每个坐标轴上搜索函数的最小值，不断迭代得到收敛值。</p>
<p>设损失函数为 $J(\theta)$，其中 $\theta$ 是 $p$ 维向量。则坐标轴下降法的步骤为：</p>
<blockquote>
<p>(1).  初始位置点 $\theta^{(0)}=(\theta_1^{(0)},\theta_2^{(0)},\dots,\theta_p^{(0)})$</p>
<p>(2).  第 $k$ 次迭代，从 $\theta_1^{(k)}$ 开始，固定后面的 $p-1$ 个参数，计算使 $J(\theta_1|\theta_2^{(k-1)},\dots,\theta_p^{(k-1)})$ 达到最小的 $\theta_1$, 记为 $\theta_1^{(k)}$，然后依次往后计算，则一共执行 $p$ 次运算：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\theta_{1}^{(k)}=\arg \min _{\theta_{1}} J\left(\theta_{1}| \theta_{2}^{(k-1)}, \theta_{3}^{(k-1)}, \ldots, \theta_{p}^{(k-1)}\right)\\
&\theta_{2}^{(k)}=\arg \min _{\theta_{2}} J\left(\theta_{2}|\theta_{1}^{(k)}, \theta_{3}^{(k-1)}, \ldots, \theta_{p}^{(k-1)}\right)\\
&\theta_{3}^{(k)}=\arg \min _{\theta_{3}} J\left(\theta_{3}|\theta_{1}^{(k)}, \theta_{2}^{(k)}, \ldots, \theta_{p}^{(k-1)}\right)\\
&\quad\ \ \ \cdots\\
&\theta_{p}^{(k)}=\arg \min _{\theta_{p}} J\left( \theta_{p}|\theta_{1}^{(k)}, \theta_{2}^{(k)}, \theta_{3}^{(k)}, \ldots\theta_{p-1}^{(k)}\right)
\end{aligned}</script><p>(3).  不断迭代，直到所有的 $\theta_i$ 的变化都小于预先设定的阈值 $\epsilon$.</p>
</blockquote>
<p>收敛性结果：见文献 [7]</p>
<h4 id="坐标轴下降法求解-Lasso-回归"><a href="#坐标轴下降法求解-Lasso-回归" class="headerlink" title="坐标轴下降法求解 Lasso 回归"></a>坐标轴下降法求解 Lasso 回归</h4><p>Lasso回归的损失函数为：</p>
<blockquote>
<p>$L(\beta)=\frac{1}{2n}\left(|y-X\beta|_2^2+\lambda|\beta|_1\right)=\frac{1}{2n}\left(\sum_{i=1}^n(y_i-x_i\beta)^2+\sum_{j=1}^p|\beta_j|\right)$</p>
<p>$=\frac{1}{2n}\left(\sum_{i=1}^n(y_i-\sum_{j=1}^px_ij\beta_j)^2+\sum_{j=1}^p|\beta_j|\right)$</p>
</blockquote>
<p>因此对第 $k$ 个变量做优化，求偏导得：</p>
<script type="math/tex; mode=display">
\begin{aligned}\frac{\partial L}{\partial \beta_{k}}&=\sum_{i=1}^{n}-2 x_{i k}\left(y_{i}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)+\left\{\begin{array}{cc}\lambda & \beta_{k}>0 \\{[-\lambda, \lambda]} & \beta_{k}=0 \\-\lambda & \beta_{k}<0
\end{array}\right.\\&=\sum_{i=1}^{n}-2 x_{i k}\left(y_{i}-\sum_{j \neq k}^{p} x_{i j} \beta_{j}-x_{i k} \beta_{k}\right)+\left\{\begin{array}{cc}\lambda & \beta_{k}>0 \\{[-\lambda, \lambda]} & \beta_{k}=0 \\-\lambda & \beta_{k}<0
\end{array}\right.  \\&=-2 \sum_{i=1}^{n} x_{i k}\left(y_{i}-\sum_{j \neq k}^{p} x_{i j} \beta_{j}\right)+2 \beta_{k} \sum_{i=1}^{n} x_{i k}^{2}+ \left\{\begin{array}{cc}\lambda & \beta_{k}>0 \\{[-\lambda, \lambda]} & \beta_{k}=0 \\-\lambda & \beta_{k}<0
\end{array}\right.\\&=-p_{k}+m_{k} \beta_{k}+\left\{\begin{array}{cc}\lambda & \beta_{k}>0 \\{[-\lambda, \lambda]} & \beta_{k}=0 \\-\lambda & \beta_{k}<0
\end{array}\right.\end{aligned}</script><p>令其偏导为 $0$, 可以得到:</p>
<blockquote>
<script type="math/tex; mode=display">
\beta_{k}^{*}=\left\{\begin{array}{ll}-\frac{1}{m_{k}}\left(p_{k}-\lambda\right) & p_{k}>\lambda \\ -\frac{1}{m_{k}}\left(p_{k}+\lambda\right) & p_{k}<-\lambda \\ 0 & -\lambda \leq p_{k} \leq \lambda\end{array}\right.</script></blockquote>
<h3 id="近端梯度下降法"><a href="#近端梯度下降法" class="headerlink" title="近端梯度下降法"></a>近端梯度下降法</h3><h4 id="算法与收敛性-1"><a href="#算法与收敛性-1" class="headerlink" title="算法与收敛性"></a>算法与收敛性</h4><p>设 $f(x)$ 表示为： $f(x)=g(x)+h(x)$, 其中 $g(x)$ 是一个凸的可微函数， $h(x)$ 是一个凸得非可微函数</p>
<p>首先，若 $f(x)$ 可微时，梯度下降公式 $x^{(k)}=x^{(k-1)}-t\nabla f(x^{(k-1)}) $ 可以通过下列公式导出:</p>
<blockquote>
<p>$x^{k}=\underset{x}{\arg \min } f\left(x^{k-1}\right)+\nabla f\left(x^{k-1}\right)^{\top}\left(x-x^{k-1}\right)+\frac{1}{2 t}\left|x-x^{k-1}\right|_{2}^{2}$</p>
</blockquote>
<p>现在 $h(x)$ 不可微 ，仍按照上述方法，可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned} x^{k} &=\underset{x}{\arg \min } g\left(x^{k-1}\right)+\nabla g\left(x^{k-1}\right)^{\top}\left(x-x^{k-1}\right)+\frac{1}{2 t}\left\|x-x^{k-1}\right\|_{2}^{2}+h(x) \\ &=\underset{x}{\arg \min } \frac{1}{2 t}\left\|x-\left(x^{k-1}-t \nabla g\left(x^{k-1}\right)\right)\right\|_{2}^{2}+h(x) \end{aligned}</script></blockquote>
<p>定义近端算子：</p>
<blockquote>
<p>$prox_{h,t}(x)=\underset{z}{\arg \min } \frac{1}{2 t}\left|x-z\right|_{2}^{2}+h(z)$</p>
</blockquote>
<p>所以迭代关系式为 $x^{(k)}=prox_{h,t}(x^{(k-1)}-t\nabla g(x^{(k-1)}) )$ .</p>
<p>下面证明迭代得收敛性：</p>
<p>设 $f$ 关于初始点 $x^{(0)}$ 的下水平集为 $S=\{x\in dom f|f(x)\le f(x^{(0)})$, 则 $S$ 为闭集。定义 $G_t(x)=\frac 1t (x-prox_{h,t}(x-t\nabla g(x)))$。</p>
<p>由于 $g(x)$ 是凸函数，则存在常数 $M$，使得 $\nabla^2f(x)\le MI$, 所以有 :</p>
<blockquote>
<p>$g(y)\le g(x)+\nabla g(x)^T(y-x)+\frac M2|y-x|^2_2$, 对任意 $x,y$ 成立。</p>
</blockquote>
<p>令 $y=x-tG_t(x)$, 且 $0&lt;t\le1/$M, 有 $g(x-tG_t(x))\le g(x)+t\nabla g(x)^TG_t(x)+\frac t2|G_t(x)|^2_2$</p>
<p>利用函数 $g,h$ 的凸性，即 $g(z)-g(x)\ge\nabla g(x)^T(z-x),h(z)-h(x)\ge \partial h(x)^T(z-x)$, 所以可以得到：</p>
<blockquote>
<p>$f(x-tG_t(x))\le g(x)+t\nabla g(x)^TG_t(x)+\frac t2|G_t(x)|^2_2+h(x-tG_t(x))$</p>
<p>$\le g(z)-\nabla g(x)^T(z-x)g(x)+t\nabla g(x)^TG_t(x)+\frac t2|G_t(x)|^2_2+h(z)-\partial h(x-tG_t(x))^T(z-(x-tG_t(x)))$</p>
</blockquote>
<p>由次梯度的定义 $G_t(x)-\nabla g(x)\in \partial h(x-tG_t(x))$ 。因此：</p>
<blockquote>
<p>$f(x-tG_t(x))\le g(z)+h(z)+G_t(x)^T(x-z)-\frac t2|G_t(x)|_2^2=f(z)+G_t(x)^T(x-z)-\frac t2|G_t(x)|^2_2$</p>
</blockquote>
<p>由于 $G_t(x)$ 定义，取 $x=x^{(k-1)}$ ,  $x^{(k)}=x^{(k-1)}-tG_t(x^{(k-1)})$,  $z=x^{(k-1)}$:</p>
<blockquote>
<p>$f(x^{(k)})\le f(x^{(k-1)})-\frac t2|G_t(x^{(k-1)})|^2_2$</p>
</blockquote>
<p>则可知这是一个下降的方法。</p>
<p>又令 $z=x^*$, 为极小值点，则可以得到：</p>
<blockquote>
<p>$f(x^{(k)})-f(x^{*})\le G_t(x^{(k-1)})^T(x^{(k-1)}-x^{*})-\frac t2|G_t(x^{(k-1)})|_2^2=\frac 1{2t}\left(|x^{(k-1)}-x^{*}|_2^2-|x^{(k-1)}-x^*-tG_t(x^{(k-1)})|^2_2\right)$</p>
<p>$=\frac 1{2t}\left(|x^{(k-1)}-x^{*}|_2^2-|x^{(k)}-x^{*}|_2^2\right)$</p>
</blockquote>
<p>因为 $f(x^{(k)})-f(x^{*})\ge0$,所以 $|x^{(k-1)}-x^{*}|_2^2\ge|x^{(k)}-x^{*}|_2^2$, 即迭代使得 $x,f(x)$ 均不断向着最优值靠近。</p>
<blockquote>
<p>$f(x^{(k)})-f(x^{*})\le \frac 1k\sum_{i=1}^kf(x^{(i)})-f(x^{*})\le\frac 1{2tk}\left(|x^{(0)}-x^{*}|^2_2-|x^{(k)}-x^{*}|_2^2\right)\le\frac 1{2tk}\left(|x^{(0)}-x^{*}|_2^2\right)$</p>
</blockquote>
<p>收敛性证毕，并且得到收敛速度为 $O(1/k)$</p>
<h4 id="近端梯度下降法求解-Lasso-回归"><a href="#近端梯度下降法求解-Lasso-回归" class="headerlink" title="近端梯度下降法求解 Lasso 回归"></a>近端梯度下降法求解 Lasso 回归</h4><p>在Lasso回归中 $g(\beta)=|y-X\beta|_2^2, h(\beta)=\lambda|\beta|_1=\lambda\sum_{j=1}^p|\beta_j|$。因此</p>
<blockquote>
<p>$\beta^k=prox_{h,t}(\beta^{(k-1)}-t\nabla g(\beta^{(k-1)}) )=prox_{h,t}(\beta^{(k-1)}+tX^Ty-tX^TX\beta^{(k-1)})$</p>
<p>其中 $h(\beta)=\lambda\sum_{j=1}^p|\beta_j|$</p>
</blockquote>
<p>该迭代公式通常也被称为<strong>迭代软阈值算法</strong></p>
<p>下面对其进行求解，令 $z^{(k-1)}=\beta^{(k-1)}+tX^Ty-tX^TX\beta^{(k-1)}$, 因此</p>
<blockquote>
<p>$\beta^{(k)}=\arg\min_{\beta}\frac 1{2t}|\beta-z^{(k-1)}|_2^2+\lambda\sum_{j=1}^p|\beta_j|$</p>
</blockquote>
<p>则可以解的：</p>
<script type="math/tex; mode=display">
\beta^{(k)}_{i}=\left\{\begin{array}{ll}z^{(k-1)}_{i}-\lambda t, & \lambda t<z^{(k-1)}_{i} \\ 0, & \left|z^{(k-1)}_{i}\right| \leqslant \lambda t\\ z^{(k-1)}_{i}+\lambda t, & z^{(k-1)}_{i}<-\lambda t\end{array}\right.</script><h3 id="最小角回归方法"><a href="#最小角回归方法" class="headerlink" title="最小角回归方法"></a>最小角回归方法</h3><p>最小角回归是一种变量选择方法，是由前向选择算法所演化而来的。</p>
<h4 id="Forward-Selection-算法"><a href="#Forward-Selection-算法" class="headerlink" title="Forward Selection 算法"></a>Forward Selection 算法</h4><p> 在这里考虑选择标准为：每一步都选择能最小化残差变量。</p>
<p>下面说明在每一步中选择最小化残差向量的特征等价于选择与上一步残差最为相关的特征：</p>
<p>设候选集中已有 $k$ 个特征，不妨将 $X$ 的前 $k$ 列就设为候选集中的特征。则可将划分成两部分： $X=[X_1\quad X_2]$, $X_1$ 为候选集中的特征，对应的划分 $Q$ ,$R$ 。由 $QR$ 分解可知， $\hat{y}=Q_1Q_1^Ty$, 残差 $r=\hat{y}-y=(I-Q_1Q_1^T)y$, 设第 $k+1$ 个添加的特征为 $x_{k+1}$,则：</p>
<blockquote>
<p>$z_{k+1}=x_{k+1}-\sum_{i=1}^k\gamma_{i(k+1)}x_i$</p>
<p>其中：$\hat{\gamma}_{i(k+1)}=\langle z_i,x_{k+1}\rangle/\langle z_i,z_i\rangle, i=1,\cdots,k$</p>
</blockquote>
<p>则 $z_{k+1}=(I-Q_1Q_1^T)x_{k+1}$, 令 $Q_1^{*}=[Q_1\quad \frac{z_{k+1}}{|z_{k+1}|^2}]=[Q_1\quad q]$, 则 $\hat{y}^{*}=Q_1^{*}(Q_1^{*})^Ty=Q_1Q_1^Ty+qq^Ty=Q_1Q_1^Ty+(q^Ty)q$, 因此残差为：</p>
<blockquote>
<p>$r^{*}=\hat{y}^{*}-y=r+(q^Ty)q$</p>
</blockquote>
<p>则 $|r^*|^2=r^Tr+2(q^Ty)r^Tq+(q^Ty)^2$</p>
<p>所以最小化 $|r^*|^2$ 等价于最小化 $q^Tyr^Tq$,  经过简单的计算 $r^Tq=\frac{y^T(I-Q_1Q_1^T)x_{k+1}}{|z_{k+1}|^2}=q^Ty$.</p>
<p>因此等价于最小化 $|r^Tq|^2=\frac{|r^Tq|}{|z_{k+1}|^2}=\frac{|r^Tq|}{|q|^2}$ .</p>
<p>综上所述算法的具体步骤为：</p>
<blockquote>
<p>(1).  对于中心化的 $p$ 个特征，分别计算与 $y$ 之间的相关系数，选择最为相关的特征 $k$ 加入候选集, 即 $k=\arg\max_j\frac{|x_j^Ty|}{|x_j|^2}$。</p>
<p>(2).  然后利用候选集中的特征求解回归问题，若候选集中的特征有 $1,\cdots,j$, 则可以计算残差为：$r_j=y-\beta_1^{*}x_1-\beta_2^{*}x_2-\cdots-\beta_j^{*}x_j$</p>
<p>(3).  在 $x_{j+1},\cdots,x_p$ 中选择与残差 $r_j$ 相关系数最大的特征，不断进行下去。</p>
</blockquote>
<h4 id="Forward-Stragewise-算法"><a href="#Forward-Stragewise-算法" class="headerlink" title="Forward Stragewise 算法"></a>Forward Stragewise 算法</h4><p>该算法是以一个步长 $\alpha$ 来迭代计算 $\beta$. 具体过程如下：</p>
<blockquote>
<p>(1). 初始化 $r=y,\beta_j=0(1\le j\le p)$</p>
<p>(2). 计算与 $y$ 最为相关的特征 $k$, 即 $k=\arg\max_j|x_i^Ty|$.</p>
<p>(3). 更新 $\beta_k=\beta_k+\alpha*sign(x_k^Tr)$.</p>
<p>(4). 更新残差 $r=r-\alpha*sign(x_k^Tr)*x_k$.</p>
<p>(5). 重复上述过程一定轮数。</p>
</blockquote>
<p>一般来说，对于很小的步长，使用前向逐渐选择算法可以产生类似Lasso的稀疏的效果。所以有时可以利用前向逐渐选择算法的结果来当作Lasso的结果，但是有三个问题：第一，步长很难确定；第二，需要迭代的次数很难确定。第三，数据运算量很大。</p>
<h4 id="最小角回归算法"><a href="#最小角回归算法" class="headerlink" title="最小角回归算法"></a><strong>最小角回归算法</strong></h4><p> 该算法中和了Forward Selection 算法和Forward Stragewise 算法，每次选取的步长为角平分线：</p>
<blockquote>
<p>(1).  令 $ x_i$ 为经过标准初始化的特征，初始化 $r=y$, $\beta_j=0,j=1\cdots,p$ .</p>
<p>(2).  寻找与 $r$ 相关性的最大的 $x_j$.</p>
<p>(3).  沿着 $sign(corr(r,x_j))$ 的方向增加 $\beta_j$, 直到存在一个其他的特征 $x_k$ 与当前残差有相同的相关性,并将 $x_k$ 加入候选集。</p>
<p>(4).  再计算当前残差 $r$ 在 $(x_j,x_k)$ 的联合最小二乘系数方向上移动来增加 $\beta_j,\beta_k$，直到存在其它的预测变量 $x_l$ 与当前残差的相关性和当前残差与 $(x_j,x_k)$ 的相关性相等。</p>
<p>(5).  这种方式继续直到所有的 $p$ 个预测变量加入到模型中．经过 $\min(n-1,p)$ 步，可以达到全最小二乘的解．</p>
</blockquote>
<p>下面理论推导迭代方向和补偿选取，特征的选取。</p>
<p>先证残差假设已经进行了第 $k$ 次特征选择，当前候选集包含特征 $x_1,x_2,\cdots,x_k$, 当前残差为 $r_k$, 假设 $x_j,j=1,\cdots,k$ 与 $r_k$ 之间的相关性一样，即 $\frac{|\langle x_j,r_k\rangle|}{|x_j|\cdot|r_k|}=\lambda,j=1,\cdots,p$, 由步骤三: 移动方向为联合最小二乘系数方向，设前 $k$ 个特征构成的特征矩阵为 $X_1$ , $X_1$ 关于 $r_k$ 的最小二乘系数为 $\hat{\beta}$, 即当前参数的移动方向为 $\hat{\beta}$, 设移动步长为 $\alpha$ . 令 $u(\alpha)=\alpha X_1\hat{\beta}$ ，则 $r_{k+1}=r_k-u(\alpha)$。</p>
<blockquote>
<p>$\frac{|\langle x_j,r_k-u(\alpha)\rangle|}{|x_j|\cdot|r_k-u(\alpha)|}=\frac{1}{|r_k-u(\alpha)|}\cdot\frac{|\langle x_j,\alpha r_k+(1-\alpha)r_k-u(\alpha)\rangle|}{|x_j|}$</p>
<p>$=\frac{1}{|r_k-u(\alpha)|}\cdot\frac{|\langle x_j,\alpha(r_k-X_1\hat{\beta})+(1-\alpha)r_k\rangle |}{|x_j|}$</p>
<p>$=\frac{(1-\alpha)\lambda|r_k|}{|r_k-u(\alpha)|}$</p>
</blockquote>
<p>其中倒数第二个等式是由于 $\langle x_j,\alpha(r_k-X_1\hat{\beta})\rangle=x_j^T(r_k-X\hat{\beta})=0$.</p>
<p>即对任意 $j,l=1,\cdots,k, $ 有 $\frac{|\langle x_j,r_k-u(\alpha)\rangle|}{|x_j|\cdot|r_k-u(\alpha)|}=\frac{|\langle x_l,r_k-u(\alpha)\rangle|}{|x_l|\cdot|r_k-u(\alpha)|}$，即相关性相同。</p>
<p>由于 $\hat{\beta}=(X_1^TX_1)^{-1}X_1^Tr_k$，因此 $u(\alpha)=\alpha X_1(X_1^TX_1)^{-1}X_1^Tr_k$，则 $X_1^Tu(\alpha)=X_1^Tr_k=\lambda\mathbf{1}$ . 即每次特征选择后，候选集里的所有特征与残差的相关性相等，也就是说 $u(\alpha)$ 为对角线方向，这也是为什么称为最小角回归的原因。</p>
<p>下面计算步长 $\alpha$ :</p>
<p>仍考虑候选集有 $k$ 个特征，令$\mathcal{A}={1,\cdots,k},$ 由于任意候选集里的特征与残差的相关系数相等，设为 $\lambda$, 则有 $\frac{|\langle x_j,r_{k+1}\rangle|}{|x_j|\cdot|r_{k+1}|}=\frac{(1-\alpha)\lambda|r_k|}{|r_{k+1}|}$,对任意 $j=1,\cdots,p$, 由于 $x_j$ 经过中心标准化，则 $|x_j|=1$，所以 $\max_{j\in\mathcal{A}^c}|\langle x_j,r_{k}-u(\alpha)\rangle |=(1-\alpha)\lambda|r_k|$, 因此$|corr(x_j,r_k)-\alpha corr(x_j,X_1\hat{\beta})|=(1-\alpha)\lambda$, 记 $C_j=corr(x_j,r_k),D_j=corr(x_j,X_1\hat{\beta})$, 则可以解的：</p>
<blockquote>
<p>$\alpha=\min _{j \in A^{c}}^{+}\left\{\frac{\lambda-C_{j}}{\lambda-D_{j}}, \frac{\lambda+C_{j}}{\lambda+D_{j}}\right\}$</p>
</blockquote>
<p>若令 $|\langle x_j,r_k\rangle|=\lambda|r_k|=\hat{C}$, $|\langle x_j,r_k\rangle|=C_j$, $|\langle x_j,X_1\hat{\beta}\rangle|=D_j,$ 则可以得到和文献 [8] 中一致的表达方式。</p>
<hr>
<p>最小角回归和Lasso的关系有空再补。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><hr>
<p>[1] T. Hastie, R.J. Tibshirani, Friedman J.H. The Elements of Statistical Learning: Springer[J]. Elements, 2009, 1(3):267-268.</p>
<p>[2] 周志华. 机器学习. 清华大学出版社，2016.1</p>
<p>[3] Tibshirani, Robert. Regression Shrinkage and Selection Via the Lasso[J]. Journal of the Royal Statistical Society, 58(1):267-288.</p>
<p>[4] Stephen Boyd, Lieven Vandenberghe. Convex Optimization[M]. 世界图书出版公司, 2004.</p>
<p>[5] Boyd, Stephen, Lin Xiao, and Almir Mutapcic. “Subgradient methods.” <em>lecture notes of EE392o, Stanford University, Autumn Quarter</em> 2004 (2003): 2004-2005.</p>
<p>[6] Nedic, Angelia, and Dimitri P. Bertsekas. “Incremental subgradient methods for nondifferentiable optimization.” <em>SIAM Journal on Optimization</em> 12.1 (2001): 109-138.</p>
<p>[7] Tseng, Paul. “Convergence of a block coordinate descent method for nondifferentiable minimization.” <em>Journal of optimization theory and applications</em> 109.3 (2001): 475-494.</p>
<p>[8] Efron, Bradley, et al. “Least angle regression.” <em>The Annals of statistics</em> 32.2 (2004): 407-499.</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">会飞的猪</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://shellyandliu.github.io/2020/04/16/Lasso%E5%9B%9E%E5%BD%92/">http://shellyandliu.github.io/2020/04/16/Lasso%E5%9B%9E%E5%BD%92/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://shellyandliu.github.io" target="_blank">会飞的猪</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E7%BB%9F%E8%AE%A1/">统计</a><a class="post-meta__tags" href="/tags/%E4%BC%98%E5%8C%96/">优化</a><a class="post-meta__tags" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">线性回归</a></div><div class="post_share"><div class="social-share" data-image="/img/post.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/weixin.png" alt="微信打赏"/><div class="post-qr-code__desc">微信打赏</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/zhifubao.png" alt="支付宝打赏"/><div class="post-qr-code__desc">支付宝打赏</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/04/16/%E4%B8%BB%E6%88%90%E5%88%86%E5%9B%9E%E5%BD%92%E5%92%8C%E5%81%8F%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E5%9B%9E%E5%BD%92/"><img class="next_cover lazyload" data-src="/img/post.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">主成分回归和偏最小二乘回归</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/04/16/一元线性回归/" title="一元线性回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">一元线性回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/多元线性回归/" title="多元线性回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">多元线性回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/线性回归违背基本假设的情况/" title="线性回归违背基本假设的情况"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">线性回归违背基本假设的情况</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/线性回归的变量选择/" title="线性回归的变量选择"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">线性回归的变量选择</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/带约束的线性回归问题与多重共线性/" title="带约束的线性回归问题与多重共线性"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">带约束的线性回归问题与多重共线性</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/岭回归和广义岭回归/" title="岭回归和广义岭回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">岭回归和广义岭回归</div></div></a></div></div><div class="clear_both"></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 会飞的猪</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">欢迎来到我的博客</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>
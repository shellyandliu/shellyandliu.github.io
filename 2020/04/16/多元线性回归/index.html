<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="会飞的猪">





<link rel="stylesheet" href="/fonts/iconfont_new/iconfont.css">


<title>多元线性回归 | 会飞的猪</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">会飞的猪</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/categories">分类</a>
                
                    <a class="menu-item" href="/tags">标签</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">会飞的猪</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/categories">分类</a>
                
                    <a class="menu-item" href="/tags">标签</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">展开目录</a>
        <a onclick="go_top()">回到顶部</a>
        <a onclick="go_bottom()">到达底部</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">多元线性回归</h1>
            
                <div class="post-meta">
                    
                        作者: <a itemprop="author" rel="author" href="/">会飞的猪</a>
                    

                    
                        <span class="post-time">
                        时间: <a href="#">四月 16, 2020&nbsp;&nbsp;16:04:44</a>
                        </span>
                    
                    
                        <span class="post-category">
                    类别:
                            
                                <a href="/categories/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">-线性回归</a>
                            
                        </span>
                    
                    
                        <span class="post-time">
                            <span class="post-meta-item-icon">
                              <i class="fa fa-keyboard-o"></i>
                              <span class="post-meta-item-text">  字数统计: </span>
                              <span class="post-count"><a href="#">3.9k字</a></span>
                            </span>
                        </span>
                        <span class="post-time">
                            <span class="post-meta-item-icon">
                            <i class="fa fa-hourglass-half"></i>
                            <span class="post-meta-item-text">  阅读时长: </span>
                            <span class="post-count"><a href="#">18分</a></span>
                            </span>
                        </span>
                    

                    
                </div>
            
        </header>

        <div class="post-content">
            <h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p>多元线性回归的基本形式：$y=\beta_{0}+\beta_{1} x_{1}+\dots+\beta_px_p+\epsilon$</p>
<p>若有n个样本，则用矩阵形式表达可得：$y=X\beta+\epsilon$</p>
<blockquote>
<p>假设：</p>
<p>(1) $r(X)=p+1,$且$x_1,\dots,x_p$不是随机变量。</p>
<p>(2)G-M条件</p>
<p>(3)$\epsilon\sim N(0,\sigma^2I_n)$</p>
</blockquote>
<p>目标函数为均方误差函数：</p>
<script type="math/tex; mode=display">
L(\beta_0,\beta_1,\dots,\beta_p)=\frac{1}{2n}\|X \beta-y\|_{2}^2</script><p>对其求偏导可得：$e^TX=(y-X\hat{\beta})^TX=0$,因此</p>
<blockquote>
<p>$\hat{\beta}=(X^TX)^{-1}X^Ty$</p>
</blockquote>
<p>定义矩阵$H=X(X^TX)^{-1}X^T$为帽子矩阵,若观测值为$y$，则估计值就为$Hy$,帽子矩阵为<strong>对称幂等矩阵</strong></p>
<h3 id="残差的协方差阵"><a href="#残差的协方差阵" class="headerlink" title="残差的协方差阵"></a>残差的协方差阵</h3><blockquote>
<p>$\operatorname{cov}(e, e)=\operatorname{cov}((I-H) y,(I-H) y)=(I-H) \operatorname{cov}(y, y)(I-H)^{T}=(I-H) \sigma^{2} I_{n}(I-H)^{T}=\sigma^{2}(I-H)(I-H)=\sigma^{2}(I-H)$</p>
</blockquote>
<h3 id="sigma-2-的无偏估计"><a href="#sigma-2-的无偏估计" class="headerlink" title="$\sigma^2$的无偏估计"></a>$\sigma^2$的无偏估计</h3><p>容易计算矩阵$(I-H)X=0$</p>
<p>因此</p>
<script type="math/tex; mode=display">
\begin{array}{l}e^{T} e=[(I-H) y]^{T}(I-H) y=y^{T}(I-H) y \\=(X \beta+\epsilon)^{T}(I-H)(X \beta+\epsilon)=\left(\beta^{T} X^{T}+\epsilon^{T}\right)(I-H)(X \beta+\epsilon) \\=\beta^{T} X^{T}(I-H) X \beta+\beta^{T} X^{T}(I-H) \epsilon+\epsilon^{T}(I-H) X \beta+\epsilon^{T}(I-H) \epsilon\\=\epsilon^{T}(I-H) \epsilon\end{array}​</script><p>则对$e^Te$求期望可得：</p>
<script type="math/tex; mode=display">
\begin{aligned}&E(e^Te)=E\left(\epsilon^{T}(I-H) \epsilon\right)=E\left(\operatorname{tr}\left(\epsilon^{T}(I-H) \epsilon\right)\right.\\&=\operatorname{tr}\left(E\left((I-H) \epsilon \epsilon^{T}\right)\right)=\operatorname{tr}\left((I-H) E\left(\epsilon \epsilon^{T}\right)\right)\\&=\sigma^{2} \operatorname{tr}(I-H)=\sigma^{2}(n-p-1)\end{aligned}</script><p>由于在多元回归中$e^Te=SSE$,所以$\sigma^2$的无偏估计为：</p>
<blockquote>
<p>$\hat{\sigma}^2=\frac{SSE}{n-p-1}$</p>
</blockquote>
<h2 id="参数的性质"><a href="#参数的性质" class="headerlink" title="参数的性质"></a>参数的性质</h2><blockquote>
<p>$\hat{\beta}$ 是 $y$ 的一个线性变换</p>
<p>$\hat{\beta}$ 是 $\beta$ 的无偏估计</p>
<p>$\hat{\sigma}^2$ 是 $\sigma^2$ 的无偏估计</p>
<p>$cov(\hat{\beta})=\sigma^2(X^TX)^{-1}$</p>
<p>$cov(\hat{\beta},e)=0$</p>
</blockquote>
<h3 id="最佳线性无偏估计"><a href="#最佳线性无偏估计" class="headerlink" title="最佳线性无偏估计"></a>最佳线性无偏估计</h3><blockquote>
<p>定理：设 $y\sim N(X\beta,\sigma^2I_n)$，则 $\beta$ 的任一线性函数 $c^T\beta$ 的最小方差线性无偏估计(BLUE)为 $c^T\beta$ ，其中$c$是常数向量。</p>
</blockquote>
<h3 id="拟合优度"><a href="#拟合优度" class="headerlink" title="拟合优度"></a>拟合优度</h3><blockquote>
<p>定义：<strong>决定系数</strong>：$R^2=\frac{SSR}{SST}$</p>
<p><strong>样本复相关系数</strong>：$R=\sqrt{\frac{SSR}{SST}}$, 表示因变量 $ y$ 与全体自变量之间的线性关系。</p>
<p><strong>偏相关系数</strong>：$r_{y 1 ; 2,3, \cdots, p}^{2}=\frac{S S E\left(x_{2}, \ldots, x_{p}\right)-S S E\left(x_{1}, \ldots, x_{p}\right)}{S S E\left(x_{2}, \ldots, x_{p}\right)}$，用来衡量回归系数显著性(即在已经有了第2，3，…，p个自变量后，新添加第1个自变量后，$SSE$ 到底下降了多少。)</p>
<p><strong>偏回归平方和</strong>: $\Delta S S R_{(j)}=S S R-S S R_{(j)}$，其中 $SSR_{(i)}$ 表示在原本 $p$ 个变量中剔除 $x_j$ 后的剩下 $p-1$ 个变量的回归后的残差平方和。同理可定义$SSE_{(i)}$</p>
</blockquote>
<h2 id="显著性检验"><a href="#显著性检验" class="headerlink" title="显著性检验"></a>显著性检验</h2><h3 id="方程显著性检验"><a href="#方程显著性检验" class="headerlink" title="方程显著性检验"></a>方程显著性检验</h3><p>目的：<strong>检验这个回归方程是否有效。</strong></p>
<p>原假设为：</p>
<blockquote>
<p>$H_0:\beta_1=\beta_2=\dots=\beta_p=0$</p>
<p>$SSR=\sum(\hat{y}_i-\bar{y})^2=\sum\hat{y}^2-n\bar{y}^2=Y^T\left[X(X^TX)^{-1}X^T-\frac 1n11^T\right]Y$</p>
<p>$SSE=\sum(y_i-\hat y_i)^2=Y^T(I_n-X(X^TX)^{-1}X^T)Y$</p>
</blockquote>
<p>其中，令$J=X(X^TX)^{-1}X^T-\frac 1n11^T$,显然该矩阵为幂等矩阵，即秩和迹相同。</p>
<blockquote>
<p>引理1：设$X\sim N_p(\mu,I_p)$，$A$对称，那么$X^TAX\sim \chi^2(r,\mu^TA\mu)$的充要条件是$A$幂等，且$r(A)=r$。</p>
<p>推论1：设$X\sim N_p(\mu,I_p)$，$A$对称，那么$X^TAX\sim \chi^2(r)$的充要条件是$A$幂等，且$r(A)=r,A\mu=0$。</p>
<p>推论2：设$X\sim N_p(\mu,\Sigma),\Sigma&gt;0$，$A$对称，那么$X^TAX\sim \chi^2(r,\mu^TA\mu)$的充要条件是$A$幂等，且$A\Sigma A=A,r(A)=r$。</p>
<p>推论3：若$Y\sim N(X\beta,\sigma^2I_n),SSE=Y^T(I_n-X(X^TX)^{-1}X^T)Y$，则$\frac 1{\sigma^2}SSE \sim \chi^2(n-r(X))$</p>
</blockquote>
<p>因此由引理1可以得到，$SSR\sim \sigma^2\chi^2\left(r(X)-1,\frac{1}{\sigma^2}\beta^TX^T\left[X(X^TX)^{-1}X^T-\frac 1n11^T\right]X\beta\right)$,由推论3，要想证明$SSR$服从中心的卡方分布，仅需说明$\frac{1}{\sigma^2}\beta^TX^T(I_n-\frac 1n11^T)X\beta=0$,由于再$H_0$得假设下，$X\beta=\beta_01,\beta^TX^T=\beta_01^T$，显然成立。则有下面结论：</p>
<blockquote>
<p>$\frac{SSR}{\sigma^2}\sim \chi^2(p)$</p>
<p>$\frac 1{\sigma^2}SSE \sim \chi^2(n-p-1)$</p>
</blockquote>
<p>下面给出一个有用得定理及其相关推论p</p>
<blockquote>
<p>Cochran定理：</p>
<p>设$X\sim N_p(\mu,I_p),X^TAX=X^TA_1X+X^TA_2X\sim \chi^2(r,\lambda),X^TA_1X\sim\chi^2(s,\lambda_1),A_2\geq 0$其中$\lambda=\mu^TA\mu,\lambda_1=\mu^TA_1\mu$，那么有以下结论：</p>
<p>(1)$X^TA_2X\sim \chi^2(r-s,\lambda_2),\lambda_2=\mu^TA_2\mu$</p>
<p>(2)$X^TA_1X,X^TA_2X$独立</p>
<p>(3)$A_1A_2=0$</p>
<p>推论4：设$X\sim N_p(\mu,I_p)$，$X^TA_1X，X^TA_2X$都服从$\chi^2$分布，那么二者独立得充要条件是$A_1A_2=0$.</p>
<p>推论5：设$X\sim N_p(\mu,\Sigma),X^TAX=X^TA_1X+X^TA_2X\sim \chi^2(r,\lambda),X^TA_1X\sim\chi^2(s,\lambda_1),A_2\geq 0$，那么有:</p>
<p>(1)$X^TA_2X\sim \chi^2(r-s,\lambda_2)$</p>
<p>(2)$X^TA_1X,X^TA_2X$独立</p>
<p>(3)$A_1\Sigma A_2=0$</p>
<p>推论6：设$X\sim N_p(\mu,\Sigma)$，$X^TA_1X，X^TA_2X$都服从$\chi^2$分布，那么二者独立得充要条件是$A_1\Sigma  A_2=0$.</p>
</blockquote>
<p>由于$\left[I_{n}-X\left(X^{T} X\right)^{-1} X^{T}\right]\left[X\left(X^{T} X\right)^{-1} X^{T}-\frac{1}{n} \mathbf{1} \mathbf{1}^{T}\right]=0$，因此可以得到下面结论：</p>
<blockquote>
<p>$SSR$和$SSE$独立</p>
</blockquote>
<p>因此可以构造F检验：</p>
<blockquote>
<p>$\frac{SSR/p}{SSE/(n-p-1)}\sim F(p,n-p-1)$</p>
</blockquote>
<p>由方差分析，当 $F$ 较小时，应该接受 $H_0$.</p>
<h3 id="系数显著性检验"><a href="#系数显著性检验" class="headerlink" title="系数显著性检验"></a>系数显著性检验</h3><p>原假设为：</p>
<blockquote>
<p>$H_{0j}:\beta_j=0$</p>
</blockquote>
<p>根据无偏性和$cov(\hat{\beta})=\sigma^2(X^TX)^{-1}$,可以得到：$\hat{\beta}_j\sim N(\beta_j,c_{jj}\sigma^2)，$其中</p>
<p>$c_{jj}$是$(X^TX)^{-1}$的对角线元素。同样的，由于$SSE/\sigma^2\sim \chi^2(n-p-1)$，则可以构建t分布检验：</p>
<blockquote>
<p>$t_j=\frac{\hat{\beta}_j}{\sqrt{c_{jj}}\sigma} \cdot\sqrt{\frac{\sigma^2\cdot(n-p-1)}{SSE}}=\frac{\hat{\beta}_j}{\sqrt{c_{jj}}\hat{\sigma}}\sim t(n-p-1)$</p>
</blockquote>
<h3 id="部分系数显著性检验"><a href="#部分系数显著性检验" class="headerlink" title="部分系数显著性检验"></a>部分系数显著性检验</h3><p>用分块矩阵表示模型为：</p>
<script type="math/tex; mode=display">
y=X\beta+\epsilon=\left[X_1\quad X_2\right]\left[\begin{array}{c}\beta_1 \\ \beta_2\end{array}\right]+\epsilon</script><p> 其中 $\beta_1,\beta_2,X_1,X_2$ 均表示分块矩阵。</p>
<p>原假设为：</p>
<blockquote>
<p>$H_0:\beta_2=0$</p>
</blockquote>
<p>令 $H=X(X^TX)^{-1}X^T, H_1=X_1(X_1^TX_1)^{-1}X_1^T$, 则可以得到：</p>
<blockquote>
<p>$SSE_1-SSE=Y^T(H-H_1)Y$</p>
</blockquote>
<p>由于:</p>
<script type="math/tex; mode=display">
\left(X^{\prime} X\right)^{-1}=\left[\begin{array}{cc}X_{1}^{\prime} X_{1} & X_{1}^{\prime} X_{2} \\ X_{2}^{\prime} X_{1} & X_{2}^{\prime} X_{2}\end{array}\right]^{-1}=\left[\begin{array}{cc}\left(X_{1}^{\prime} X_{1}\right)^{-1} & 0 \\ 0 & 0\end{array}\right]+\left[\begin{array}{c}-\left(X_{1}^{\prime} X_{1}\right)^{-1} X_{1}^{\prime} X_{2} \\ I\end{array}\right] \Sigma_{22 \cdot 1}^{-1}\left[-X_{2}^{\prime} X_{1}\left(X_{1}^{\prime} X_{1}\right)^{-1} \quad I\right]</script><p>，其中 $\Sigma_{22 \cdot 1}^{-1}=X_2’(I-H_1)X_2$，因此：</p>
<blockquote>
<p>$H-H_1=(I-H_1)X_2[X_2^T(I-H_1)X_2]^{-1}X_2^T(I-H_1)$</p>
</blockquote>
<p>易证 $H-H_1$ 是对称幂等矩阵，因此：</p>
<blockquote>
<p>$\frac{SSE_1-SSE}{\sigma^2}\sim \chi^2(r,\lambda)$</p>
<p>其中 $r=tr(H-H_1)=r(X)-r(X_1)$, $\lambda=\frac 1{\sigma^2}\beta^TX^T(H-H_1)X\beta=\frac 1{\sigma^2}\beta_2^TX_2^T(I-H_1)X_2\beta_2=0$</p>
</blockquote>
<p>由于$(I-H)(H-H_1)=0$ , 可以得到 $SSE_1-SSE$ 与 $SSE$ 独立，因此可以构造统计量：</p>
<blockquote>
<p>$F=\frac{(SSE_1-SSE)/(r(X)-r(X_1))}{SSE/(n-r(X))}\sim F(r(X_2),n-r(X))$</p>
</blockquote>
<p>当 $F$ 较小时，应该接受 $H_0$.</p>
<h3 id="中心化，标准化"><a href="#中心化，标准化" class="headerlink" title="中心化，标准化"></a>中心化，标准化</h3><p><strong>中心化</strong>是指<strong>将所有数据点平移，使得回归的远点都变成数据的均值点</strong>。</p>
<p>则所需做的变换为：$x_{ij}’=x_{ij}-\bar{x}_j,j=1,\dots,p$ 和$y_i’=y_i-\bar{y}$。</p>
<p>这样就可以得到没有常数项的中心化方程：</p>
<blockquote>
<p>$\hat{y}^{\prime}=\hat{\beta}_{1} x_{1}^{\prime}+\cdots+\hat{\beta}_{p} x_{p}^{\prime}$</p>
</blockquote>
<p>实际上中心化相当于在原回归方程$y=X\beta+\epsilon$ 上乘了一个中心化矩阵$I_n-\frac 1n 11^T$ 。而由于$\epsilon$ 被乘了一个矩阵，它的独立同分布条件就不一定能够满足。</p>
<p><strong>标准化</strong>就是做如下变换：</p>
<blockquote>
<p>$x_{i j}^{*}=\frac{x_{ij}-\bar{x}_{j}}{\sqrt{L_{j j}}},$ $ y_{i}^{*}=\frac{y_{i}-\bar{y}}{\sqrt{L_{y y}}}, i=1, \cdots, n$</p>
<p>其中$L_{jj}=\sum_{i=1}^n(x_{ij}-\bar{x}_j)^2$ </p>
</blockquote>
<p>标准化后的离差平方和是1.</p>
<h3 id="样本相关阵"><a href="#样本相关阵" class="headerlink" title="样本相关阵"></a>样本相关阵</h3><p>定义相关系数：</p>
<blockquote>
<p>$r_{ij}=\frac{L_{ij}}{\sqrt{L_{ii}L_{jj}}}$</p>
</blockquote>
<p>将相关系数写成一个矩阵，可以得到样本相关矩阵：</p>
<blockquote>
<script type="math/tex; mode=display">
r=\left[\begin{array}{cccc}1 & r_{12} & \cdots & r_{1 p} \\ r_{21} & 1 & \cdots & r_{2 p} \\ \vdots & \vdots & \ddots & \vdots \\ r_{p 1} & r_{p 2} & \cdots & 1\end{array}\right]</script></blockquote>
<p>设$X^<em>$是标准化后的样本矩阵，则可以验证:$r=(X^</em>)^TX^*$</p>
<p>在高维数据的处理中(<strong>自变量的个数大于或者接近数据的个数</strong>)，会使用<strong>增广相关矩阵</strong>来筛掉一部分变量。：</p>
<blockquote>
<script type="math/tex; mode=display">
r=\left[\begin{array}{ccccc}1 & r_{y 1} & r_{y 2} & \cdots & r_{y p} \\ r_{1 y} & 1 & r_{12} & \cdots & r_{2 p} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ r_{p y} & r_{p 1} & \cdots & r_{p, p-1} & 1\end{array}\right]</script></blockquote>
<h2 id="矩阵的条件数与QR分解"><a href="#矩阵的条件数与QR分解" class="headerlink" title="矩阵的条件数与QR分解"></a>矩阵的条件数与QR分解</h2><h3 id="矩阵的条件数"><a href="#矩阵的条件数" class="headerlink" title="矩阵的条件数"></a>矩阵的条件数</h3><p>对于线性系统 $Ax=b$, $A$ 为系数矩阵，若通过某种方法得到的解为 $\hat{x}$, 而精确解为 $x_e$, 则相对误差为 $\frac{|x_e-\hat{x}|}{|x_e|}$ ,由于不知道精确解 $x_e$ ，因此可以通过残差 $\hat{e}=b-A\hat{x}$ 来判断解是否准确，但在某些情况下，尽管 $\hat{e}$ 的范数很小， $\hat{x}-x_e$ 仍相差很远，因此引入矩阵的条件数的概念。</p>
<p>在 $A$ 可逆的情况下: $x_e-\hat{x}=A^{-1}\hat{e}$ ,选择对应的向量矩阵范数，可以得到 </p>
<blockquote>
<p>$|x_e-\hat{x}|=|A^{-1}\hat{e}|\le |A^{-1}||\hat{e}|$.</p>
</blockquote>
<p>又 $|b|=|Ax_e|\le|A||x_e|$, 可以得到 $\frac{1}{|x_e|}\le\frac{|A|}{|b|}$</p>
<blockquote>
<p>$\frac{|x_e-\hat{x}|}{|x_e|}\le|A||A^{-1}|\frac{|\hat{e}|}{|b|}$</p>
</blockquote>
<p>则可以定义<strong>矩阵 $A$ 的条件数为：$\kappa(\mathbf{A})=|\mathbf{A}|\left|\mathbf{A}^{-1}\right|$</strong></p>
<blockquote>
<p>$\frac{|x_e-\hat{x}|}{|x_e|}\le\kappa(A)\frac{|\hat{e}|}{|b|}$</p>
</blockquote>
<p><strong>线性系统解的相对误差由残差和条件数约束。当矩阵接近奇异时，条件数较大，解的相对误差可能较大</strong></p>
<blockquote>
<p>引理：正交矩阵的条件数为1，对称正定矩阵的条件数为 $\frac{\lambda_1}{\lambda_n}$，其中 $\lambda_1$ 为最大特征值，$\lambda_n$ 为最小特征值。</p>
</blockquote>
<p>当 $A$ 为$n\times p$ 阶矩阵，$n&gt;p$, 同样可以得到 $\kappa(A)=|A|\left|A^{\dagger}\right|$, 设 $A$ 的奇异值为 $\sigma_1\ge\sigma_2\ge\dots\ge\sigma_p$,则 $\kappa(A)=\frac{\sigma_1}{\sigma_p}$</p>
<p>现在考虑最小二乘得到的正规方程 $X’X\beta=A’y$ 的条件数</p>
<blockquote>
<p>$\kappa(X’X)=\frac{\lambda_1}{\lambda_p}=\frac{\sigma_1^2}{\sigma_p^2}=\kappa(X)^2$</p>
</blockquote>
<p>即直接解正规方程得到的解的相对误差由 $\kappa(A)^2$ 约束。因此当 $\kappa(A)^2$ 较大时，即使残差很小时，解的x相对误差也可能很大。</p>
<h3 id="最小二乘问题的QR分解"><a href="#最小二乘问题的QR分解" class="headerlink" title="最小二乘问题的QR分解"></a>最小二乘问题的QR分解</h3><p>记 $X_{n\times p}$ 为列满秩矩阵，$n&gt;p$， 对 $X$ 进行QR分解得：</p>
<blockquote>
<script type="math/tex; mode=display">
X_{n \times p}=Q_{n \times n}\left[\begin{array}{l}R_{p \times p} \\ 0_{(n-p) \times p}\end{array}\right]</script></blockquote>
<p>其中 $Q$ 为正交矩阵， $R$ 为上三角矩阵</p>
<blockquote>
<p>其中 $Q$ 为正交矩阵， $R$ 为上三角矩阵</p>
</blockquote>
<p>则对于最小二乘问题 $\min_{\beta}|e|_2^2=|y-X\beta|^2_2$ 有</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}\|e\|_{2}^{2} &=\left\|y-Q\left[\begin{array}{l}R \\ 0\end{array}\right] \beta\right\|_{2}^{2}=\left\|Q^T y-Q^T Q\left[\begin{array}{l}R \\ 0\end{array}\right] \beta\right\|_{2}^{2} \\ &=\left\|[Q_1\quad Q_2]^T y-\left[\begin{array}{l}R \\ 0\end{array}\right] \beta\right\|_{2}^{2}=\left\|\left[\begin{array}{l}Q_1^Ty \\ Q_2^Ty\end{array}\right]-\left[\begin{array}{l}R \beta \\ 0\end{array}\right]\right\|_{2}^{2} \\ &=\|Q_1^Ty-R \beta\|_{2}^{2}+\|Q_2^Ty\|_{2}^{2} \end{aligned}</script></blockquote>
<p>则 $\min_{\beta}|e|_2^2$ 即为优化 $\min_{\beta}|Q_1^Ty-R \beta|_{2}^{2}$, 由于条件数 $\kappa(R)=\kappa(A)$, 所以QR分解下得误差由 $\kappa(A)$ 约束。</p>
<h2 id="施密特正交化与QR分解"><a href="#施密特正交化与QR分解" class="headerlink" title="施密特正交化与QR分解"></a>施密特正交化与QR分解</h2><h3 id="线性回归的施密特正交化"><a href="#线性回归的施密特正交化" class="headerlink" title="线性回归的施密特正交化"></a>线性回归的施密特正交化</h3><p>首先，若假设变量 $x_1,x_2,\cdots,x_p$ 是彼此正交的，也就是说对于任意的 $j\ne k$, 有 $\left\langle x_{j}, x_{k}\right\rangle=0$. 于是很容易由最小二乘估计出 $\hat{\beta}_j=\langle x_j,y\rangle/\langle x_j,x_j\rangle$, 即是说当输入变量正交时，它们对模型中的其他参数的估计没有影响。但在实际中，完全正交的数据几乎不会产生。因此考虑将其正交化。</p>
<p>考虑单变量的线性回归问题，可知其最小二乘估计为 $\hat{\beta}=\frac{\langle x-\bar{x}\mathbf{1},y\rangle}{\langle x-\bar{x}\mathbf{1},x-\bar{x}\mathbf{1}\rangle}$ , 则可以将其看作两次应用。分别是</p>
<blockquote>
<ol>
<li>在 $\mathbf{1}$ 上回归残生残差 $z=x-\bar{x}\mathbf{1}$;</li>
<li>在残差 $z$ 上回归 $y$ 得到系数 $\hat{\beta}$, 产生系数 $\hat{\gamma}=\langle x,y\rangle/\langle z,z\rangle$. </li>
</ol>
</blockquote>
<p>即第一步对 $x$ 作关于 $\mathbf{1}$ 的正交化，第二步是一个用正交预测变量 $\mathbf{1}$ 和 $z$ 的简单的单变量回归。将其推广到 $p$ 个变量可以得到下面的算法：</p>
<blockquote>
<ol>
<li>初始化 $z_0=x_0=1$.</li>
<li>对于 $j=1,2,\cdots,p$, 在 $z_0,z_1,\cdots,z_{j-1}$ 上关于 $x_j$ 做线性回归，得到系数为 $\hat{\gamma}_{lj}=\langle z_l,x_j\rangle/\langle z_l,z_l\rangle, l=1,\cdots,j-1$, 以及残差向量 $z_j=x_j-\sum_{k=0}^{j-1}\hat{\gamma}_{kj}z_k$.</li>
<li>在 $z_p$ 上关于 $y$ 做线性回归，得到参数 $\hat{\beta}_p$ 的估计。</li>
</ol>
</blockquote>
<p>按照算法所述 $\hat{\beta}_p=\frac{\langle z_p,y\rangle}{\langle z_p,z_p\rangle}$ ，这是容易证明的：由于 $z_j$ 的构造方式，容易证明 $cov(z_j,z_k)=0$ 对任意的 $j\ne k$ 成立，即 $z_j$ 是正交向量，又因为每个 $x_j$ 都可以写成 $z_k,k\le j$ 的线性组合，则 $z_j$ 构成了 $p+1$ 维变量组成的线性空间的一组基，并且从 $z_p$ 的形式上可以看出 $z_p$ 仅与 $x_p$ 相关，因此 $y$ 在 $z_p$ 上的正交投影的长度就等价于 $\hat{y}$ 在 $x_p$ 上的正交投影的长度。</p>
<p>可以通过将 $x_j$ 任意重排，进而计算出所有的参数估计 $\beta_j$，而从最终的参数估计表达式也可以看出：</p>
<blockquote>
<p>多重回归系数 $\hat{\beta}_j$ 表示 $x_j$ 剔除 $x_0,x_1,\cdots,x_p$ 对它的影响后，与 $y$ 的贡献大小。</p>
</blockquote>
<p>由 $\hat{\beta}_p=\frac{\langle z_p,y\rangle}{\langle z_p,z_p\rangle}$,也可以得出一个新的 $\hat{\beta}_p$ 的方差表达式：</p>
<blockquote>
<p>$var(\hat{\beta}_p)=\frac{\sigma^2}{|z_p|_2^2}$</p>
</blockquote>
<p>即估计 $\hat{\beta}_p$ 的精度取决于残差向量 $z_p$ 的长度；它表示 $x_p$ 不能被其他 $x_k$ 解释的程度．即 $x_p$ 约不能被其他变量解释，方差越小。</p>
<p>实际上，上述算法所作的过程就是对 $X$ 列向量的施密特正交化，也被称维多重回归的施密特正交化。</p>
<h3 id="QR-分解与参数-hat-beta-的求解"><a href="#QR-分解与参数-hat-beta-的求解" class="headerlink" title="QR 分解与参数 $\hat{\beta}$ 的求解"></a>QR 分解与参数 $\hat{\beta}$ 的求解</h3><p>上述过程可以用矩阵形式简化为 </p>
<blockquote>
<p>$X=Z\Gamma$</p>
<p>其中 $Z$ 为 $z_j$ 作为列向量的矩阵，$\Gamma$ 为值为 $\hat{\gamma_{kj}}$ 的上三角矩阵。</p>
</blockquote>
<p>令 $D=diag(|z_1|,|z_2|,\cdots,|z_p|)$，则：</p>
<blockquote>
<p>$X=ZD^{-1}D\Gamma=QR$</p>
<p>$Q$ 为正交矩阵， $R$ 为上三角矩阵。</p>
</blockquote>
<p>即是 $X$ 的 QR 分解。由此最小二乘解为 $\hat{\beta}=R^{-1}Q^Ty$ ,从该方程也显然能够求出 $\hat{\beta}_p=\frac{\langle z_p,y\rangle}{\langle z_p,z_p\rangle}$</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><hr>
<p>[1] 何晓群，刘文卿. 应用回归分析[M]. 中国人民大学出版社，2015.3</p>
<p>[2] 孙荣恒. 应用数理统计.科学出版社，1998</p>
<p>[3] 陈希孺，王松桂. 近代回归分析-原理方法及应用.安徽教育出版社，1987</p>
<p>[4] T. Hastie, R.J. Tibshirani, Friedman J.H. The Elements of Statistical Learning: Springer[J]. Elements, 2009, 1(3):267-268.</p>
<p>[5] <a href="https://zhuanlan.zhihu.com/p/48541799" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/48541799</a></p>
<p>[6] <a href="https://zhuanlan.zhihu.com/p/49276967" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49276967</a></p>
<p>[7] <a href="https://zhuanlan.zhihu.com/p/60743204" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/60743204</a></p>

        </div>
        
            <div style="text-align:center;color: rgb(125,125,125);font-size:14px;">
               -------------本文结束 感谢您的阅读-------------
            </div>
        
        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>文章作者:</span>
                        <span>会飞的猪</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>文章链接:</span>
                        <span><a href="http://shellyandliu.github.io/2020/04/16/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">http://shellyandliu.github.io/2020/04/16/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>版权声明:</span>
                        <span>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC-BY-NC-4.0</a> ， 转载请注明来自 会飞的猪！</span>
                    </p>
                
            </section>
        
        <section class="post-tags">
            <div>
                <span>标签:</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"> <i class="iconfont icon-tags"></i> 机器学习</a>
                    
                        <a href="/tags/%E7%BB%9F%E8%AE%A1/"> <i class="iconfont icon-tags"></i> 统计</a>
                    
                        <a href="/tags/%E4%BC%98%E5%8C%96/"> <i class="iconfont icon-tags"></i> 优化</a>
                    
                        <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"> <i class="iconfont icon-tags"></i> 线性回归</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">返回</a>
                <span> /  </span>
                <a href="/">首页</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/04/16/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E8%BF%9D%E8%83%8C%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE%E7%9A%84%E6%83%85%E5%86%B5/">线性回归违背基本假设的情况</a>
            
            
            <a class="next" rel="next" href="/2020/04/16/%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">一元线性回归</a>
            
        </section>
        <div class="post-content">
            <section style="font-size: 20px;font-weight: 700;margin-bottom: 10px;margin-top: 20px;">
                <i class="iconfont icon-comments"></i>
                <span>
                    评论
                </span>
            </section>
        </div>
        
            <section id="comments" class="comments">
              <style>
                .comments{margin:0px;padding:10px;background:#fff}
                @media screen and (max-width:800px){.comments{margin:auto;padding:10px;background:#fff}}
              </style>
              <div class="valine_comment"></div>
<!--����js����</body>֮ǰ���뼴��-->
<!--Leancloud ������:-->
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<!--Valine �ĺ��Ĵ����-->
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
  new Valine({
      el: '.valine_comment',
      app_id: 'bCvyxhtL2V59mmqAp81lAGob-gzGzoHsz#Leancloud应用的appId',
      app_key: 'J4zVKQfUjhToGJwBfLcoOHOv',
      placeholder: '欢迎大家评论交流~',
      notify: 'true',
      verify: 'true',
    });
</script>
            </section>
        
        

    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 会飞的猪 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>

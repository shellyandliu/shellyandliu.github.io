<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="会飞的猪">





<link rel="stylesheet" href="/fonts/iconfont_new/iconfont.css">


<title>岭回归和广义岭回归 | 会飞的猪</title>



    <link rel="icon" href="/image/longmao.jpg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">会飞的猪</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/categories">分类</a>
                
                    <a class="menu-item" href="/tags">标签</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">会飞的猪</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/categories">分类</a>
                
                    <a class="menu-item" href="/tags">标签</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">展开目录</a>
        <a onclick="go_top()">回到顶部</a>
        <a onclick="go_bottom()">到达底部</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">岭回归和广义岭回归</h1>
            
                <div class="post-meta">
                    
                        作者: <a itemprop="author" rel="author" href="/">会飞的猪</a>
                    

                    
                        <span class="post-time">
                        时间: <a href="#">四月 16, 2020&nbsp;&nbsp;17:40:15</a>
                        </span>
                    
                    
                        <span class="post-category">
                    类别:
                            
                                <a href="/categories/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">-线性回归</a>
                            
                        </span>
                    
                    
                        <span class="post-time">
                            <span class="post-meta-item-icon">
                              <i class="fa fa-keyboard-o"></i>
                              <span class="post-meta-item-text">  字数统计: </span>
                              <span class="post-count"><a href="#">2.3k字</a></span>
                            </span>
                        </span>
                        <span class="post-time">
                            <span class="post-meta-item-icon">
                            <i class="fa fa-hourglass-half"></i>
                            <span class="post-meta-item-text">  阅读时长: </span>
                            <span class="post-count"><a href="#">11分</a></span>
                            </span>
                        </span>
                    

                    
                </div>
            
        </header>

        <div class="post-content">
            <h2 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h2><blockquote>
<p>称 $\hat{\beta}(k)=(X’X+kI)^{-1}X’y$ 为回归参数的岭回归估计，其中 $k$ 为岭参数。</p>
</blockquote>
<h3 id="岭迹分析"><a href="#岭迹分析" class="headerlink" title="岭迹分析"></a>岭迹分析</h3><p>即通过分析 $\hat{\beta}_i(k)$ 关于 $k$ 的函数变化趋势，来分析因素 $i$ 是否符合要求。</p>
<h3 id="岭回归的理论性质"><a href="#岭回归的理论性质" class="headerlink" title="岭回归的理论性质"></a>岭回归的理论性质</h3><blockquote>
<p>引理1：$\hat{\beta}(k)=A_k\hat{\beta}$, 其中 $A_k=(X’X+kI)^{-1}X’X$</p>
<p>引理2：$\hat{\beta}(k)$ 是 $\beta$ 的有偏估计。</p>
<p>引理3：对于任意的 $k&gt;0, |\hat{\beta}|\ne 0$, 总会有 $|\hat{\beta}(k)|&lt;|\hat{\beta}|$</p>
<p>引理4：存在 $k&gt;0$, 使得 $MSE(\hat{\beta}(k))&lt;MSE(\hat{\beta})$</p>
</blockquote>
<p><strong>引理1：</strong></p>
<blockquote>
<p>$\hat{\beta}(k)=\left(X^{\prime} X+k I\right)^{-1} X^{\prime} Y=\left(X^{\prime} X+k I\right)^{-1}\left[\left(X^{\prime} X\right)\left(X^{\prime} X\right)^{-1}\right] X^{\prime} Y=\left(X^{\prime} X+k I\right)^{-1} X^{\prime} X\left[\left(X^{\prime} X\right)^{-1} X^{\prime} Y\right]=A_{k} \hat{\beta}$</p>
</blockquote>
<p><strong>引理2：</strong></p>
<p>由于 $E(\hat{\beta}(k))=A_k\beta$，当且仅当 $k=0$ 时是无偏估计。</p>
<p><strong>引理3：</strong></p>
<p>考虑使用典则形式，将回归模型写为 $Y=\alpha1+X\beta+\epsilon$，假设矩阵 $X$ 已经标准化。则有 $\hat{\alpha}=\bar{Y},\hat{\beta}=(X’X)^{-1}X’Y$.设 $\lambda1\ge\dots\ge\lambda_p$ 为 $X’X$ 的特征根，$\phi_1,\dots,\phi_p$ 为对应的标准正交特征向量，记 $\Phi=(\phi_1,\dots,\phi_p),\Lambda=diag(\lambda_1,\dots,\lambda_p),$ 则会有：</p>
<blockquote>
<p>$Y=\alpha_01+Z\alpha+\epsilon$</p>
<p>其中 $Z=X\Phi,\alpha=\Phi’\beta$</p>
</blockquote>
<p>显然，在新模型中，$\alpha_0$ 的最小二乘估计仍是 $\bar{Y}$, 且 $\alpha$的最小二乘估计为：</p>
<blockquote>
<p>$\hat{\alpha}=(Z’Z)^{-1}Z’Y=\Lambda^{-1}Z’Y$</p>
</blockquote>
<p>对应的岭回归估计值为</p>
<blockquote>
<p>$\hat{\alpha}(k)=(\Lambda+kI)^{-1}Z’Y$</p>
</blockquote>
<p>则</p>
<blockquote>
<p>$\hat{\beta}(k)=(X’X+kI)^{-1}X’Y=(\Phi Z’Z\Phi’+\Phi(kI)\Phi’)^{-1}\Phi Z’Y=\Phi(Z’Z+kI)^{-1}\Phi’\Phi Z’Y=\Phi\hat{\alpha}(k)$</p>
</blockquote>
<p>则$|\hat{\beta}(k)|=|\hat{\alpha}(k)|$, 且 $\hat{\alpha}(k)=(\Lambda+kI)^{-1}\Lambda\hat{\alpha}$, 由于 $\Lambda$ 是充满特征值的对角阵，所以实际上矩阵 $(\Lambda+kI)^{-1}\Lambda$是对角矩阵，且每一个对角元都是 $\frac{\lambda_i}{\lambda_i+k}&lt;1$.</p>
<p>因此有：$|(\Lambda+kI)^{-1}\Lambda\hat{\alpha}|&lt;|\hat{\alpha}|=|\hat{\beta}|$</p>
<p><strong>引理4：</strong></p>
<blockquote>
<p>$\operatorname{MSE}(\beta(k))=E(\hat{\beta}(k)-\beta(k))^{\prime}(\hat{\beta}(k)-\beta(k))=E(\hat{\alpha}(k)-\alpha(k))^{\prime} \Phi^{\prime} \Phi(\hat{\alpha}(k)-\alpha(k))=\operatorname{MSE}(\alpha(k))$</p>
</blockquote>
<p>由于均方误差的正交不变性，只需证明 $MSE(\hat{\alpha}(k))&lt;MSE(\hat{\alpha})$</p>
<blockquote>
<p>$\operatorname{cov}(\hat{\alpha}(k))=\sigma^{2}(\Lambda+k I)^{-1} Z^{\prime} Z(\Lambda+k I)^{-1}=\sigma^{2}(\Lambda+k I)^{-1} \Lambda(\Lambda+k I)^{-1}$<br>$E(\hat{\alpha}(k))=(\Lambda+k I)^{-1} Z^{\prime}\left(\alpha_{0} \mathbf{1}+Z \alpha\right)=(\Lambda+k I)^{-1}\Lambda\alpha$</p>
<p>$MSE(\hat{\alpha}(k))=E[(\hat{\alpha}(k)-E(\hat{\alpha}(k))+E(\hat{\alpha}(k))-\alpha)’(\hat{\alpha}(k)-E(\hat{\alpha}(k))+E(\hat{\alpha}(k))-\alpha)]$</p>
<p>$=E[(\hat{\alpha}(k)-E(\hat{\alpha}(k))’(\hat{\alpha}(k)-E(\hat{\alpha}(k))]+E((\hat{\alpha}(k))-\alpha)’E((\hat{\alpha}(k))-\alpha)$</p>
<p>$=tr(cov(\hat{\alpha}(k)))+E((\hat{\alpha}(k))-\alpha)’E((\hat{\alpha}(k))-\alpha)=\sigma^{2} \sum_{i=1}^{p} \frac{\lambda_{i}}{\left(\lambda_{i}+k\right)^{2}}+k^{2} \sum_{i=1}^{p} \frac{\alpha_{i}^{2}}{\left(\lambda_{i}+k\right)^{2}}$</p>
</blockquote>
<p>这是一个关于 $k$ 的连续函数，对其求导可以发现，其导函数在 $k=0$ 时是负的，则在0周围，一定存在 $k$ ,使得条件成立。</p>
<h3 id="岭参数-k-的选择"><a href="#岭参数-k-的选择" class="headerlink" title="岭参数 $k$ 的选择"></a>岭参数 $k$ 的选择</h3><h4 id="岭迹法"><a href="#岭迹法" class="headerlink" title="岭迹法"></a>岭迹法</h4><p>则通过岭迹图来主观选择 $k$ 值，使得以下条件满足：</p>
<blockquote>
<ol>
<li>各回归系数的岭估计基本稳定</li>
<li>用最小二乘估计时符号不合理的回归系数，其岭估计的符号变得合理；</li>
<li>回归系数没有不符合实际的值</li>
<li>残差平方和增大的不太多</li>
</ol>
</blockquote>
<h4 id="方差扩大因子法"><a href="#方差扩大因子法" class="headerlink" title="方差扩大因子法"></a>方差扩大因子法</h4><p>由于当方差扩大因子大于 $10$ 就认为模型存在严重多重共线性，则只需要让协方差阵中的方差扩大因子小于 $10$ 。</p>
<blockquote>
<p>$var(\hat{\beta}(k))=cov(\hat{\beta}(k),\hat{\beta}(k))=\sigma^2(X’X+kI)^{-1}X’X(X’X+kI)^{-1}=\sigma^2c(k)$</p>
<p>其中 $c(k)=(X’X+kI)^{-1}X’X(X’X+kI)^{-1}$</p>
</blockquote>
<p>显然 $k$ 不断增大的时候，对应的 $c(k)$ 的对角元会减小。只需选择 $k$ 使得所有的方差扩大因子 $c_{jj}(k)&lt;10$</p>
<h4 id="由残差平方和来确定-k-值"><a href="#由残差平方和来确定-k-值" class="headerlink" title="由残差平方和来确定 $k$ 值"></a>由残差平方和来确定 $k$ 值</h4><p>由于 $\hat{\beta}(k)$在减小均方误差的同时增大了残差平方和，所以希望岭回归的残差平方和 $SSE(k)$ 的增加在一定限度内，因此可以给定一个大于 $1$ 的 $c$ 值，寻找满足下列条件的最大 $k$：</p>
<blockquote>
<p>$SSE(k)&lt;cSSE$</p>
</blockquote>
<h4 id="Hoerl-Kennad-公式"><a href="#Hoerl-Kennad-公式" class="headerlink" title="Hoerl-Kennad 公式"></a>Hoerl-Kennad 公式</h4><p>因为 $\operatorname{MSE}(\hat{\beta}(k))=\operatorname{MSE}(\hat{\alpha}(k))=\sigma^{2} \sum_{i=1}^{p} \frac{\lambda_{i}}{\left(\lambda_{i}+k\right)^{2}}+k^{2} \sum_{i=1}^{p} \frac{\alpha_{i}^{2}}{\left(\lambda_{i}+k\right)^{2}}$</p>
<p>令 $MSE(\hat{\beta}(k))=g(k)$ 关于 $k$ 求导可得 $g^{\prime}(k)=2  \sum_{i=1}^{p} \frac{\lambda_{i} (\alpha_{i}^{2}k-\sigma^2)}{\left(\lambda_{i}+k\right)^{3}}$</p>
<p>顾可以取 $k^*=\frac{\sigma^2}{\max_i\alpha_i^2}$</p>
<h4 id="Mcdorard-Gararneau方法"><a href="#Mcdorard-Gararneau方法" class="headerlink" title="Mcdorard-Gararneau方法"></a>Mcdorard-Gararneau方法</h4><p>当 $X$ 病态时，最小二乘估计得 $\hat{\beta}$ 会偏长，将 $\hat{\beta}$ 的长度与 $MSE(\hat{\beta})$ 的估计 $\hat{\sigma}^2\sum_{i=1}^p\lambda_i^{-1}$作比较：若 $|\hat{\beta}|-\hat{\sigma}^2\sum_{i=1}^p\lambda_i^{-1}&gt;0$，则认为 $\hat{\beta}$ 太长，需要压缩，因此建议选取 $k$：</p>
<blockquote>
<p>$|\hat{\beta}(k)\approx||\hat{\beta}|-\hat{\sigma}^2\sum_{i=1}^p\lambda_i^{-1}$</p>
</blockquote>
<h4 id="双h公式"><a href="#双h公式" class="headerlink" title="双h公式"></a>双h公式</h4><blockquote>
<p>$\hat{k}(h_1,h_2)=\frac{h_1\hat{\sigma}^2}{\hat{\beta}’A\hat{\beta}+h_2\hat{\sigma}^2}$</p>
</blockquote>
<h3 id="用岭回归做变量选择"><a href="#用岭回归做变量选择" class="headerlink" title="用岭回归做变量选择"></a>用岭回归做变量选择</h3><p>在岭回归计算时，需要对数据经行中心标准化。</p>
<ol>
<li>可以删除岭回归后<strong>回归系数稳定而绝对值很小的自变量</strong></li>
<li>去掉岭回归系数不稳定的变量。</li>
</ol>
<h3 id="岭回归的不同解释"><a href="#岭回归的不同解释" class="headerlink" title="岭回归的不同解释"></a>岭回归的不同解释</h3><h4 id="L-2正则化解释"><a href="#L-2正则化解释" class="headerlink" title="L-2正则化解释"></a>L-2正则化解释</h4><p>岭回归等价于优化下列目标函数，即加上$L-2$ 范数的正则项：</p>
<blockquote>
<p>$L(\beta_0,\beta)=\frac{1}{2 N} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda|\beta|_{2}^{2}$</p>
</blockquote>
<h4 id="压缩参数"><a href="#压缩参数" class="headerlink" title="压缩参数"></a>压缩参数</h4><p>同样的，利用拉格朗日对偶法，岭回归也等价与下列优化问题:</p>
<blockquote>
<p>$\min _{ \beta}\left\{\frac{1}{2 N}\left|y-X \beta\right|_{2}^{2}\right\},|\beta|_{2}^{2} \leq t$</p>
</blockquote>
<h4 id="贝叶斯先验解释"><a href="#贝叶斯先验解释" class="headerlink" title="贝叶斯先验解释"></a>贝叶斯先验解释</h4><p>假设拟合函数的参数 $\beta$ 属于一个均值为 $0$ ，协方差为 $\lambda I$ 的多元高斯分布.</p>
<p>则利用最大后验估计：</p>
<blockquote>
<p>$P(\beta|y,X,\lambda,\sigma^2)\propto P(y|X,\beta,\sigma^2)P(\beta|\lambda)$ </p>
</blockquote>
<p>对其取对数并求最大值可得最终目标为求函数：</p>
<blockquote>
<p>$L(\beta,\lambda)=\sum_{i=1}^{N}|y-X\beta|_2^{2}+\frac{\lambda}{2}|\beta|_{2}^{2}$</p>
</blockquote>
<p>的最小值。</p>
<h2 id="广义岭回归"><a href="#广义岭回归" class="headerlink" title="广义岭回归"></a>广义岭回归</h2><p>称 $\hat{\beta}(k)=(X’X+\Phi K\Phi’)^{-1}X’y$ 为回归参数的广义岭回归估计，其中 $K=diag(k_1,\dots,k_p),k_i\ge0$ 为岭参数,而$\Phi$为标准正交化特征向量所构成的矩阵。</p>
<h3 id="广义岭估计的性质"><a href="#广义岭估计的性质" class="headerlink" title="广义岭估计的性质"></a>广义岭估计的性质</h3><blockquote>
<p>引理1：$\hat{\beta}(K)=B_k\hat{\beta}$, 其中 $B_k=(X’X+\Phi K\Phi’)^{-1}(X’X)^{-1}$</p>
<p>引理2：$E\hat{\beta}(K)=B_k\beta$, 只要 $B_k\ne I$, 即 $K\ne 0$, 广义岭估计就是有偏估计。</p>
<p>引理3：对任意 $K=diag(k_1,\dots,k_p), k_1&gt;0,|\hat{\beta}|&gt;0$, 总有 $|\hat{\beta}(k)|&lt;|\hat{\beta}|,$即广义岭估计也是最小二乘估计向远点的一种压缩。</p>
<p>引理4：存在 $K=diag(k_1,\dots,k_p)&gt;0,$使得 $MSE(\hat{\beta}(K))=MSE(\hat{\beta})$</p>
</blockquote>
<p><strong>引理4：</strong></p>
<p>$MSE(\hat{\beta}(K))=MSE(\hat{\alpha}(K))=\sigma^{2} \sum_{i=1}^{p} \frac{\lambda_{i}}{\left(\lambda_{i}+k_i\right)^{2}}+k_i^{2} \sum_{i=1}^{p} \frac{\alpha_{i}^{2}}{\left(\lambda_{i}+k_i\right)^{2}}$</p>
<h3 id="岭参数-K-的选择"><a href="#岭参数-K-的选择" class="headerlink" title="岭参数 $K$ 的选择"></a>岭参数 $K$ 的选择</h3><h4 id="Hemmerie-Brantle法"><a href="#Hemmerie-Brantle法" class="headerlink" title="Hemmerie-Brantle法"></a>Hemmerie-Brantle法</h4><blockquote>
<p>$\hat{k}_i=\frac{\hat{\sigma}^2}{\hat{\alpha}_i^2-\hat{\sigma}^2/\lambda_i}$</p>
</blockquote>
<p>考虑两种方法导出：</p>
<ol>
<li><p>在引理4中，令 $k_i^*=\frac{\sigma^2}{\alpha_i^2}$,则可以得到最小化的 $MSE(\hat{\beta}(K))$,利用 $\sigma^2$ 和 $\alpha_i^2$ 的无偏估计 $\hat{\sigma}^2$ 和 $\hat{\alpha}_i^2-\frac{\hat{\sigma}^2}{\lambda_i}$,来代替。</p>
</li>
<li><p>$\hat{k}_i=\frac{\hat{\sigma}^2}{\hat{\alpha}_i^2-\hat{\sigma}^2/\lambda_i}$ 是使 $MSE(\hat{\alpha}(K))$ 的一个无偏估计达到最小。由于 $\hat{\alpha}(K)=(\Lambda+K)^{-1}\Lambda\hat{\alpha}$, 可以得到：$E(\hat{\alpha}(K)-\hat{\alpha})’(\hat{\alpha}(K)-\hat{\alpha})=\sum_{i=1}^p\frac{k_i^2\alpha_i^2}{(\lambda_i+k_i)^2}+\sigma^2\sum_{i=1}^p\frac{k_i^2}{\lambda_i(\lambda_i+k_i)}$,因此 $MSE(\hat{\alpha}(K))-E(\hat{\alpha}(K)-\hat{\alpha})’(\hat{\alpha}(K)-\hat{\alpha})=\sigma^2\sum_{i=1}^p\frac{\lambda_i-k_i}{\lambda_i(\lambda_i+k_i)}$, 所以可以得到一个 $MSE(\hat{\alpha}(K))$ 的一个无偏估计： $(\hat{\alpha}(K)-\hat{\alpha})’(\hat{\alpha}(K)-\hat{\alpha}+\hat{\sigma}^2\sum_{i=1}^p\frac{\lambda_i-k_i}{\lambda_i(\lambda_i+k_i)}$,则可以证明 $\hat{k}_i$ 使得上式达到最小。</p>
</li>
</ol>
<h4 id="Hemmerie法"><a href="#Hemmerie法" class="headerlink" title="Hemmerie法"></a>Hemmerie法</h4><blockquote>
<p>$d_i=\begin{cases}\frac 12+\sqrt{\frac 14 -\frac{\hat{\sigma}^2}{\lambda_i\hat{\alpha}_i}},\quad&amp;\frac{\lambda_i\hat{\alpha}_i}{\hat{\sigma}^2}\ge4\\0,&amp;\frac{\lambda_i\hat{\alpha}_i}{\hat{\sigma}^2}&lt;4\end{cases}$</p>
</blockquote>
<p>理由是迭代，即第一步利用最小二乘估计出 $\hat{\alpha}_i$ 和 $\hat{\sigma}^2$, 得到 $k_i^{(0)}=\frac{\hat{\sigma}^2}{\hat{\alpha}_i^2}$, 将此 $k_i^{(0)}$ 代入 $\hat{\alpha}(K)=(\Lambda+K)^{-1}\Phi’Z’Y$，可以得到 $\alpha_i^{(1)}$ 的估计, 再由 $k_i^{(1)}=\frac{ \hat{\sigma}^2}{\alpha_i^{(1)}}$ ，不断迭代下去。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><hr>
<p>[1] 何晓群，刘文卿. 应用回归分析[M]. 中国人民大学出版社，2015.3</p>
<p>[2] 孙荣恒. 应用数理统计.科学出版社，1998</p>
<p>[3] 陈希孺，王松桂. 近代回归分析-原理方法及应用.安徽教育出版社，1987</p>
<p>[4] <a href="https://zhuanlan.zhihu.com/p/51431045" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/51431045</a></p>

        </div>
        
            <div style="text-align:center;color: rgb(125,125,125);font-size:14px;">
               -------------本文结束 感谢您的阅读-------------
            </div>
        
        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>文章作者:</span>
                        <span>会飞的猪</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>文章链接:</span>
                        <span><a href="http://shellyandliu.github.io/2020/04/16/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%92%8C%E5%B9%BF%E4%B9%89%E5%B2%AD%E5%9B%9E%E5%BD%92/">http://shellyandliu.github.io/2020/04/16/%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%92%8C%E5%B9%BF%E4%B9%89%E5%B2%AD%E5%9B%9E%E5%BD%92/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>版权声明:</span>
                        <span>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC-BY-NC-4.0</a> ， 转载请注明来自 会飞的猪！</span>
                    </p>
                
            </section>
        
        <section class="post-tags">
            <div>
                <span>标签:</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"> <i class="iconfont icon-tags"></i> 线性模型</a>
                    
                        <a href="/tags/%E5%9B%9E%E5%BD%92/"> <i class="iconfont icon-tags"></i> 回归</a>
                    
                        <a href="/tags/%E6%AD%A3%E5%88%99%E5%8C%96/"> <i class="iconfont icon-tags"></i> 正则化</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">返回</a>
                <span> /  </span>
                <a href="/">首页</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/04/16/%E4%B8%BB%E6%88%90%E5%88%86%E5%9B%9E%E5%BD%92%E5%92%8C%E5%81%8F%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E5%9B%9E%E5%BD%92/">主成分回归和偏最小二乘回归</a>
            
            
            <a class="next" rel="next" href="/2020/04/16/%E5%B8%A6%E7%BA%A6%E6%9D%9F%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E4%B8%8E%E5%A4%9A%E9%87%8D%E5%85%B1%E7%BA%BF%E6%80%A7/">带约束的线性回归问题与多重共线性</a>
            
        </section>
        <div class="post-content">
            <section style="font-size: 20px;font-weight: 700;margin-bottom: 10px;margin-top: 20px;">
                <i class="iconfont icon-comments"></i>
                <span>
                    评论
                </span>
            </section>
        </div>
        
            <section id="comments" class="comments">
              <style>
                .comments{margin:0px;padding:10px;background:#fff}
                @media screen and (max-width:800px){.comments{margin:auto;padding:10px;background:#fff}}
              </style>
              <div class="valine_comment"></div>
<!--����js����</body>֮ǰ���뼴��-->
<!--Leancloud ������:-->
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<!--Valine �ĺ��Ĵ����-->
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
  new Valine({
      el: '.valine_comment',
      app_id: 'bCvyxhtL2V59mmqAp81lAGob-gzGzoHsz#Leancloud应用的appId',
      app_key: 'J4zVKQfUjhToGJwBfLcoOHOv',
      placeholder: '欢迎大家评论交流~',
      notify: 'true',
      verify: 'true',
    });
</script>
            </section>
        
        

    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 会飞的猪 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="会飞的猪">





<link rel="stylesheet" href="/fonts/iconfont_new/iconfont.css">


<title>线性回归的变量选择 | 会飞的猪</title>



    <link rel="icon" href="/image/longmao.jpg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">会飞的猪</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/categories">分类</a>
                
                    <a class="menu-item" href="/tags">标签</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">会飞的猪</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/categories">分类</a>
                
                    <a class="menu-item" href="/tags">标签</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">展开目录</a>
        <a onclick="go_top()">回到顶部</a>
        <a onclick="go_bottom()">到达底部</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">线性回归的变量选择</h1>
            
                <div class="post-meta">
                    
                        作者: <a itemprop="author" rel="author" href="/">会飞的猪</a>
                    

                    
                        <span class="post-time">
                        时间: <a href="#">四月 16, 2020&nbsp;&nbsp;17:14:53</a>
                        </span>
                    
                    
                        <span class="post-category">
                    类别:
                            
                                <a href="/categories/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">-线性回归</a>
                            
                        </span>
                    
                    
                        <span class="post-time">
                            <span class="post-meta-item-icon">
                              <i class="fa fa-keyboard-o"></i>
                              <span class="post-meta-item-text">  字数统计: </span>
                              <span class="post-count"><a href="#">3.1k字</a></span>
                            </span>
                        </span>
                        <span class="post-time">
                            <span class="post-meta-item-icon">
                            <i class="fa fa-hourglass-half"></i>
                            <span class="post-meta-item-text">  阅读时长: </span>
                            <span class="post-count"><a href="#">14分</a></span>
                            </span>
                        </span>
                    

                    
                </div>
            
        </header>

        <div class="post-content">
            <h2 id="变量选择"><a href="#变量选择" class="headerlink" title="变量选择"></a>变量选择</h2><ul>
<li>通过变量选择，选出影响该因素的所有自变量。</li>
<li>通过变量选择，筛选出作用较大的变量。</li>
</ul>
<blockquote>
<p>定义：将所有对因变量有影响的m个因素选入做回归，得到的模型称之为<strong>全模型</strong>，如果从中挑出 $p$ 个影响因素做回归得到的模型称之为选模型。</p>
</blockquote>
<h2 id="全模型正确，选模型错误"><a href="#全模型正确，选模型错误" class="headerlink" title="全模型正确，选模型错误"></a>全模型正确，选模型错误</h2><blockquote>
<p>引理1：在 $x_j$ 与$x_{p+1},\dots,x_m$不全正交的时候，选模型中的回归系数的估计是有偏的，即$E(\hat{\beta}_p)\ne \beta_p$</p>
<p>引理2：$E(\hat{\sigma}_p^2)=\sigma^2+\frac{\beta_{m-p}’\Sigma_{22.1}^{-1}\beta_{m-p}}{n-p-1}$, 其中，$\beta_{m-p}$ 为后 $\beta$ 的后 $m-p$ 个分量，$\Sigma_{22.1}=X_{m-p}^{\prime} X_{m-p}-X_{m-p}^{\prime} X_{p}\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} X_{m-p}$ .</p>
<p>引理3：对于任意新自变量值$x_{0m}$ 和因变量新值$y_0=x_{0m}’\beta_m+\epsilon_0$ 和选模型的预测这$\hat{y}_{0p}=x_{0p}’\hat{\beta}_p$ ,有$E(\hat y_{0p}-y_0)\ne0$.</p>
<p>引理4：选模型的最小二乘估计 $\hat{\beta}_p$ 比全模型的最小二乘估计 $\hat{\beta}_m$ 的前 $p$ 个分量有较小的方差。</p>
<p>引理5：选模型的预测误差有比全模型的预测误差更小的方差。</p>
<p>引理6：在 $var(\hat{\beta}_{m-p})\ge\beta_{m-p}\beta_{m-p}’$ 的情况下，$MSE(e_{0p})\le var(e_{0p})$, 其中$MSE$是均方误差的意思。</p>
</blockquote>
<p><strong>引理1</strong>:</p>
<p>将 $X_m\beta_m$ 展开为 $X_p\beta_p+X_{m-p}\beta_{m-p}$ ,这时，全模型的期望为 $E(y) = X_p\beta_p+X_{m-p}\beta_{m-p}$,因此：</p>
<blockquote>
<p>$E\left(\hat{\beta}_{p}\right)=E\left[\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} y\right]=\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime}\left(X_{p} \beta_{p}+X_{m-p} \beta_{m-p}\right)=\beta_{p}+\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} X_{m-p} \beta_{m-p}$</p>
</blockquote>
<p>则只需要说明$X_p’X_{m-p}\ne 0$，由于不相交即等价于对任意 $i=1,\dots,p,j=p+1,\dots,m$ 相关系数$r_{ij}$不全为0，对样本做标准化可以得到 $r_{ij}=x_i’x_j$ ，而标准化不会改变样本之间的相关系数，即$X_p’X_{m-p}\ne 0$</p>
<p><strong>引理2：</strong></p>
<p>由于 $\hat{\sigma}^2_p=\frac{SEE_p}{n-p-1}=\frac{Y^T(I_n-X_p(X_p^TX_p)^{-1}X_p^T)Y}{n-p-1}$, 因此 $E(\hat{\sigma}^2_p)=\frac{E(SEE_p)}{n-p-1}=\frac{( X\beta)^T(I_n-X_p(X_p^TX_p)^{-1}X_p^T) X\beta+\sigma^2tr(I_n-X_p(X_p^TX_p)^{-1}X_p^T)}{n-p-1}=\sigma^2+\frac{\beta_{m-p}’\Sigma_{22.1}^{-1}\beta_{m-p}}{n-p-1}$</p>
<p><strong>引理3：</strong></p>
<p>由于$E\left(\hat{y}_{0 p}\right)=x_{0 p}^{\prime} E\left(\hat{\beta}_{p}\right)=x_{0 p}^{\prime}\left[\beta_{p}+\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} X_{m-p} \beta_{m-p}\right]$，$E\left(y_{0}\right)=x_{0 p}^{\prime} \beta_{p}+x_{0, m-p}^{\prime} \beta_{m-p}$</p>
<p>根据 $\beta_{m-p}$ 的任意性，只需要说明 $x_{0 p}^{\prime}\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} X_{m-p}\ne x_{0,m-p}’$ .这是显然的</p>
<p><strong>引理4：</strong></p>
<p>容易知道 $var(\hat{\beta}_p)=\sigma^2(X_p’X_p)^{-1},var(\hat{\beta}_m)=\sigma^2(X_m’X_m)^{-1}$ .</p>
<script type="math/tex; mode=display">
\left(X_{m}^{\prime} X_{m}\right)^{-1}=\left[\begin{array}{cc}X_{p}^{\prime} X_{p} & X_{p}^{\prime} X_{m-p} \\ X_{m-p}^{\prime} X_{p} & X_{m-p}^{\prime} X_{m-p}\end{array}\right]^{-1}=\left[\begin{array}{cc}\left(X_{p}^{\prime} X_{p}\right)^{-1} & 0 \\ 0 & 0\end{array}\right]+\left[\begin{array}{c}-\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} X_{m-p} \\ I\end{array}\right] \Sigma_{22 \cdot 1}^{-1}\left[-X_{m-p}^{\prime} X_{p}\left(X_{p}^{\prime} X_{p}\right)^{-1} \quad I\right]</script><p>其中 $\Sigma_{22.1}=X_{m-p}^{\prime} X_{m-p}-X_{m-p}^{\prime} X_{p}\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} X_{m-p}$。因此只需说明 $\Sigma^{-1}_{22\cdot 1}$ 是正定的，由于 $\Sigma^{-1}_{22\cdot 1}$ 是通过 $\left(X_{m}^{\prime} X_{m}\right)^{-1}$ 做初等变换得到的，并且是在对角元上，且 $\left(X_{m}^{\prime} X_{m}\right)^{-1}$ 是正定的，因此得证。</p>
<p><strong>引理5：</strong></p>
<p>由于 $\operatorname{var}\left(e_{0 m}\right) =\sigma^{2}\left(1+x_{0}^{\prime}\left(X_{m}^{\prime} X_{m}\right)^{-1} x_{0}\right) , \operatorname{var}\left(e_{0 p}\right)=\sigma^{2}\left(1+x_{0 p}^{\prime}\left(X_{p}^{\prime} X_{p}\right)^{-1} x_{0 p}\right) $，两式相减，类似于引理3可证明结论。</p>
<p><strong>引理6：</strong></p>
<p>$\left[E\left(e_{0 p}\right)\right]^{2}=\left(x_{0(m-p)}^{\prime} \beta_{m-p}-x_{0 p}^{\prime}\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} X_{m-p} \beta_{m-p}\right)^{2}$</p>
<p>$\le\left(x_{0(m-p)}^{\prime} -x_{0 p}^{\prime}\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} X_{m-p} \right) \operatorname{var}\left(\hat{\beta}_{m-p}\right)\left(x_{0(m-p)}^{\prime} -x_{0 p}^{\prime}\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} X_{m-p}\right)^{\prime}$</p>
<p>注意到 $var(\hat{\beta}_{m-p})=\sigma^2\Sigma^{-1}_{22\cdot1}$.并且 :</p>
<script type="math/tex; mode=display">
var(e_{0m})-var(e_{0p})=\left[\begin{array}{cc}
x_{0 p}^{\prime} & x_{0(m-p)}^{\prime}
\end{array}\right]\left[\begin{array}{c}
-\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} X_{m-p} \\
I
\end{array}\right] \Sigma_{22 \cdot 1}^{-1}\left[\begin{array}{cc}
-X_{m-p}^{\prime} X_{p}\left(X_{p}^{\prime} X_{p}\right)^{-1} & I
\end{array}\right]\left[\begin{array}{c}
x_{0 p} \\
x_{0(m-p)}
\end{array}\right]</script><p>等于不等式右边的式子，因此得到：$\left[E\left(e_{0 p}\right)\right]^{2}\le var(e_{0m})-var(e_{0p})$.</p>
<h2 id="全模型错误，选模型正确"><a href="#全模型错误，选模型正确" class="headerlink" title="全模型错误，选模型正确"></a>全模型错误，选模型正确</h2><p>这个时候选模型是无偏的，全模型是有偏的，并且选模型残差的方差小于全模型。</p>
<h2 id="决定选模型的准则"><a href="#决定选模型的准则" class="headerlink" title="决定选模型的准则"></a>决定选模型的准则</h2><h3 id="基于残差平方和的准则"><a href="#基于残差平方和的准则" class="headerlink" title="基于残差平方和的准则"></a>基于残差平方和的准则</h3><h4 id="平均残差平方和"><a href="#平均残差平方和" class="headerlink" title="平均残差平方和"></a>平均残差平方和</h4><p>因为 $SSE_p=Y^T(I_n-X_p(X_p^TX_p)^{-1}X_p^T)Y=Y^T(I_n-H_p)Y$ ,则由分块矩阵的逆矩阵可知 $H_{p+1}\ge H_p$, 因此$SSE_{p+1}\le SSE_p$ ，即当自变量子集越大时，残差平方和越小，因此可以考虑平均残差平方和，即:</p>
<blockquote>
<p>平均残差平方和：$SME_p=\frac{SSE_p}{n-p-1}$</p>
</blockquote>
<p>使得$SMP_p$取得最小</p>
<h4 id="自由度调整复决定系数"><a href="#自由度调整复决定系数" class="headerlink" title="自由度调整复决定系数"></a>自由度调整复决定系数</h4><p>定义如下表达式，其中 $R$ 为复决定系数。</p>
<blockquote>
<p>$R_a^2=1-\frac{n-1}{n-p-1}(1-R^2)$</p>
</blockquote>
<p>则 $R_a^2=1-\frac{n-1}{n-p-1}\frac{SSR_p}{SST}=1-\frac{n-1}{SST}SME_p$，即 $R_a^2$ 最大化等价于 $SME_p$ 的最小化。</p>
<h4 id="基于预测偏差的方差"><a href="#基于预测偏差的方差" class="headerlink" title="基于预测偏差的方差"></a>基于预测偏差的方差</h4><p>考虑选模型下 $n$ 个实验点 $x_{ip},i=1,\dots,n$， 则定义预测的偏差 $U_{ip}=y_i-x_{ip}\hat{\beta}_p $，由于 $var(U_{ip})=$</p>
<blockquote>
<p>$\sum_{i=1}^nvar(U_{ip})=\sum_{i=1}^nvar(y_i-x_{ip}\hat{\beta}_p)=\sum_{i=1}^n(var(y_i)+x_{ip}’var(\hat{\beta}_p)x_{ip})=\sum_{i=1}^n\sigma^2(1+x_{ip}’(X_p’X_p)^{-1}x_{ip})$</p>
<p>$=\sigma^2(n+\sum_{i=1}^nx_{ip}’(X_p’X_p)^{-1}x_{ip})=\sigma^2(n+\sum_{i=1}^ntr(x_{ip}’(X_p’X_p)^{-1}x_{ip}))=\sigma^2(n+\sum_{i=1}^ntr((X_p’X_p)^{-1}x_{ip}’x_{ip})$</p>
<p>$=\sigma^2(n+p+1)$</p>
</blockquote>
<p>由于 $\frac{SSE_p}{n-p-1}$ 是 $\sigma^2$ 的选模型估计，所以可以选择 $(n+p+1)SMP_p$ 取最小</p>
<h3 id="赤池信息量AIC（最小化）"><a href="#赤池信息量AIC（最小化）" class="headerlink" title="赤池信息量AIC（最小化）"></a>赤池信息量AIC（最小化）</h3><blockquote>
<p>设模型的似然函数为 $L(\theta,x)$ ，$\theta$ 的维数为 $p$ , $x$ 为随机样本，那么 $AIC=-2\ln L(\hat{\theta}_L,x)+2p$</p>
</blockquote>
<p>将极大似然估计所得到的参数 $\hat{\beta}=(X’X)^{-1}X’y,\hat{\sigma}=\frac {SSE}{n}$ 带入可得：</p>
<blockquote>
<p>$\ln L_{max}=-\frac n2\ln (2\pi)-\frac n2 \ln(\frac{SSE}{n}-\frac n2)$</p>
<p>去掉常数，则可得到：$AIC=n\ln(SSE)+2p$</p>
</blockquote>
<h3 id="C-p-准则"><a href="#C-p-准则" class="headerlink" title="$C_p$ 准则"></a>$C_p$ 准则</h3><p>标准线性回归的目的是最小化 $\sum(\hat y_i-y_i)^2$ ，这样做的情况下，如果有一个异常点，则回归结果会答辩，即<strong>过拟合</strong>问题。因此考虑最小化$\sum(\hat y_i-E(y_i))^2$.</p>
<p>假定全模型正确，选模型错误。令全模型为 $Y=X_p\beta_p+X_{m-p}\beta_{m-p}+\epsilon$, 选模型为$Y=X_p\beta_p+\epsilon_p$, 选模型下的最小二乘估计值为 $\tilde{\beta}_p$ ,那么过拟合值为 $\tilde{Y}=X_p\tilde{\beta}_p$ </p>
<blockquote>
<p>定义：$J_p=\frac{1}{\sigma^{2}}(\tilde{Y}-E(Y))^{\prime}(\tilde{Y}-E(Y))$</p>
</blockquote>
<p>下面计算 $E(J_p)$</p>
<blockquote>
<p>$E(J_p)=\frac{1}{\sigma^{2}}E\left[(\tilde{Y}-E(Y))^{\prime}(\tilde{Y}-E(Y))\right]=\frac{1}{\sigma^{2}}E\left[(\tilde{Y}-E(\tilde{Y})+E(\tilde{Y})-E(Y))^{\prime}(\tilde{Y}-E(\tilde{Y})+E(\tilde{Y})-E(Y))\right]$</p>
<p>$=\frac{1}{\sigma^{2}}E\left[(\tilde{Y}-E(\tilde{Y}))’(\tilde{Y}-E(\tilde{Y}))\right]+(E(\tilde{Y})-E(Y))’(E(\tilde{Y})-E(Y))=\frac{1}{\sigma^{2}}\left[tr(var(\tilde{Y}))+(E(\tilde{Y})-E(Y))’(E(\tilde{Y})-E(Y))\right]$</p>
</blockquote>
<p>其中 </p>
<blockquote>
<p>$var(\tilde{Y})=var(X_p\tilde{\beta})=X_{p}\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime} var(Y) X_{p}\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime}=\sigma^{2} X_{p}\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime}$</p>
<p>$(E(\tilde{Y})-E(Y))^{\prime}(E(\tilde{Y})-E(Y))=\left[X_{p}\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime}-I_n\right] X_{m-p} \beta_{m-p}=\beta_{m-p}^{\prime} X_{m-p}^{\prime}\left[I_{n}-X_{p}\left(X_{p}^{\prime} X_{p}\right)^{-1} X_{p}^{\prime}\right] X_{m-p} \beta_{m-p}$</p>
</blockquote>
<p>由于 $SSE_p=Y^T(I_n-X_p(X_p^TX_p)^{-1}X_p^T)Y$ ,所以：</p>
<blockquote>
<p>$E(SSE_p)=(E(Y))^T(I_n-X_p(X_p^TX_p)^{-1}X_p^T)E(Y)+\sigma^2tr\left[I_n-X_p(X_p^TX_p)^{-1}X_p^T\right]$</p>
<p>$=\beta_{m-p}’X_{m-p}’\left[I_n-X_p(X_p^TX_p)^{-1}X_p^T\right]X_{m-p}\beta_{m-p}+(n-p-1)\sigma^2$</p>
<p>$E(J_p)=\frac 1{\sigma^2}\left[(p+1)\sigma^2+E(SSR_p)-(n-p-1)\sigma^2\right]=\frac {E(SSR_p)}{\sigma^2}-n+2p+2$</p>
</blockquote>
<p>用 $\hat{\sigma}^2= \frac {SSE}{n-m-1}$ 代替 $\sigma^2$,可以得到最终的 $C_p$ 准则。</p>
<blockquote>
<p>$C_p=\frac {SSR_p}{\hat{\sigma}^2}-n+2p=(n-m-1)\frac {SSE_p}{SSE_m}-n+2p$</p>
</blockquote>
<p>我们要最小化它。</p>
<h3 id="预测平方和准则"><a href="#预测平方和准则" class="headerlink" title="预测平方和准则"></a>预测平方和准则</h3><p>前面的基于残差平方和的准则和 $C_p$ 准则都有一个缺点，即在计算某点的预测偏差时，该点曾在建立经验回归方程中使用过。</p>
<p>由于删除残差 $e_{(i)}=\frac{e_i}{1-h_{ii}}$ ，则预测残差和为:</p>
<blockquote>
<p>$PRSSE=\sum_{i=1}^n(\frac{e_i}{1-h_{ii}})$</p>
<p>其中 $h_{ii}$ 为 $H=X(X’X)^{-1}X’$ 的第 $i$ 个对角线的值，即杠杆值</p>
</blockquote>
<p>对于选模型的预测残差和：</p>
<blockquote>
<p>$PRSSE_p=\sum_{i=1}^n(\frac{e_{ip}}{1-h_{iip}})$</p>
</blockquote>
<p>则目的是使 $PRSSE_p$ 最小。</p>
<h2 id="挑选自变量的简单方法"><a href="#挑选自变量的简单方法" class="headerlink" title="挑选自变量的简单方法"></a>挑选自变量的简单方法</h2><p>上面的准则随着自变量的增大，工作呈指数级的增长，因此需要新的方法。</p>
<h3 id="前进法，后退法"><a href="#前进法，后退法" class="headerlink" title="前进法，后退法"></a>前进法，后退法</h3><h4 id="前进法："><a href="#前进法：" class="headerlink" title="前进法："></a><strong>前进法：</strong></h4><blockquote>
<p>第一步：将所有的自变量对因变量 $y$ 分别做一次一元线性回归，并计算这些系数的F值（上一节中我们说过，它是检测部分系数是否为0的统计量），并且选取其中最大的。设为 $F_j^1$。</p>
<p>第二步：给定显著性水平 $\alpha$ ，如果 $F_j^1\ge F_{\alpha}(1,n-2)$ ，则首先引入 $x_j$ ，并且在新模型中令这个 $x_j$ 就是 $x_1$ 。</p>
<p>第三步：分别让 $(x_1,x_2),(x_1,x_3),\dots,(x_1,x_m)$ 与因变量 $y$ 做线性回归（这个时候有 $m-1$ 个自变量，并且都是二元回归方程）。同样计算F值，取最大的为 $F_j^2$ 。</p>
<p>第四步：给定显著性水平 $\alpha$ ，若 $F_j^2\ge F_{\alpha}(1,n-3)$，则引入 $x_j$ 并在新模型中记为 $x_2$。</p>
<p>第五步：继续下去，直到所有未被引入模型中的自变量 $F$ 值小于时 $F_{\alpha}(1,n-p-1)$ 算法停止。</p>
</blockquote>
<h4 id="后退法："><a href="#后退法：" class="headerlink" title="后退法："></a><strong>后退法</strong>：</h4><p>类似于前进法，就是从“所有自变量都不在模型内”改成了“所有自变量都在模型内”。类似的，“添加”就变成了“删除”。</p>
<p>前进法和后退法的缺点：前进法虽然每一次都能够选取最显著的一个自变量，但实际情况下，可能有的自变量在开始很OK，<strong>但是在其余自变量添加进去之后，它与因变量就不显著了</strong>。后退法可能会遗漏一些很重要的变量，虽然刚开始它可能与因变量之间不显著。并且后退法因为刚开始是对所有自变量做回归，所以还不可避免的会出现计算量过大的问题。</p>
<h3 id="逐步回归法"><a href="#逐步回归法" class="headerlink" title="逐步回归法"></a>逐步回归法</h3><p>一个一个引入变量，每次引入之后对已经引入的变量进行检验，<strong>如果原来引入的变量因为新的变量的引入而不再显著的时候就剔除掉</strong>。</p>
<p>引入和剔除对应两个不同的显著性水平 $\alpha_{entry}$ 和 $\alpha_{remove}$ ,一般来说遵循 $\alpha_{remove}&gt;\alpha_{entry}$</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><hr>
<p>[1] 何晓群，刘文卿. 应用回归分析[M]. 中国人民大学出版社，2015.3</p>
<p>[2] 孙荣恒. 应用数理统计.科学出版社，1998</p>
<p>[3] 陈希孺，王松桂. 近代回归分析-原理方法及应用.安徽教育出版社，1987</p>
<p>[4] <a href="https://zhuanlan.zhihu.com/p/50577508" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/50577508</a></p>

        </div>
        
            <div style="text-align:center;color: rgb(125,125,125);font-size:14px;">
               -------------本文结束 感谢您的阅读-------------
            </div>
        
        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>文章作者:</span>
                        <span>会飞的猪</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>文章链接:</span>
                        <span><a href="http://shellyandliu.github.io/2020/04/16/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%8F%98%E9%87%8F%E9%80%89%E6%8B%A9/">http://shellyandliu.github.io/2020/04/16/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%8F%98%E9%87%8F%E9%80%89%E6%8B%A9/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>版权声明:</span>
                        <span>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC-BY-NC-4.0</a> ， 转载请注明来自 会飞的猪！</span>
                    </p>
                
            </section>
        
        <section class="post-tags">
            <div>
                <span>标签:</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"> <i class="iconfont icon-tags"></i> 机器学习</a>
                    
                        <a href="/tags/%E7%BB%9F%E8%AE%A1/"> <i class="iconfont icon-tags"></i> 统计</a>
                    
                        <a href="/tags/%E4%BC%98%E5%8C%96/"> <i class="iconfont icon-tags"></i> 优化</a>
                    
                        <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"> <i class="iconfont icon-tags"></i> 线性回归</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">返回</a>
                <span> /  </span>
                <a href="/">首页</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/04/16/%E5%B8%A6%E7%BA%A6%E6%9D%9F%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E4%B8%8E%E5%A4%9A%E9%87%8D%E5%85%B1%E7%BA%BF%E6%80%A7/">带约束的线性回归问题与多重共线性</a>
            
            
            <a class="next" rel="next" href="/2020/04/16/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E8%BF%9D%E8%83%8C%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE%E7%9A%84%E6%83%85%E5%86%B5/">线性回归违背基本假设的情况</a>
            
        </section>
        <div class="post-content">
            <section style="font-size: 20px;font-weight: 700;margin-bottom: 10px;margin-top: 20px;">
                <i class="iconfont icon-comments"></i>
                <span>
                    评论
                </span>
            </section>
        </div>
        
            <section id="comments" class="comments">
              <style>
                .comments{margin:0px;padding:10px;background:#fff}
                @media screen and (max-width:800px){.comments{margin:auto;padding:10px;background:#fff}}
              </style>
              <div class="valine_comment"></div>
<!--����js����</body>֮ǰ���뼴��-->
<!--Leancloud ������:-->
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<!--Valine �ĺ��Ĵ����-->
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
  new Valine({
      el: '.valine_comment',
      app_id: 'bCvyxhtL2V59mmqAp81lAGob-gzGzoHsz#Leancloud应用的appId',
      app_key: 'J4zVKQfUjhToGJwBfLcoOHOv',
      placeholder: '欢迎大家评论交流~',
      notify: 'true',
      verify: 'true',
    });
</script>
            </section>
        
        

    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 会飞的猪 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>

<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>梯度下降法 | 会飞的猪</title><meta name="description" content="梯度下降法是最为经典的下降方法，选择梯度的负方向作为下降的方向，并且在强凸的情况下，能保证算法线性收敛性。"><meta name="keywords" content="机器学习,统计,优化方法,梯度下降"><meta name="author" content="会飞的猪"><meta name="copyright" content="会飞的猪"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="梯度下降法"><meta name="twitter:description" content="梯度下降法是最为经典的下降方法，选择梯度的负方向作为下降的方向，并且在强凸的情况下，能保证算法线性收敛性。"><meta name="twitter:image" content="http://shellyandliu.github.io/img/post.jpg"><meta property="og:type" content="article"><meta property="og:title" content="梯度下降法"><meta property="og:url" content="http://shellyandliu.github.io/2020/05/08/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><meta property="og:site_name" content="会飞的猪"><meta property="og:description" content="梯度下降法是最为经典的下降方法，选择梯度的负方向作为下降的方向，并且在强凸的情况下，能保证算法线性收敛性。"><meta property="og:image" content="http://shellyandliu.github.io/img/post.jpg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://shellyandliu.github.io/2020/05/08/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><link rel="prev" title="随机梯度下降法" href="http://shellyandliu.github.io/2020/05/09/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"><link rel="next" title="Lasso回归" href="http://shellyandliu.github.io/2020/04/16/Lasso%E5%9B%9E%E5%BD%92/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/longmao.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">11</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">12</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">2</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#梯度下降法"><span class="toc-number">1.</span> <span class="toc-text">梯度下降法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#收敛性和收敛率"><span class="toc-number">2.</span> <span class="toc-text">收敛性和收敛率</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#非强凸假设下"><span class="toc-number">2.1.</span> <span class="toc-text">非强凸假设下</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#强凸假设下"><span class="toc-number">2.2.</span> <span class="toc-text">强凸假设下</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#精确直线搜索分析"><span class="toc-number">2.2.1.</span> <span class="toc-text">精确直线搜索分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#回溯直线搜索分析"><span class="toc-number">2.2.2.</span> <span class="toc-text">回溯直线搜索分析</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考文献"><span class="toc-number">3.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(/img/post.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">会飞的猪</a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">梯度下降法</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-05-08 15:45:22"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-05-08</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-05-11 14:24:45"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-05-11</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/">-优化方法</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/05/08/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/2020/05/08/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" itemprop="commentCount"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><h1 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h1><p>下降方法的定义：即寻找算法，使得产生一个优化点列 $x^{(k)}, k= 1,\cdots,$ 其中：</p>
<blockquote>
<p>$x^{(k+1)} = x^{(k)}+t^{(k)}\Delta x^{(k)}$</p>
</blockquote>
<p>并且 $t^{(k)}\ge0$ 代表搜索步长， $\Delta x$ 表示 $\mathcal{R}^n$ 上得一个向量，称为搜索方向。$k=0,1,\cdots,$ 代表迭代次数。</p>
<p>而梯度下降法是指：使用负梯度作为搜索方向，即令 $\Delta x = -\nabla f(x)$, 算法具体步骤为：</p>
<blockquote>
<p>(1). 令 $\operatorname{dom}f$ 表示 $f$ 的定义域，给定初始点 $x^{(0)}\in \operatorname{dom}f$.</p>
<p>(2). 确定搜索步长 $t^{(k)}$.</p>
<p>(3). 计算 $x^{(k+1)}=x^{(k)}-t^{(k)}\nabla f(x^{(k)})$.</p>
<p>(4). 直到满足停止准则: $\left|\nabla f\left(x^{(k)}\right)\right|_{2} \leqslant \eta$, 其中 $\eta$ 是预先给定的阈值。</p>
</blockquote>
<h1 id="收敛性和收敛率"><a href="#收敛性和收敛率" class="headerlink" title="收敛性和收敛率"></a>收敛性和收敛率</h1><h2 id="非强凸假设下"><a href="#非强凸假设下" class="headerlink" title="非强凸假设下"></a>非强凸假设下</h2><blockquote>
<p>假设1：$f(x)$ 是一阶连续可微的凸函数，并且一阶导数满足 Lipschitz 条件，即对于任意的 $x,y\in \operatorname{dom}f$, 有 $|\nabla f(x)-\nabla f(y)|_2 \leq L|x-y|_2$.</p>
</blockquote>
<p>下面给出在假设1下，步长 $t^{(k)}$ 取固定值 $\frac{1}{L}$ 情况下的收敛性。</p>
<p>由 Lipschitz 连续性，可以得到:</p>
<blockquote>
<script type="math/tex; mode=display">
f(x)-f(y)-\nabla f(y)^T(x-y)\le \frac L2\|x-y\|_2^2\tag{1}</script></blockquote>
<p>对于任意给定的 $x$, 令 $g_x(y)=f(y)-\nabla f(x)^Ty$，则显然 $g_x(y)$ 是一阶连续可微凸函数，并且满足 Lipschitz 连续条件，由 (1) 可知，$g_x(x)-g_x(y)\le\frac L2|x-y|_2^2$, 因此 $g_x(y)$ 在 $x$ 处取最小值。并且 $\nabla g_x(y)=\nabla f(y)-\nabla f(x)$ . 因此可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
f(x)-f(y)-\nabla f(x)^T(x-y) &=g_{x}(x)-g_{x}(y) \\
& \leq g_{x}\left(y-\frac{1}{L} \nabla g_{x}(y)\right)-g_{x}(y) \\
& \leq \nabla g_{x}(y)\left(-\frac{1}{L} \nabla g_{x}(y)^{T}\right)+\frac{L}{2} \frac{1}{L^{2}}\left\|\nabla g_{x}(y)\right\|_2^{2} \\
&=-\frac{1}{2 L}\left\|\nabla g_{x}(y)\right\|_2^{2} \\
&=-\frac{1}{2L}\|\nabla f(x)-\nabla f(y)\|_2^2
\end{aligned}\tag{2}</script></blockquote>
<p>同理可得：</p>
<blockquote>
<script type="math/tex; mode=display">
f(x)-f(y)-\nabla f(x)^T(x-y) \le -\frac{1}{2L}\|\nabla f(x)-\nabla f(y)\|_2^2\tag{3}</script></blockquote>
<p>因此结合 (2)(3) 式可得：</p>
<blockquote>
<script type="math/tex; mode=display">
(\nabla f(x)^T-\nabla f(y)^T)(x-y)\ge\frac 1L\|\nabla f(x)-\nabla f(y)\|_2^2\tag{4}</script></blockquote>
<p>由于 $x^{(k+1)}=x^{(k)}-\frac 1L\nabla f(x^{(k)}) $, 在 (4) 中令 $x=x^{(k+1)},y=x^{(k)}$, 则有：</p>
<blockquote>
<script type="math/tex; mode=display">
-\frac 1L\nabla f(x^{(k)})^T(\nabla f(x^{(k+1)})-\nabla f(x^{(k)}))\ge \frac 1L \|\nabla f(x^{(k+1)})-\nabla f(x^{(k)})\|_2^2\tag{5}</script></blockquote>
<p>即：</p>
<blockquote>
<script type="math/tex; mode=display">
\|\nabla f(x^{(k+1)})\|_2^2\le\nabla f(x^{(k+1)})^T\nabla f(x^{(k)})\le \|\nabla f(x^{(k+1)})^T\|_2\|\nabla f(x^{(k)})\|_2\tag{6}</script></blockquote>
<p>因此可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\|\nabla f(x^{(k+1)})\|_2\le \|\nabla f(x^{(k)})\|_2\tag{7}</script></blockquote>
<p>则 (7) 说明 $f$ 的一阶偏导是单调递减的。这正是我们需要达到的目的。</p>
<p>令 $x^*$ 为最优点，$d_k=(x^{(k)}-x^*)$，在 (2) 中代入 $x=x^{(k)},y=x^*$,并且由 $\nabla f(x^*)=0$ ,可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
f(x^{(k)})-f(x^*)&\le \nabla f(x^{(k)})^Td_k-\frac 1{2L}\|\nabla f(x^{(k)})\|_2^2\\
&=-L(x^{(k+1)}-x^{(k)})^Td_k-\frac L{2}\|x^{(k+1)}-x^{(k)}\|_2^2\\
&=-L(d_{k+1}-d_k)^Td_k-\frac L2\|d_{k+1}-d_k\|_2^2\\
&=\frac L2(\|d_k\|_2^2-\|d_{k+1}\|_2^2).
\end{aligned}\tag{8}</script></blockquote>
<p>对 (8) 从 $0$ 到 $k$ 进行求和：</p>
<blockquote>
<script type="math/tex; mode=display">
\sum_{i=0}^k\left(f(x^{(i)})-f(x^*)\right)\le \frac L2(\|d_0\|_2^2-\|d_{i+1}\|_2^2)\le \frac L2\|d_0\|_2^2\tag{9}</script></blockquote>
<p>再一次在 (2) 中代入 $x=x^{(k+1)},y=x^{(k)}$ ，</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}f(x^{(k+1)})-f(x^{(k)})&\le \nabla f(x^{(k+1)})^T(-\frac 1L\nabla f(x^{(k)}))-\frac 1{2L}\|\nabla f(x^{(k+1)})-\nabla f(x^{(k)})\|_2^2\\
&=-\frac {1}{2L}(\|\nabla f(x^{(k+1)})\|_2^2+\|\nabla f(x^{(k)})\|_2^2)
\end{aligned}\tag{10}</script></blockquote>
<p>令 $\delta_k=f(x^{(k)})-f(x^*)$, 结合 (9)(10) 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_{i=0}^k\delta_i&=\sum_{i=0}^k\delta_i(i+1)-\sum_{i=0}^k\delta_ii\\
&=\sum_{i=1}^{k+1}\delta_{i-1}i-\sum_{i=1}^k\delta_ii\\
&=\delta_k(k+1)+\sum_{i=1}^k(\delta_{i-1}-\delta_i)i\\
&\ge \delta_k(k+1)+\sum_{i=1}^k\frac{i}{2L}(\|\nabla f(x^{(i)})\|_2^2+\|\nabla f(x^{(i-1)})\|_2^2)\\
&\ge \delta_k(k+1)+\frac{k(k+1)}{2L}\|f(x^{(k)})\|_2^2
\end{aligned}\tag{11}</script></blockquote>
<p>其中，最后一个不等式是因为 $\nabla f(x^{(k)})$ 关于 $k$ 单调递减。</p>
<p>因此结合 (9)(11) 可以得到</p>
<blockquote>
<script type="math/tex; mode=display">
(k+1)\delta_k+\frac{k(k+1)}{2L}\|f(x^{(k)})\|_2^2\le \frac L2\|d_0\|_2^2\tag{12}</script></blockquote>
<p>因此可以得到 $f(x^{(k)})-f(x^*)\le \frac{L}{2(k+1)}|x^{(0)}-x^*|_2^2$ ，且 $|\nabla f(x^{(k)})|_2^2\le \frac{L^2}{\sqrt{k(k+1)}}|x^{(0)}-x^*|_2^2$.</p>
<p>则可以说明是以算术收敛率收敛的。</p>
<h2 id="强凸假设下"><a href="#强凸假设下" class="headerlink" title="强凸假设下"></a>强凸假设下</h2><p>事实上，当 $f(x)$ 是强凸函数时，收敛率能达到线性收敛。给出下面两个假设：</p>
<blockquote>
<p>假设2：$f$ 是二次可微的凸函数，且对于给定的初始点 $x^{(0)}\in \operatorname{dom}f$ , 下水平集 $S = \{x\in\operatorname{dom}f | f(x)\le f(x^{(0)})\}$ 是闭集。</p>
<p>假设3: 对于给定的初始点 $x^{(0)}$ ,$f$ 在 $S$ 上是强凸的，即 $\nabla^2f(x)\ge mI$ 对任意的 $x\in S$ 成立。</p>
</blockquote>
<p>注：当 $f$ 是闭函数时，条件1自然成立。由于 $\operatorname{dom}f=\mathcal{R}^n$ 的连续函数是闭函数，所以这种条件下，对于任意的 $x^{(0)}$ 均能满足条件1。</p>
<p>由于 $f$ 为二次可微强凸函数L，因此对任意的 $x,y\in S$ 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
f(y)=f(x)+\nabla f(x)^{T}(y-x)+\frac{1}{2}(y-x)^{T} \nabla^{2} f(z)(y-x)\tag{13}</script></blockquote>
<p>其中 $z$ 属于线段 $[x,y]$。利用强凸性有：</p>
<blockquote>
<script type="math/tex; mode=display">
f(y)\ge f(x)+\nabla f(x)^{T}(y-x)+\frac{m}{2}\|y-x\|_2^2\tag{14}</script></blockquote>
<p>在上式中，取 $y=x,x=x^{(0)}$，可以得到：</p>
<p>$f(x)\ge f(x^{(0)})+\nabla f(x^{(0)})^{T}(x-x^{(0)})+\frac{m}{2}|x-x^{(0)}|_2^2$</p>
<p>当 $x\to\infty$ 时，$f(x)\to\infty$，与 $x\in S$ 矛盾。因此 $S$ 必为有界集，又因为$\nabla^2f(x)$ 的最大特征是 $x$ 在 $S$ 上的连续函数，所以有界，即存在常数 $M$ 使得：$\nabla^2f(x)\le MI$</p>
<p>因此有下列式子：</p>
<blockquote>
<p>$f(y)\le f(x)+\nabla f(x)^{T}(y-x)+\frac{M}{2}|y-x|_2^2$</p>
</blockquote>
<p>令 $y=x-t\nabla f(x)$, $\hat{f}(t)=f(x-t\nabla f(x))$就得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\hat{f}(t)\le f(x)-t\|\nabla f(x)\|_2^2+\frac{Mt^2}{2}\|\nabla f(x)\|_2^2\tag{15}</script></blockquote>
<h3 id="精确直线搜索分析"><a href="#精确直线搜索分析" class="headerlink" title="精确直线搜索分析"></a>精确直线搜索分析</h3><p>下面采用精确直线搜索方法，即每次迭代之前求解最优迭代步长： $t=\arg\min_{t\ge0}f(x-t\nabla f(x))$.</p>
<p>在式子 (15) 两边同时关于 $t$ 求最小，左边等于 $\hat{f}(t_{\text{exact}})$ ,其中 $t_{\text{exact}}$ 是使 $\hat{f}$ 最小的步长。右边的最小解为 $t=\frac 1M$ ，最小值为 $f(x)-(\frac{1}{2M})|\nabla f(x)|_2^2$ ，因此有:</p>
<blockquote>
<script type="math/tex; mode=display">
f(x-t\nabla f(x))\le f(x)-\frac{1}{2M}\|
\nabla f(x)\|_2^2 \tag{16}</script></blockquote>
<p>令 $p^<em>$ 为 $f$ 的最小值，在 (16) 式中减去 $p^</em>$ 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
f(x-t\nabla f(x))-p^*\le f(x)-p^*-\frac{1}{2M}\|
\nabla f(x)\|_2^2 \tag{17}</script></blockquote>
<p>另一方面，在式子 (14) 的右边对 $y$ 求最小值，因为是关于 $y$ 的二次凸函数，所以有：</p>
<blockquote>
<script type="math/tex; mode=display">
f(y)\ge f(x)-\frac{1}{2m}\|\nabla f(x)\|_2^2\tag{18}</script></blockquote>
<p>对任意 $y\in S$ 成立，所以有 $p^*\ge f(x)-\frac 1{2m}|\nabla f(x)|_2^2$，即 $|\nabla f(x)|_2^2\ge2m(f(x)-p^*)$ , 代入式子 (17) 有：</p>
<blockquote>
<script type="math/tex; mode=display">
f(x-t\nabla f(x))-p^*\le (1-\frac mM)(f(x)-p^*)\tag{19}</script></blockquote>
<p>由于 $c = 1-\frac mM&lt;1$, 所以当 $k\to\infty$ 时 $f(x^{(k)})$ 将收敛于 $p^*$，并且至多经过 $\frac{\log((f(x^{(0)})-p^*)/\epsilon)}{\log(1/c)}$ 次迭代，一定可以得到 $f(x^{(k)})-p^*\le \epsilon$.</p>
<p>显然此收敛速度至少为线性收敛。</p>
<h3 id="回溯直线搜索分析"><a href="#回溯直线搜索分析" class="headerlink" title="回溯直线搜索分析"></a>回溯直线搜索分析</h3><p>在实际操作中不必寻找最优的步长，因此一般使用非精确直线搜索，即只要能使得 $f$ 有着足够的减小就可以了。回溯直线搜索是一种简单有效的方法。</p>
<blockquote>
<p>回溯直线搜索：给定参数 $\alpha\in(0,0.5),\beta\in(0,1)$.令 $t:=1$, 若 $f(x+t\Delta x)&gt;f(x)+\alpha t\nabla f(x)^T\Delta x$, 则令 $t:=\beta t$. 直到不等号反向，将 $t$ 选取为迭代步长。</p>
</blockquote>
<p>下面说明，只要 $0\le t\le 1/M$ ，就能满足回溯停止条件：$\hat{f}(t) \leqslant f(x)-\alpha t|\nabla f(x)|_{2}^{2}$.</p>
<p>由于 $0 \leqslant t \leqslant 1 / M \Longrightarrow-t+\frac{M t^{2}}{2} \leqslant-t / 2$</p>
<p>所以由式子 (15) 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{f}(t)&\le f(x)-t\|\nabla f(x)\|_2^2+\frac{Mt^2}{2}\|\nabla f(x)\|_2^2\\
&\le f(x)-(\frac t2)\|\nabla f(x)\|_2^2\\
&\le f(x)-\alpha t\|\nabla f(x)\|_2^2
\end{aligned}\tag{20}</script></blockquote>
<p>因此回溯直线搜索将终止于 $t=1$ 或 $t\ge \frac {\beta}{M}$.</p>
<p>分别代入两种情况，可以得到一个目标函数减少的下界：</p>
<blockquote>
<script type="math/tex; mode=display">
f(x-t\nabla f(x))\le f(x)-\min\{\alpha,\beta\alpha/M\}\|\nabla f(x)\|_2^2\tag{21}</script></blockquote>
<p>两边减去 $p^*$ 并结合 (18) 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
f(x-t\nabla f(x))-p^*\le (1-\min\{2m\alpha,2\beta\alpha m/M\})(f(x)-p^*)\tag{22}</script></blockquote>
<p>收敛速度同样为线性收敛。</p>
<hr>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Nocedal, Jorge, and Stephen Wright. <em>Numerical optimization</em>. Springer Science &amp; Business Media, 2006.</p>
<p>[2] Boyd, Stephen, Stephen P. Boyd, and Lieven Vandenberghe. <em>Convex optimization</em>. Cambridge university press, 2004.</p>
<p>[3] Luenberger, David G., and Yinyu Ye. <em>Linear and nonlinear programming</em>. Vol. 2. Reading, MA: Addison-wesley, 1984.</p>
<p>[4] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. <em>Deep learning</em>. MIT press, 2016.</p>
<p>[5] 李航. 统计学习方法. 清华大学出版社，2019</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">会飞的猪</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://shellyandliu.github.io/2020/05/08/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">http://shellyandliu.github.io/2020/05/08/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://shellyandliu.github.io" target="_blank">会飞的猪</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E7%BB%9F%E8%AE%A1/">统计</a><a class="post-meta__tags" href="/tags/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/">优化方法</a><a class="post-meta__tags" href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">梯度下降</a></div><div class="post_share"><div class="social-share" data-image="/img/post.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/weixin.png" alt="微信打赏"/><div class="post-qr-code__desc">微信打赏</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/zhifubao.png" alt="支付宝打赏"/><div class="post-qr-code__desc">支付宝打赏</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/05/09/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"><img class="prev_cover lazyload" data-src="/img/post.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">随机梯度下降法</div></div></a></div><div class="next-post pull_right"><a href="/2020/04/16/Lasso%E5%9B%9E%E5%BD%92/"><img class="next_cover lazyload" data-src="/img/post.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Lasso回归</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/05/10/牛顿法/" title="牛顿法"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-10</div><div class="relatedPosts_title">牛顿法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/05/09/随机梯度下降法/" title="随机梯度下降法"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-09</div><div class="relatedPosts_title">随机梯度下降法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/Lasso回归/" title="Lasso回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">Lasso回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/一元线性回归/" title="一元线性回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">一元线性回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/岭回归和广义岭回归/" title="岭回归和广义岭回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">岭回归和广义岭回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/主成分回归和偏最小二乘回归/" title="主成分回归和偏最小二乘回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">主成分回归和偏最小二乘回归</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify: false,
  verify: false,
  appId: 'bCvyxhtL2V59mmqAp81lAGob-gzGzoHsz',
  appKey: 'J4zVKQfUjhToGJwBfLcoOHOv',
  placeholder: '留下邮箱可以收到回复提醒哦~~',
  avatar: 'monsterid',
  meta: guest_info,
  pageSize: '10',
  lang: 'zh-cn',
  recordIP: false,
  serverURLs: ''
});</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 会飞的猪</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">欢迎来到我的博客</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>
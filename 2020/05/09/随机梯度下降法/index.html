<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>随机梯度下降法 | 会飞的猪</title><meta name="description" content="在机器学习中，传统的梯度下降法在每一步迭代时需要用到所有样本的信息，当样本数量过于巨大时，算法将难以运行。随机梯度下降算法通过随机的选取样本点，在减小计算量的同时，也能保证其算法的收敛性。"><meta name="keywords" content="机器学习,统计,优化方法,随机梯度下降,小批量梯度下降"><meta name="author" content="会飞的猪"><meta name="copyright" content="会飞的猪"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="随机梯度下降法"><meta name="twitter:description" content="在机器学习中，传统的梯度下降法在每一步迭代时需要用到所有样本的信息，当样本数量过于巨大时，算法将难以运行。随机梯度下降算法通过随机的选取样本点，在减小计算量的同时，也能保证其算法的收敛性。"><meta name="twitter:image" content="http://shellyandliu.github.io/img/post.jpg"><meta property="og:type" content="article"><meta property="og:title" content="随机梯度下降法"><meta property="og:url" content="http://shellyandliu.github.io/2020/05/09/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"><meta property="og:site_name" content="会飞的猪"><meta property="og:description" content="在机器学习中，传统的梯度下降法在每一步迭代时需要用到所有样本的信息，当样本数量过于巨大时，算法将难以运行。随机梯度下降算法通过随机的选取样本点，在减小计算量的同时，也能保证其算法的收敛性。"><meta property="og:image" content="http://shellyandliu.github.io/img/post.jpg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://shellyandliu.github.io/2020/05/09/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"><link rel="prev" title="牛顿法" href="http://shellyandliu.github.io/2020/05/10/%E7%89%9B%E9%A1%BF%E6%B3%95/"><link rel="next" title="梯度下降法" href="http://shellyandliu.github.io/2020/05/08/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/longmao.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">11</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">12</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">2</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#机器学习中的优化问题"><span class="toc-number">1.</span> <span class="toc-text">机器学习中的优化问题</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#梯度下降算法"><span class="toc-number">2.</span> <span class="toc-text">梯度下降算法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#随机梯度下降算法"><span class="toc-number">3.</span> <span class="toc-text">随机梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#收敛性"><span class="toc-number">3.1.</span> <span class="toc-text">收敛性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#固定步长"><span class="toc-number">3.1.1.</span> <span class="toc-text">固定步长</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#递减步长"><span class="toc-number">3.1.2.</span> <span class="toc-text">递减步长</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考文献"><span class="toc-number">4.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(/img/post.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">会飞的猪</a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">随机梯度下降法</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-05-09 18:51:36"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-05-09</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-05-11 16:07:17"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-05-11</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/">-优化方法</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/05/09/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/2020/05/09/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/" itemprop="commentCount"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><h1 id="机器学习中的优化问题"><a href="#机器学习中的优化问题" class="headerlink" title="机器学习中的优化问题"></a>机器学习中的优化问题</h1><p>在机器学习问题（监督学习）中，设 $h(x;\beta)$ 为预测函数，其中 $x$ 表示输入，$\beta$ 表示参数，预测函数有着固定的形式，而损失函数设为 $l(h(x;\beta),y)$  , 其中 $h(x;\beta)$ 和 $y$ 分别表示预测输出和真实输出。</p>
<p>理想情况下，我们希望选择参数 $\beta$ 来最小化期望损失函数，设 $P(x,y)$ 表示输入和真实输出的概率分布，则我们的希望最小化的目标函数为：</p>
<blockquote>
<script type="math/tex; mode=display">
R(\beta)=\int l(h(x;\beta),y)d P(x,y)=E_{P(x,y)}[l(h(x;\beta),y)]</script></blockquote>
<p>然而这样的目标永远不能达到，因为我们不知道 $P(x,y)$ 的真实分布。因此在实践中，我们一般寻求解决一个估计期望风险R的问题，若我们拥有 $n$ 个彼此独立的样本数据 $\{(x_i,y_i)\}_{i=1}^n$  。则我们可以定义经验风险函数：</p>
<blockquote>
<script type="math/tex; mode=display">
R_n(\beta)=\frac 1n\sum_{i=1}^nl(h(x_i;\beta),y_i)</script></blockquote>
<p>在样本独立性要求的情况下，通过大数定律我们可以得到当 $n\to\infty$ 时， $R_n\to R$ 。因此我们的目标是优化在拥有 $n$ 个训练样本的情况下的经验风险函数 $R_n(\beta)$.</p>
<p>为了方便表示令 $f(\beta;\xi):=l(h(x;\beta),y)$, $f_i(\beta) := f(\beta;\xi_i)$, 其中 $\xi_i$ 表示样本点 $(x_i,y_i)$则期望风险函数为：</p>
<blockquote>
<script type="math/tex; mode=display">
R(\beta)=E_{P}(f(\beta;\xi))</script></blockquote>
<p>经验风险函数为： </p>
<blockquote>
<script type="math/tex; mode=display">
R_n(\beta)=\frac 1n\sum_{i=1}^nf_i(\beta).</script></blockquote>
<h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><p>在机器学习中，将传统的梯度下降法称为批量梯度下降法，其具体过程为：</p>
<blockquote>
<p>(1). 给定初值 $\beta_0$,</p>
<p>(2). 在第 $k$ 次迭代时，确定学习率（迭代步长）$\alpha_k$。</p>
<p>(3). $\beta_{k+1}=\beta_k-\alpha_k\nabla R_n(\beta_k)=\beta_k-\frac{\alpha_k}{n}\sum_{i=1}^n\nabla f_i(\beta_k)$.</p>
<p>(4). 直到满足停止准则。</p>
</blockquote>
<p>若采用上述的批量梯度下降算法来最小化 $R_n$。当样本数量 $n$ 较大时，计算量将非常大。原因在于：</p>
<p>(1). 由于$n$ 个样本均值的标准差是 $\sigma/\sqrt n$，其中 $\sigma$ 是真是样本的标准差，则通过增加样本数量来估计梯度的回报是低于线性的。</p>
<p>(2). 当训练集冗余时，及相同的样本有 $m$ 个，那么采用梯度下降估计会花去 $m$ 倍的运算时间。</p>
<h1 id="随机梯度下降算法"><a href="#随机梯度下降算法" class="headerlink" title="随机梯度下降算法"></a>随机梯度下降算法</h1><p>随机梯度下降法的具体步骤：</p>
<blockquote>
<p>(1). 给定初值 $\beta_0$,</p>
<p>(2). 在第 $k$ 次迭代时，确定学习率（迭代步长）$\alpha_k$。</p>
<p>(3). $\beta_{k+1}=\beta_k-{\alpha_k}\nabla f_{i_k}(\beta_k)$. 其中 $i_k$ 是从 $\{1,\cdots,n\}$ 中随机选取。</p>
<p>(4). 直到满足停止准则。</p>
</blockquote>
<p>由于 $i_k$ 为随机选取的，因此 $\beta_k$ 为一个随机过程，下面证明其算法的收敛性。</p>
<p>我们设 $\xi_k$ 为第 $k$ 次迭代随机选择的样本（是一个随机变量）。不失一般性，设</p>
<blockquote>
<script type="math/tex; mode=display">
F(\beta)=\left\{\begin{array}{c}R(\beta)=\mathbb{E}[f(\beta; \xi)] \\ \text { or } \\ R_{n}(\beta)=\frac{1}{n} \sum_{i=1}^{n} f_{i}(\beta)\end{array}\right.</script></blockquote>
<p>若迭代过程中，每次选择 $n_k$ 个样本，则该方程称为小批量梯度下降法。不是一般性，为了使算法包含随机梯度，小批量梯度下降，随机牛顿（拟牛顿）的情况，令：</p>
<blockquote>
<script type="math/tex; mode=display">
g\left(\beta_{k}, \xi_{k}\right)=\left\{\begin{array}{l}
\nabla f\left(\beta_{k} ; \xi_{k}\right) \\
\frac{1}{n_{k}} \sum_{i=1}^{n_{k}} \nabla f\left(\beta_{k} ; \xi_{k, i}\right) \\
H_{k} \frac{1}{n_{k}} \sum_{i=1}^{n_{k}} \nabla f\left(\beta_{k} ; \xi_{k, i}\right)
\end{array}\right.</script></blockquote>
<p>则每次迭代为：$\beta_{k+1}=\beta_k-{\alpha_k}g(\beta_k,\xi_k)$</p>
<p>我们用 $\mathbb E_{\xi_k}[\cdot]$ 表示在已知迭代序列 $\beta_k$ 下，关于 $\xi_k$ 的期望。用 $E$ 表示所有随机变量的联合分布的期望值，即 $\mathbb E[F(\beta_k)]=\mathbb{E}_{\xi_{1}}\mathbb{E}_{\xi_{2}}\cdots\mathbb{E}_{\xi_{k-1}}[F(\beta_k)]$</p>
<p>做下列假设：</p>
<blockquote>
<p>假设1：目标函数 $F$ 是连续可微的，并且偏导满足 Lipschitz 连续条件：对任意的 $\beta,\bar{\beta}$, 有 $|\nabla F(\beta )-\nabla F(\bar{\beta})|_{2} \leq L|\beta-\bar{\beta}|_{2}$.</p>
<p>假设2：对于目标函数 $F$，和随机梯度下降算法满足以下条件：</p>
<p>(1). 迭代序列 $\{\beta_k\}$ 属于一个开集中，并且在这个开集满足 $F$ 有下界 $F_{inf}$。</p>
<p>(2). 存在常数 $\mu_G\ge \mu&gt;0$ , 满足对任意的 $k\in\mathbb{N},$ 有$<br>\nabla F\left(\beta_{k}\right)^{T} \mathbb{E}_{\xi_{k}}\left[g\left(\beta_{k}, \xi_{k}\right)\right] \geq \mu\left|\nabla F\left(\beta_{k}\right)\right|_{2}^{2},<br>\left|\mathbb{E}_{\xi_{k}}\left[g\left(\beta_{k}, \xi_{k}\right)\right]\right|_{2} \leq \mu_{G}\left|\nabla F\left(\beta_{k}\right)\right|_{2}<br>$</p>
<p>(3). 存在常数 $M\ge0,M_V\ge0$, 对任意的 $k\in\mathbb{N}$ , 有：$\mathbb{V}_{\xi_{k}}\left[g\left(\beta_{k}, \xi_{k}\right)\right] \leq M+M_{V}\left|\nabla F\left(\beta_{k}\right)\right|_{2}^{2}$，其中 $\mathbb{V}_{\xi_{k}}\left[g\left(\beta_{k}, \xi_{k}\right)\right]:=\mathbb{E}_{\xi_{k}}\left[\left|g\left(\beta_{k}, \xi_{k}\right)\right|_{2}^{2}\right]-\left|\mathbb{E}_{\xi_{k}}\left[g\left(\beta_{k}, \xi_{k}\right)\right]\right|_{2}^{2}$.</p>
</blockquote>
<p>其中条件2的2是为了保证 $-g(\beta_k,\xi_k)$ 能使 $F$ 的梯度从 $\beta_k$ 开始是足够的下降方向，并且其范数与梯度范数相当。若 $g(\beta_k,\xi_k)$ 是 $\nabla F(\beta_k)$ 的无偏估计，则显然 $\mu_G=\mu=1$. 条件2的3是为了限制 $g(\beta_k,\xi_k)$ 的方差，但允许它关于梯度二次增长。为了简单起见，我们对 $F$ 做一个强凸性假设：</p>
<blockquote>
<p>假设3：函数 $F$ 是强凸的，即存在常数 $c$ 满足对任意的 $\beta,\bar{\beta}$, 有 $F(\bar{\beta})\ge F(\beta)+\nabla F(\beta)^T(\bar{\beta}-\beta)+\frac 12c|\bar{\beta}-\beta|_2^2$.</p>
</blockquote>
<p>在强凸性假设的存在下， $F$ 将存在最小值点 $\beta^*$, 并且 $F^*:=F(\beta^*)=F_{inf}$</p>
<h2 id="收敛性"><a href="#收敛性" class="headerlink" title="收敛性"></a>收敛性</h2><p>显然，在条件1的假设下我们有：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned} F\left(\beta_{k+1}\right)-F\left(\beta_{k}\right) & \leq \nabla F\left(\beta_{k}\right)^{T}\left(\beta_{k+1}-\beta_{k}\right)+\frac{1}{2} L\left\|\beta_{k+1}-\beta_{k}\right\|_{2}^{2} \\ & \leq-\alpha_{k} \nabla F\left(\beta_{k}\right)^{T} g\left(\beta_{k}, \xi_{k}\right)+\frac{1}{2} \alpha_{k}^{2} L\left\|g\left(\beta_{k}, \xi_{k}\right)\right\|_{2}^{2} \end{aligned}</script></blockquote>
<p>两边取期望可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\mathbb{E}_{\xi_{k}}\left[F\left(\beta_{k+1}\right)\right]-F\left(\beta_{k}\right) \leq-\alpha_{k} \nabla F\left(\beta_{k}\right)^{T} \mathbb{E}_{\xi_{k}}\left[g\left(\beta_{k}, \xi_{k}\right)\right]+\frac{1}{2} \alpha_{k}^{2} L \mathbb{E}_{\xi_{k}}\left[\left\|g\left(\beta_{k}, \xi_{k}\right)\right\|_{2}^{2}\right]\tag{1}</script></blockquote>
<p>又由条件2的2、3，有：</p>
<blockquote>
<script type="math/tex; mode=display">
\mathbb{E}_{\xi_{k}}\left[\left\|g\left(w_{k}, \xi_{k}\right)\right\|_{2}^{2}\right] \leq M+M_{G}\left\|\nabla F\left(w_{k}\right)\right\|_{2}^{2}\tag{2}</script><p>其中 $M_{G}:=M_{V}+\mu_{G}^{2} \geq \mu^{2}&gt;0$</p>
</blockquote>
<p>因此结合 (1)(2) 和假设2的2可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}_{\xi_{k}}\left[F\left(\beta_{k+1}\right)\right]-F\left(\beta_{k}\right) & \leq-\alpha_{k} \nabla F\left(\beta_{k}\right)^{T} \mathbb{E}_{\xi_{k}}\left[g\left(\beta_{k}, \xi_{k}\right)\right]+\frac{1}{2} \alpha_{k}^{2} L \mathbb{E}_{\xi_{k}}\left[\left\|g\left(\beta_{k}, \xi_{k}\right)\right\|_{2}^{2}\right] \\
& \leq-\mu \alpha_{k}\left\|\nabla F\left(\beta_{k}\right)\right\|_{2}^{2}+\frac{1}{2} \alpha_{k}^{2} L \mathbb{E}_{\xi_{k}}\left[\left\|g\left(\beta_{k}, \xi_{k}\right)\right\|_{2}^{2}\right]\\
&\le-(\mu-\frac12\alpha_kLM_G)\alpha_k\|\nabla F(\beta_k)\|_2^2+\frac12\alpha_k^2LM
\end{aligned}\tag{3}</script></blockquote>
<p>由假设3容易推出对任意的 $\beta$ ：</p>
<blockquote>
<script type="math/tex; mode=display">
2 c\left(F(\beta)-F_{*}\right) \leq\|\nabla F(\beta)\|_{2}^{2}\tag{4}</script></blockquote>
<h3 id="固定步长"><a href="#固定步长" class="headerlink" title="固定步长"></a>固定步长</h3><p>下面给出确定性步长的收敛性结果：</p>
<blockquote>
<p>定理1：在假设1、2、3下，若随机梯度下降法采用固定步长 $\alpha_k = \bar{\alpha}$，且满足 </p>
<script type="math/tex; mode=display">
0<\bar{\alpha}\le \frac{\mu}{LM_G}\tag{5}</script><p>则期望下与最优值的误差满足：</p>
<p>$\mathbb{E}\left[F\left(\beta_{k}\right)-F_{*}\right] \leq \frac{\bar{\alpha} L M}{2 c \mu}+(1-\bar{\alpha} c \mu)^{k-1}\left(F\left(\beta_{1}\right)-F_{*}-\frac{\bar{\alpha} L M}{2 c \mu}\right) \xrightarrow{k\to\infty}\frac {\bar{\alpha}LM}{2c\mu}\tag{6}$</p>
</blockquote>
<p>由 (3)(4)(5) 可得：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}_{\xi_{k}}\left[F\left(\beta_{k+1}\right)\right]-F\left(\beta_{k}\right) & \leq-(\mu-\frac12\alpha_kLM_G)\alpha_k\|\nabla F(\beta_k)\|_2^2+\frac12\alpha_k^2LM\\
&\le-\frac 12 \bar{\alpha}\mu\|\nabla F(\beta_k)\|_2^2+\frac12\bar{\alpha}^2LM\\
&\le -\bar{\alpha}c\mu(F(\beta_k)-F^*)+\frac12\bar{\alpha}^2LM
\end{aligned}\tag{7}</script></blockquote>
<p>两边同时减去 $F^*$ 并取期望的：</p>
<blockquote>
<script type="math/tex; mode=display">
\mathbb E[F(\beta_{k+1})-F^*]\le (1-\bar{\alpha}c\mu)\mathbb E[F(\beta_k)-F^*]+\frac12\bar{\alpha}^2LM</script></blockquote>
<p>因此两边同时减掉 $\bar{\alpha}LM/(2c\mu)$ ，可得：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[F\left(\beta_{k+1}\right)-F_{*}\right]-\frac{\bar{\alpha} L M}{2 c \mu} & \leq(1-\bar{\alpha} c \mu) \mathbb{E}\left[F\left(\beta_{k}\right)-F_{*}\right]+\frac{\bar{\alpha}^{2} L M}{2}-\frac{\bar{\alpha} L M}{2 c \mu} \\
&=(1-\bar{\alpha} c \mu)\left(\mathbb{E}\left[F\left(\beta_{k}\right)-F_{*}\right]-\frac{\bar{\alpha} L M}{2 c \mu}\right)
\end{aligned}\tag{8}</script></blockquote>
<p>由 (5) 可知：</p>
<blockquote>
<script type="math/tex; mode=display">
0<\bar{\alpha} c \mu \leq \frac{c \mu^{2}}{L M_{G}} \leq \frac{c \mu^{2}}{L \mu^{2}}=\frac{c}{L} \leq 1\tag{9}</script></blockquote>
<p>得证。</p>
<p>如果 $g(\beta_k,\xi_k)$ 是 $\nabla F(\beta_k)$ 的无偏估计，则 $\mu_G=\mu=1$, 又若 $g(\beta_k,\xi_k)$ 不含噪声的话，则 $M_G=1$, 在这种情况下，$\bar{\alpha}\in (0,1/L]$ , 与经典的批量梯度下降算法一致。</p>
<p>可以看到若采用固定步长的随机梯度下降算法，则最终将不会收敛到最优值，而与最优值之间相差 $\frac{\bar{\alpha}LM}{2c\mu} $. 若梯度计算中没有噪声，或者噪声随着 $|\nabla F(\beta_k)|_2^2$ 衰减到 $0$ (即 $M=0$ .)。则可以得到最优值的线性收敛。 这是具有足够小的正步长的批量梯度方法的标准结果。当梯度计算有噪声时，显然会失去这一特性。 但我们仍然可以使用固定的步长，并确保预期的目标值将线性收敛到最优值的邻域，在某一点之后，梯度估计中的噪声阻止了进一步的进展。选择较小的步长会使收敛速度中的收缩常数恶化，但允许接近最优值。</p>
<p>因为这个原因，在实践中选择固定步长时，我们通常采用如下方法：</p>
<blockquote>
<p>(1). 选择 $\alpha_1\in(0,\frac{\mu}{LM_G})$, 运行固定步长的随机梯度下降算法。直到迭代从 $k_1=1$ 进行到了 $k_2$ 步。其中 $\beta_{k_2}$ 是当迭代值与最优值之间的差值小于渐进最优值的两倍，即：$\mathbb E[F(\beta_{k_{2}})-F^*]\le 2F_{\alpha_1}$, 其中 $F_{\alpha}:=\frac{\alpha LM}{2c\mu}$。</p>
<p>(2). 令第 $k_r$ 次迭代后的步长为 $\{\alpha_{r+1}\}=\{\alpha_12^{-r}\}$，依次进行下去。则最终 $\{F_{\alpha_r}\}=\{\frac{\alpha_r LM}{2c\mu}\}\to0$, 即迭代误差最终将趋于 $0$.</p>
</blockquote>
<p>进一步分析，可以得到，若采用这种方式 $\mathbb E[F(\beta_{k_{r+1}})-F^*]\le 2F_{\alpha_r}$, 其中 $\mathbb E[F(\beta_{k_r})-F^*]\approx 2F_{\alpha_{r-1}}=4F_{\alpha_r}$.则有定理1的迭代表达式 (8) 可得：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{array}{c}
\left(1-\alpha_{r} c \mu\right)^{\left(k_{r+1}-k_{r}\right)}\left(4 F_{\alpha_{r}}-F_{\alpha_{r}}\right) \leq F_{\alpha_{r}} \\
\Rightarrow k_{r+1}-k_{r} \geq \frac{\log (1 / 3)}{\log \left(1-\alpha_{r} c \mu\right)} \approx \frac{\log (3)}{\alpha_{r} c \mu}=\mathcal{O}\left(2^{r}\right)
\end{array}\tag{10}</script></blockquote>
<p>也就是说每次步长减半，所需迭代次数将会翻倍。这将是次线性的收敛率。</p>
<h3 id="递减步长"><a href="#递减步长" class="headerlink" title="递减步长"></a>递减步长</h3><blockquote>
<p>定理2：在假设1、2、3下，若随机梯度下降法采用递减步长 $\alpha_k $ 满足对任意的 $k\in \mathbb N$, </p>
<script type="math/tex; mode=display">
\alpha_k=\frac{\lambda}{\gamma+k}\tag{11}</script><p>其中 $\lambda&gt;\frac1{c\mu}$, 并且 $\gamma&gt;0,\alpha_1\le \frac{\mu}{LM_G}$.</p>
<p>则对任意的 $k\in\mathbb N$,满足：</p>
<script type="math/tex; mode=display">
\mathbb E[F(\beta_k)-F^*]\le \frac{\nu}{\gamma+k}，\tag{12}</script><p>其中:</p>
<script type="math/tex; mode=display">
\nu:=\max \left\{\frac{\beta^{2} L M}{2(\beta c \mu-1)},(\gamma+1)\left(F\left(w_{1}\right)-F_{*}\right)\right\}\tag{13}</script></blockquote>
<p>下面证明之：</p>
<p>由 (11) 可知 $\alpha_k LM_G\le \alpha_1LM_G\le \mu$，因此通过 (3)(4) 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}_{\xi_{k}}\left[F\left(\beta_{k+1}\right)\right]-F\left(\beta_{k}\right) & \leq-(\mu-\frac12\alpha_kLM_G)\alpha_k\|\nabla F(\beta_k)\|_2^2+\frac12\alpha_k^2LM\\
&\le-\frac 12 \alpha_k\mu\|\nabla F(\beta_k)\|_2^2+\frac12\alpha_k^2LM\\
&\le -\alpha_kc\mu(F(\beta_k)-F^*)+\frac12\alpha_k^2LM
\end{aligned}\tag{14}</script></blockquote>
<p>同样的两边减去 $F^*$ 的：</p>
<blockquote>
<script type="math/tex; mode=display">
\mathbb E[F(\beta_{k+1})-F^*]\le (1-\alpha_kc\mu)\mathbb E[F(\beta_k)-F^*]+\frac12\alpha_k^2LM\tag{15}</script></blockquote>
<p>由于当 $k=1$ 时，(12) 显然满足，若对任意的 $k\ge 1$, （12）式成立，则：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[F\left(\beta_{k+1}\right)-F_{*}\right] & \leq\left(1-\frac{\lambda c \mu}{\hat{k}}\right) \frac{\nu}{\hat{k}}+\frac{\lambda{2} L M}{2 \hat{k}^{2}} \quad( \hat{k}:=\gamma+k) \\
&=\left(\frac{\hat{k}-\lambda c \mu}{\hat{k}^{2}}\right) \nu+\frac{\lambda^{2} L M}{2 \hat{k}^{2}} \\
&=\left(\frac{\hat{k}-1}{\hat{k}^{2}}\right) \nu \underbrace{-\left(\frac{\lambda c \mu-1}{\hat{k}^{2}}\right) \nu+\frac{\lambda^{2} L M}{2 \hat{k}^{2}}}_{\text {因为 $\nu$ 的非负性}} \leq \frac{\nu}{\hat{k}+1}
\end{aligned}</script><p>其中最后一个不等式是因为 $\hat{k}^2\ge (\hat{k}+1)(\hat{k}-1)$.</p>
</blockquote>
<p>证毕。</p>
<p>实际上当迭代步长满足 $\sum_{k=1}^{\infty}\alpha_k=\infty,\sum_{k=1}^{\infty}\alpha_k^2&lt;\infty$ 时，仍能证明迭代算法满足收敛性。在实践中，一般会线性衰减迭代步长直到第 $l$ 次迭代：$\alpha_k=(1-\nu)\alpha_1+\nu\alpha_l$, 其中 $\nu=\frac{k}{l}$, 在第 $l$ 步迭代之后，一般使 $\alpha$ 保持常数。通常 $\alpha_l$ 应设为大约 $\alpha_1$的百分之一。</p>
<blockquote>
<p><strong>remark1</strong>：两种步长选取方法中强凸性所起到的作用不一样。在固定步长中，步长的上界与$c$ 无关。在递减步长的方法中，$\alpha_1$ 的上界仍与 $c$ 无关，但是步长参数 $\lambda$ 必须要大于 $\frac1{c\mu}$.这是至关重要的。 </p>
<p><strong>remark2</strong>：初始点所起到的作用。在固定步长的情况下，初始差距 $F(\beta_1)-F^*$ 将以指数递减形式出现。而在递减步长的情况下，可以简单的消除初始作用点的影响。例如假设从一个固定步长 $\bar{\alpha}$ 开始运行随机梯度下降算法。直到得到一个点 $\beta_1$ ，满足 $F(\beta_1)-F^*\le \bar{\alpha}LM/(2c\mu)$. 然后令 $\alpha_1=\bar{\alpha}$，选择定理2选择 $\lambda,\gamma$.则：$(\gamma+1) \mathbb{E}\left[F\left(\beta_{1}\right)-F_{*}\right] \leq \lambda \alpha_{1}^{-1} \frac{\alpha_{1} L M}{2 c \mu}=\frac{\lambda L M}{2 c \mu}&lt;\frac{\lambda^{2} L M}{2(\lambda c \mu-1)}$. 也就是说初始点 $\beta_1$ 的作用将消失。</p>
<p><strong>remark3</strong>：随机梯度下降算法和小批量随机梯度下降算法的比较。假设 $n_k$ 为第 $k$ 次迭代，小批量算法所使用的样本个数， $n_k\ll n$. 则 $g(\beta_k,\xi_k)$ 将比随机梯度下降算法增加 $n_k$ 倍的计算量。另一方面，由于选择样本迭代所带来的方差将减为 $1/n_k$. 也就是说 $M,M_V$ 将变为 $M/n_k,M_V/n_k$, 为了方便起见，考虑固定步长的迭代算法，则小批量梯度下降算法将有：</p>
<p>$\mathbb{E}\left[F\left(\beta_{k}\right)-F_{*}\right] \leq \frac{\bar{\alpha} L M}{2 c \mu n_{k}}+[1-\bar{\alpha} c \mu]^{k-1}\left(F\left(\beta_{1}\right)-F_{*}-\frac{\bar{\alpha} L M}{2 c \mu n_{k}}\right)$</p>
<p>在随机梯度下降算法中，取步长为 $\bar{\alpha}/n_k$ ，则：</p>
<p>$\mathbb{E}\left[F\left(\beta_{k}\right)-F_{*}\right] \leq \frac{\bar{\alpha} L M}{2 c \mu n_{k}}+[1-\frac{\bar{\alpha} c \mu}{n_k}]^{k-1}\left(F\left(\beta_{1}\right)-F_{*}-\frac{\bar{\alpha} L M}{2 c \mu n_{k}}\right)$</p>
<p>也就是说在与 $F^*$ 相同逼近程度下，小批量梯度下降算法虽然每次迭代增加了 $n_k$ 倍的计算量，但将比随机梯度下降算法减少 $n_k$ 倍的迭代量。因此从这方面来看，两种算法的运算量差不多。但是在实际运算中，考虑到GPU的运算效率，小批量梯度下降算法相较于随机梯度下降算法仍有优势。</p>
</blockquote>
<p>由于随机梯度下降算法和小批量梯度下降算法在学习过程中加入了噪声，因此它们会有一些正则化的效果。而泛化误差通常在随机梯度下降算法时最好（即批量大小为1时）。</p>
<hr>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. <em>Deep learning</em>. MIT press, 2016.</p>
<p>[2] Bottou, Léon, Frank E. Curtis, and Jorge Nocedal. “Optimization methods for large-scale machine learning.” <em>Siam Review</em> 60.2 (2018): 223-311.</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">会飞的猪</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://shellyandliu.github.io/2020/05/09/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/">http://shellyandliu.github.io/2020/05/09/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://shellyandliu.github.io" target="_blank">会飞的猪</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E7%BB%9F%E8%AE%A1/">统计</a><a class="post-meta__tags" href="/tags/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/">优化方法</a><a class="post-meta__tags" href="/tags/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">随机梯度下降</a><a class="post-meta__tags" href="/tags/%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">小批量梯度下降</a></div><div class="post_share"><div class="social-share" data-image="/img/post.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/weixin.png" alt="微信打赏"/><div class="post-qr-code__desc">微信打赏</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/zhifubao.png" alt="支付宝打赏"/><div class="post-qr-code__desc">支付宝打赏</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/05/10/%E7%89%9B%E9%A1%BF%E6%B3%95/"><img class="prev_cover lazyload" data-src="/img/post.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">牛顿法</div></div></a></div><div class="next-post pull_right"><a href="/2020/05/08/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><img class="next_cover lazyload" data-src="/img/post.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">梯度下降法</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/05/08/梯度下降/" title="梯度下降法"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-08</div><div class="relatedPosts_title">梯度下降法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/05/10/牛顿法/" title="牛顿法"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-10</div><div class="relatedPosts_title">牛顿法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/Lasso回归/" title="Lasso回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">Lasso回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/一元线性回归/" title="一元线性回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">一元线性回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/岭回归和广义岭回归/" title="岭回归和广义岭回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">岭回归和广义岭回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/主成分回归和偏最小二乘回归/" title="主成分回归和偏最小二乘回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">主成分回归和偏最小二乘回归</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify: false,
  verify: false,
  appId: 'bCvyxhtL2V59mmqAp81lAGob-gzGzoHsz',
  appKey: 'J4zVKQfUjhToGJwBfLcoOHOv',
  placeholder: '留下邮箱可以收到回复提醒哦~~',
  avatar: 'monsterid',
  meta: guest_info,
  pageSize: '10',
  lang: 'zh-cn',
  recordIP: false,
  serverURLs: ''
});</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 会飞的猪</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">欢迎来到我的博客</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="会飞的猪">





<link rel="stylesheet" href="/fonts/iconfont_new/iconfont.css">


<title>牛顿法 | 会飞的猪</title>



    <link rel="icon" href="/image/longmao.jpg">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 4.2.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">会飞的猪</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/categories">分类</a>
                
                    <a class="menu-item" href="/tags">标签</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">会飞的猪</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/categories">分类</a>
                
                    <a class="menu-item" href="/tags">标签</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">展开目录</a>
        <a onclick="go_top()">回到顶部</a>
        <a onclick="go_bottom()">到达底部</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">牛顿法</h1>
            
                <div class="post-meta">
                    
                        作者: <a itemprop="author" rel="author" href="/">会飞的猪</a>
                    

                    
                        <span class="post-time">
                        时间: <a href="#">五月 10, 2020&nbsp;&nbsp;18:15:26</a>
                        </span>
                    
                    
                        <span class="post-category">
                    类别:
                            
                                <a href="/categories/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/">-优化方法</a>
                            
                        </span>
                    
                    
                        <span class="post-time">
                            <span class="post-meta-item-icon">
                              <i class="fa fa-keyboard-o"></i>
                              <span class="post-meta-item-text">  字数统计: </span>
                              <span class="post-count"><a href="#">5.5k字</a></span>
                            </span>
                        </span>
                        <span class="post-time">
                            <span class="post-meta-item-icon">
                            <i class="fa fa-hourglass-half"></i>
                            <span class="post-meta-item-text">  阅读时长: </span>
                            <span class="post-count"><a href="#">26分</a></span>
                            </span>
                        </span>
                    

                    
                </div>
            
        </header>

        <div class="post-content">
            <h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>在梯度下降法中，我们选择的下降方向为 $\nabla f$ , 即 $f$ 的梯度，实际上该算法仅考虑了函数在该点处的下降速度，并没有考虑在下一点处下降速度是增加还是减少。牛顿法则是同时考虑了函数 $f$ 的二阶导数。</p>
<p>牛顿法的具体算法为：</p>
<blockquote>
<p>(1). 选取初始点 $x^{(1)}$。</p>
<p>(2). 在第 $k$ 步迭代时，计算 $\nabla^2f(x^{(k)})$ 和 $\nabla f(x^{(k)})$ .</p>
<p>(3). 置 $x^{(k+1)}=x^{(k)}-(\nabla^2f(x^{(k)}))^{-1}\nabla f(x^{(k)})$.</p>
<p>(4). 直到满足停止准则： $\left|\nabla f\left(x^{(k)}\right)\right|_{2} \leqslant \eta$, 其中 $\eta$ 是预先给定的阈值。</p>
</blockquote>
<p>其中 $(\nabla^2f(x^{(k)}))$ 被称为 Hessian 矩阵，需要它可逆。$\nabla^2f(x^{(k)}))^{-1}\nabla f(x^{(k)})$ 为牛顿法的下降方向，称为牛顿步径。</p>
<p>下面给出为什么取这个下降方向的解释：</p>
<p>考虑在 $x$ 处对 $f$ 进行二阶 Talor 展开：</p>
<blockquote>
<script type="math/tex; mode=display">
f(x+v)\approx f(x)+\nabla f(x)^T v+\frac12v^T\nabla^2f(x)v\tag{1}</script></blockquote>
<p>这是关于 $v$ 的二次函数，在 $v=-(\nabla^2f(x))^{-1}\nabla f(x)$ 处函数达到最小。则若 $f$ 时二次的，那么 $x-(\nabla^2f(x))^{-1}\nabla f(x)$ 是 $f$ 的精确最优解。若 $f$ 是近似二次的，直观上 $x-(\nabla^2f(x))^{-1}\nabla f(x)$ 应该是 $f$ 的最优解的一个很好的近似估计。并且由于 $f(x+v)$ 的二阶展开在 $v=-(\nabla^2f(x))^{-1}\nabla f(x)$ 处取最小，因此，相较于梯度下降法，将提供更加迅速的下降。</p>
<p>另一方面，考虑梯度下降法在强凸条件下的收敛性：</p>
<p> 即 $f$ 在经历最多 $\frac{\log((f(x^{(0)})-p^*)/\epsilon)}{\log(1/c)}$ 次迭代，一定可以得到 $f(x^{(k)})-p^*\le \epsilon$.其中 $c=1-\frac mM$. 也就是说，当 $\frac mM$ 较小时，迭代次数受 $\log (1 / c)=-\log (1-m / M) \approx m / M$ 的影响，将随着 $\frac Mm$ 的增大，线性增长。当 $\frac Mm$ 过于巨大时，将使得迭代速度缓慢到难以进行。</p>
<p>若考虑对 $x$ 做一个坐标变换 $\bar{x}=P^{1/2}x$, 其中 $P$ 为对称正定矩阵。则最优化问题为优化目标函数 $\bar{f}(\bar{x}):=f(P^{-1/2}\bar{x})=f(x)$,此时 $\bar{f}$ 在最优点处的 Hessian 矩阵为： $P^{-1/2}\nabla^2 f(x^*)P^{-1/2}$. 若 $P$ 取为最优点处的 Hessian 矩阵的近似矩阵 $\hat{H}$ 来代替。则 $\bar{f}$ 在最优点处的 Hessian 矩阵近似为 $\hat{H}^{-1/2}\nabla^2\hat{H}^{-1/2}\approx I$。将大大题高迭代效率。而 $\bar{f}(\bar{x})$ 按梯度下降法导出的下降方向为 $-(\nabla^2f(x))^{-1}\nabla f(x)$。</p>
<p>为方便起见，下面将第 $k$ 次迭代的 Hessian 矩阵 $\nabla^2f(x^{(k)})$ 表示为 $H_k$.</p>
<h3 id="收敛性"><a href="#收敛性" class="headerlink" title="收敛性"></a>收敛性</h3><p>下面给出牛顿法的收敛性证明:</p>
<blockquote>
<p>定理1：若初始点 $x^{(1)}$在极值点 $x^*$ 附近，并且 $x^*$ 附近的 Hessian 矩阵正定，并且 Hessian 矩阵满足 Lipschitz 连续条件，即 $|\nabla ^2f(x)-\nabla^2 f(y)|_2\le L|x-y|_2 $，则牛顿法满足二阶收敛.</p>
</blockquote>
<p>证明如下：</p>
<p>由于 $x^{(k+1)}=x^{(k)}-H_k\nabla f(x^{(k)})$:</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}\left\|x_{k+1}-x^{*}\right\|_2 &=\left\|x_{k}-H_{k}^{-1} \nabla f(x^{(k)})-x^{*}\right\|_2 \\ &=\left\|H_{k}^{-1}\right\| _2*\left\|H_{k}\left(x_{k}-x^{*}\right)-\nabla f(x^{(k)})\right\|_2 \\ &=\left\|H_{k}^{-1}\right\| _2*\left\|H_{k}\left(x_{k}-x^{*}\right)-\nabla f(x^{(k)})+\nabla f\left(x^{*}\right)\right\| _2\end{aligned}\tag{2}</script></blockquote>
<p>又因为 $\nabla f(x^{(k)})-g\left(x^{*}\right)=\int_{0}^{1}\left(x^{(k)}-x^{*}\right) \nabla^2f\left(x^{*}+t\left(x^{(k)}-x^{*}\right)\right) d t$.</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}\left\|x^{(k+1)}-x^{*}\right\|_2 & \leq \left\|H_{k}^{-1}\right\| _2*\left\|H_{k}\left(x^{(k)}-x^{*}\right)-g_{k}+g\left(x^{*}\right)\right\|_2 \\ &=\left\|H_{k}^{-1}\right\| _2*\left\|H_{k}\left(x^{(k)}-x^{*}\right)-\int_{0}^{1}\left(x^{(k)}-x^{*}\right) \nabla^2 f\left(x^{*}+t\left(x^{(k)}-x^{*}\right)\right) d t\right\|_2 \\ &=\left\|H_{k}^{-1}\right\| _2*\left\|\int_{0}^{1}\left[H_{k}\left(x^{(k)}-x^{*}\right)-\left(x^{(k)}-x^{*}\right) \nabla^2 f\left(x^{*}+t\left(x^{(k)}-x^{*}\right)\right)\right] d t\right\|_2 \\ &=\left\|H_{k}^{-1}\right\| _2*\left\|x^{(k)}-x^{*}\right\|_2 *\left\|\int_{0}^{1} H_{k}-\nabla^2 f\left(x^{*}+t\left(x^{(k)}-x^{*}\right)\right) d t\right\|_2 \end{aligned}\tag{3}</script></blockquote>
<p>利用 Lipschitz 连续性，得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\left\|x^{(k+1)}-x^{*}\right\|_2 & \leq \left\|H_{k}^{-1}\right\| _2* L\left\|x^{(k)}-x^{*}\right\|_2^{2} \int_{0}^{1}(1-t) d t \\
&=\frac{1}{2} \left\|H_{k}^{-1}\right\| _2* L\left\|x^{(k)}-x^{*}\right\|^{2}_2
\end{aligned}\tag{4}</script></blockquote>
<p>显然，当 $k\to\infty$ 时， $H_k\to H^*$, 于是 $|H_k^{-1}|_2\le2|H^*|_2$. 最终：</p>
<blockquote>
<script type="math/tex; mode=display">
\left\|x^{(k+1)}-x^{*}\right\|_2\le L\|H^*\|_2*\left\|x^{(k)}-x^{*}\right\|^{2}_2\tag{5}</script></blockquote>
<p>证毕。</p>
<p>显然从上面证明可知，牛顿法不是整体收敛的，当初始点远离最优解时，牛顿方向不一定是下降方向，因此作为牛顿法的改进，引入阻尼牛顿法。</p>
<h2 id="阻尼牛顿法"><a href="#阻尼牛顿法" class="headerlink" title="阻尼牛顿法"></a>阻尼牛顿法</h2><p>相较于纯牛顿法，阻尼牛顿法在牛顿迭代过程中增加了迭代步长这样一个参数，而不是统一的取 $1$.</p>
<p>下面先给出牛顿减量的定义：</p>
<blockquote>
<p>$\lambda(x):=(\nabla f(x)^T\nabla^2f(x)^{-1}\nabla f(x))^{1/2}$</p>
</blockquote>
<p>令 $\hat{f}(y)=f(x)+\nabla f(x)^T(y-x)+\frac12(y-x)^T\nabla^2f(x)(y-x)$ 为 $f$ 在 $x$ 附近的二阶泰勒展开。则 $f(x)-\inf_y\hat f(y)=\frac12 \lambda(x)^2$.</p>
<p>因此牛顿减量是基于 $f$ 在 $x$ 处的二阶近似对 $f(x)-f(x^*)$ 做出的估计值，可以用来设计停止准则。</p>
<p>阻尼牛顿法的算法步骤：</p>
<blockquote>
<p>(1). 选取初始点 $x^{(1)}$。</p>
<p>(2). 在第 $k$ 步迭代时，计算 $\nabla^2f(x^{(k)})$ 和 $\nabla f(x^{(k)})$ .</p>
<p>(3). 确定迭代步长 $t_k$.</p>
<p>(4). 置 $x^{(k+1)}=x^{(k)}-t_k(\nabla^2f(x^{(k)}))^{-1}\nabla f(x^{(k)})$.</p>
<p>(5). 直到满足停止准则： $\lambda(x^{(k)})^2/2\le\epsilon$, 其中 $\epsilon$ 是预先给定的阈值。</p>
</blockquote>
<p>下面采用回溯直线算法确定步长 $t_k$ . 并给出在强凸条件下收敛性证明：</p>
<h3 id="收敛性-1"><a href="#收敛性-1" class="headerlink" title="收敛性"></a>收敛性</h3><blockquote>
<p>定理2：假设 $f$ 满足以下条件：</p>
<p>​    a.  $f$ 的梯度满足 Lipschitz 连续条件，即$|\nabla f(x)-\nabla f(y)|_2\le M|x-y|_2 $.</p>
<p>​    b. $f$ 是强凸的，即 $\nabla^2f(x)\ge mI$ 。</p>
<p>​    c. Hessian 矩阵满足 Lipschitz 连续条件，即 $|\nabla ^2f(x)-\nabla^2 f(y)|_2\le L|x-y|_2 $.</p>
<p>当满足以上条件时采用回溯直线的阻尼牛顿法将满足以下的收敛边界：</p>
<script type="math/tex; mode=display">
f\left(x^{(k)}\right)-f^{*} \leq\left\{\begin{array}{ll}
\left(f\left(x^{(0)}\right)-f^{*}\right)-\gamma k & \text { if } k \leq k_{0} \\
\frac{2 m^{3}}{L^{2}}\left(\frac{1}{2}\right)^{2^{k-k_{0}+1}} & \text { if } k>k_{0}
\end{array}\right.\tag{6}</script><p>其中 $\gamma=\alpha\beta\eta^2 m/M$, $\eta=\min\{1,3(1-2\alpha)\}m^2/L,$ $k_0$ 是直到 $|\nabla f(x^{(k_0+1)})|_2\le\eta$ 的迭代数。并且当 $k&gt;k_0$ 时，迭代步长 $t=1$.</p>
</blockquote>
<p>证明如下：</p>
<p>先证明第一阶段，假定 $|\nabla f(x)|_2\ge\eta$ ，由于条件 a ,可以得出 $\nabla^2f(x)\le MI$, 因此：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
&f\left(x-t (\nabla^2f(x))^{-1}\nabla f(x)\right)\\
 \leqslant& f(x)+t \nabla f(x)^{T} (\nabla^2f(x))^{-1}\nabla f(x)+\frac{M\left\|(\nabla^2f(x))^{-1}\nabla f(x)\right\|_{2}^{2}}{2} t^{2} \\
\leqslant& f(x)-t \lambda(x)^{2}+\frac{M}{2 m} t^{2} \lambda(x)^{2}
\end{aligned}\tag{7}</script></blockquote>
<p>由于步长$\hat{t}=m/M$ 满足：</p>
<blockquote>
<script type="math/tex; mode=display">
f\left(x-\hat t (\nabla^2f(x))^{-1}\nabla f(x)\right) \le f(x)-\frac {m}{2M}\lambda (x)^2\le f(x)-\alpha\hat t\lambda(x)^2\tag{8}</script></blockquote>
<p>符合回溯直线搜索的退出条件，所以回溯直线搜索确定的步长满足 $t\ge \beta m/M$</p>
<p>则目标函数的减少量为：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
f\left(x- t (\nabla^2f(x))^{-1}\nabla f(x)\right) -f(x)&\le-\alpha t \lambda(x)^2\\
&\le-\alpha\beta\frac mM\lambda(x)^2\\
&\le-\alpha\beta\frac{m}{M^2}\|\nabla f(x)\|_2^2\\
&\le-\alpha\beta\eta^2\frac m{M^2}\\
&=-\gamma
\end{aligned}\tag{9}</script></blockquote>
<p>因此 $f\left(x^{(k)}\right)-f^{*} \leq\left(f\left(x^{(0)}\right)-f^{*}\right)-\gamma k\quad\text { if } k \leq k_{0} $. 得证</p>
<p>下面证第二阶段，假定 $|\nabla f(x)|_2&lt;\eta$. 首先说明，若 $\eta\le3(1-2\alpha)m^2/L$，回溯直线搜索的步长就是单位步长。</p>
<p>由 Hessian 矩阵的 Lipschitz 条件，对于 $t\ge 0$, 我们有 $|\nabla^2f(x-t (\nabla^2f(x))^{-1}\nabla f(x))-\nabla^2 f(x)|_2\le tL|(\nabla^2f(x))^{-1}\nabla f(x)|_2$.</p>
<p>若令 $\hat{f}(t)=f(x-t (\nabla^2f(x))^{-1}\nabla f(x))$, 则有：</p>
<blockquote>
<script type="math/tex; mode=display">
\|\hat{f}''(t)-\hat{f}''(0)\|_2\le tL\|t (\nabla^2f(x))^{-1}\nabla f(x)\|_2^3\tag{10}</script></blockquote>
<p>由于 $\hat{f}’’(0)=\lambda(x)^2$：</p>
<blockquote>
<script type="math/tex; mode=display">
\hat f''(t)\le\hat f''(0)+tL\|t (\nabla^2f(x))^{-1}\nabla f(x)\|_2^3\le\lambda(x)^2+t\frac{L}{m^{3/2}}\lambda(x)^3\tag{11}</script></blockquote>
<p>由于 $\hat f’’(0)=-\lambda(x)^2$, 对 (11) 积分可得：</p>
<blockquote>
<script type="math/tex; mode=display">
\hat{f}'(t)\le\hat f'(0)+t\lambda(x)^2+t^2\frac{L}{2m^{3/2}}\lambda(x)^3=-\lambda(x)^2+t\lambda(x)^2+t^2\frac L{2m^{3/2}}\lambda(x)^3\tag{12}</script></blockquote>
<p>再次积分得：</p>
<blockquote>
<script type="math/tex; mode=display">
\hat f(t)\le \hat f(0)-t\lambda(x)^2+t^2\frac 12\lambda(x)^2+t^3\frac{L}{6m^{3/2}}\lambda(x)^3\tag{13}</script></blockquote>
<p>在 (13) 中取 $t=1$ 得：</p>
<blockquote>
<script type="math/tex; mode=display">
f(x-(\nabla^2f(x))^{-1}\nabla f(x))\le f(x)-\frac 12 \lambda(x)^2+\frac L{6m^{3/2}}\lambda(x)^3\tag{14}</script></blockquote>
<p>由于 $|\nabla f(x)|_2\le \eta\le 3(1-2\alpha)m^2/L$, 由强凸性可知：</p>
<blockquote>
<script type="math/tex; mode=display">
\lambda(x)\le3(1-2\alpha)m^{3/2}/L\tag{15}</script></blockquote>
<p>则由 (14)(15) 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
f(x-(\nabla^2f(x))^{-1}\nabla f(x))&\le f(x)- \lambda(x)^2\left(\frac 12-\frac {L\lambda(x)}{6m^{3/2}}\right)\\
&\le f(x)-\alpha\lambda(x)^2\\
&=f(x)+\alpha\nabla f(x)^T(\nabla^2f(x))^{-1}\nabla f(x)
\end{aligned}\tag{16}</script></blockquote>
<p>该式表明 $t=1$ 满足回溯直线搜索得退出条件。</p>
<p>现在分析其收敛速度：令 $d(x)=(\nabla^2f(x))^{-1}\nabla f(x)$ , 应用 Hessian 矩阵得 Lipschitz 条件，我们有：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\left\|\nabla f\left(x-d(x)\right)\right\|_{2} &=\left\|\nabla f\left(x+d(x)\right)-\nabla f(x)-\nabla^{2} f(x) d(x)\right\|_{2} \\
&=\left\|\int_{0}^{1}\left(\nabla^{2} f\left(x+td(x)\right)-\nabla^{2} f(x)\right)d(x) d t\right\|_{2} \\
& \leqslant \frac{L}{2}\left\|d(x)\right\|_{2}^{2} \\
&=\frac{L}{2}\left\|\nabla^{2} f(x)^{-1} \nabla f(x)\right\|_{2}^{2} \\
& \leqslant \frac{L}{2 m^{2}}\|\nabla f(x)\|_{2}^{2}
\end{aligned}\tag{17}</script></blockquote>
<p>因此 $f\left(x^{(k)}\right)-f^{*} \leq\frac{2 m^{3}}{L^{2}}\left(\frac{1}{2}\right)^{2^{k-k_{0}+1}} \quad\text { if } k&gt;k_{0}$. 得证</p>
<p>因此由上述分析，拟牛顿法若要满足 $f\left(x^{(k)}\right)-f^{*}\le\epsilon$, 最多需要 $\frac{f\left(x^{(0)}\right)-f^{\star}}{\gamma}+\log \log \left(\epsilon_{0} / \epsilon\right)$ 次迭代，其中 $\epsilon_0=2m^3/L$。</p>
<p>上面通过增加学习步长，使得牛顿法得到了全局收敛性，但是相较于梯度下降法，牛顿法仍有它的缺点：</p>
<p>(1). 牛顿法在每次迭代需要用到 $O(n^2)$ 得内存来存储 Hessian 矩阵，而梯度下降法只用 $O(n)$。</p>
<p>(2). 牛顿法要求函数必须是强凸得，即 Hessian 矩阵得逆存在，而梯度下降法不需要。</p>
<p>(3). 牛顿法需要 $O(n^3)$ 来计算 Hessian 矩阵得逆。这在 $n$ 较大时，计算量是相当大得。</p>
<p>其中第 (2)(3) 条缺点，我们可以通过对 Hessian 矩阵近似来克服它。</p>
<h2 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h2><p>假设我们用矩阵 $B_k$ 来替代在 $x^{(k)}$ 处的 Hessian 矩阵，则迭代表达式为：</p>
<blockquote>
<script type="math/tex; mode=display">
x^{(k
+1)}=x^{(k)}-\alpha_kB_k^{-1}\nabla f(x^{(k)})\tag{18}</script></blockquote>
<p>由于是用 $B_k$ 来近似 $\nabla ^2f(x^{(k)})$. 因此怎样的 $B_k$ 才算合理的非常重要，因此给出下面定理</p>
<blockquote>
<p>定理3：假设 $f$ 是二次连续可微的，考虑迭代关系式为 (18) ，假设 $\alpha_k=1$, 若 $\{x^{(k)}\}$ 收敛到 $x^*$ 使得 $\nabla f(x^*)=0$, 且 $\nabla^2 f(x^*) $ 是正定的，那么 $\{x^{(k)}\}$ 将超过线性收敛的速度收敛，当且仅当 $\lim _{k \rightarrow \infty} \frac{\left|\left(B_{k}-\nabla^{2} f\left(x^{*}\right)\right) B_k^{-1}\nabla f(x^{(k)})\right|_2}{\left|B_k^{-1}\nabla f(x^{(k)})\right|_2}=0$</p>
</blockquote>
<p> 证明如下：令 $p_k=B_k^{-1}\nabla f(x^{(k)})$, $p_k^N=-(\nabla^2f(x^{(k)}))^{-1}\nabla f(x^{(k)})$. 则可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
p_k-p_k^N&=(\nabla^2f(x^{(k)}))^{-1}(\nabla^2f(x^{(k)})p_k+\nabla f(x^{(k)}))\\
&=(\nabla^2f(x^{(k)}))^{-1}(\nabla^2 f(x^{(k)})-B_k)p_k\\
&=O(\|(\nabla^2 f(x^{(k)})-B_k)p_k\|_2)\\
&=o(\|p_k\|_2)
\end{aligned}\tag{19}</script></blockquote>
<p>其中导数第二个等式是由于当 $x^{(k)}$ 充分靠近 $x^*$ 时，$|\nabla^2f(x^{(k)})|_2$ 有界。因此：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\left\|x^{(k+1)}-x^{*}\right\|_2 &=\left\|x^{(k)}+p_{k}-x^{*}\right\|_2 \\
&=\left\|x^{(k)}+p_{k}^{N}-x^{*}+p_{k}-p_{k}^{N}\right\| \\
& \leq\left\|x^{(k)}+p_{k}^{N}-x^{*}\right\|+\left\|p_{k}-p_{k}^{N}\right\| \\
&=O\left(\left\|x^{(k)}-x^{*}\right\|^{2}\right)+o\left(\left\|p_{k}\right\|\right) \\
&=O\left(\left\|x^{(k)}-x^{*}\right\|^{2}\right)+o\left(\left\|x_{k}-x^{*}\right\|\right) \\
&=o\left(\left\|x^{(k)}-x^{*}\right\|\right)
\end{aligned}\tag{20}</script></blockquote>
<p>证毕。</p>
<p>下面介绍如何确定 $B_k$ 。</p>
<p>令 $g_{k}=\nabla f(x^{(k)})$, $p_k=x^{(k+1)}-x^{(k)}$, $F_k=\nabla ^2f(x^{(k)})$ , $q_k=g_{k+1}-g_k$. 则由泰勒展开有：</p>
<blockquote>
<script type="math/tex; mode=display">
q_k=g_{k+1}-g_k\approx F_kp_k\tag{21}</script></blockquote>
<p>显然当优化函数为二次函数时，上式取等号。若希望用 $H_k$ 来近似 $F_k$，则它满足：$H_{k}q_k\approx p_k$. 因此我们可以取 $H_{k+1}$ 满足：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}q_k=p_k\tag{22}</script></blockquote>
<p>因此 $H_{k+1}$ 的不同选取方法，对应了不同的拟牛顿法。</p>
<h3 id="SR1方法"><a href="#SR1方法" class="headerlink" title="SR1方法"></a>SR1方法</h3><p>最简单的一种迭代方法是，考虑：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=H_k+E_k=H_k+a_kz_kz_k^T\tag{23}</script></blockquote>
<p>其中 $a_k$ 为常数，$z_k$ 为向量。并且只要初始选择的 $H_0$ 为对称矩阵，则 $H_{k+1}$ 为对称的。</p>
<p>将 (23) 代入 (22) 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
p_k=H_{k+1}q_k=H_kq_k+a_kz_kz_k^Tq_k\tag{24}</script></blockquote>
<p>与 $q_k$ 取内积得：</p>
<blockquote>
<script type="math/tex; mode=display">
q_k^Tp_k-q_k^TH_kq_k=a_k(z_k^Tq_k)^2\tag{25}</script></blockquote>
<p>另一方面，将 (24) 代入 (23) 得：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=H_k+\frac{(p_k-H_kq_k)(p_k-H_kq_k)^T}{a_k(z_k^Tq_k)^2}</script></blockquote>
<p>将 (25) 代入得：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=H_k+\frac{(p_k-H_kq_k)(p_k-H_kq_k)^T}{q_k^T(p_k-H_kq_k)}\tag{26}</script></blockquote>
<p>上面得到的 $H_{k+1}$ 不一定正定。且分母可能为很小，这将导致数值上的困难。</p>
<h3 id="DFP方法"><a href="#DFP方法" class="headerlink" title="DFP方法"></a>DFP方法</h3><p>上述的SR1方法的更新量为秩为 $1$ 的矩阵，现在考虑更新量为秩为2矩阵：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=H_k+a_ku_ku_k^T+b_kv_kv_k^T\tag{27}</script></blockquote>
<p>因此，将 (27) 代入 (22) 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
p_k=H_{k+1}q_k=H_kq_k+a_ku_ku_k^Tq_k+b_kv_kv_k^Tq_k\tag{28}</script></blockquote>
<p>显然，这个时候，$u_k,v_k$ 不能够被唯一的确定。一个显然的选择是 $u_k=p_k,v_k=H_kq_k$. 取 使 $au_k^Tq_k=1,bv_k^Tq_k=-1$ 成立的 $a,b$.</p>
<p>因此：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=H_k+\frac{p_kp_k^T}{p_k^Tq_k}-\frac{H_kq_kq_k^TH_k}{q_k^TH_kq_k}\tag{29}</script></blockquote>
<p>使用上述对 $H_k$ 估计的方法称为DFP方法：</p>
<blockquote>
<p>(1). 选定初始的对称正定矩阵 $H_0$, 和初始迭代点 $x^{(0)}$.</p>
<p>(2). 令 $d_k=-H_kg_k$. 选取步长 $\alpha_k$ . 则可以得到 $x^{(k+1)}=x^{(k)}+\alpha_kd_k$ .</p>
<p>(3). 按照 (29) 迭代得到 $H_{k+1}$.</p>
<p>(4). 重复上述步骤直到满足停止条件。</p>
</blockquote>
<p>给出下列定理：</p>
<blockquote>
<p>定理4：若 $p_k^Tq_k&gt;0$, 对任意的 $k$ 成立，则 DFP 方法得到的 $H_{k}$ 矩阵为正定矩阵。特别的，若步长 $\alpha_k$ 是通过精确一维线性搜索得到的，即 $\alpha_k=\arg\min_\alpha f(x^{(k)}+\alpha d_k)$. 则 DFP 方法得到的 $H_k$ 为正定矩阵。</p>
</blockquote>
<p>证明如下：</p>
<p>由 (29) 式，对任意 $x\in R^n$, 我们有：</p>
<blockquote>
<script type="math/tex; mode=display">
x^TH_{k+1}x=x^TH_kx+\frac{(x^Tp_k)^2}{p_k^Tq_k}-\frac{(x^TH_kq_k)^2}{q_k^TH_kq_k}\tag{30}</script></blockquote>
<p>定义 $a=H_k^{1/2}x,b=H_k^{1/2}q_k$, 可以将 (30) 重写为：</p>
<blockquote>
<script type="math/tex; mode=display">
x^TH_{k+1}x=\frac{(a^Ta)(b^Tb)-(a^Tb)^2}{(b^Tb)}+\frac{(x^Tp_k)^2}{p_k^Tq_k}\tag{31}</script></blockquote>
<p>由于 $p_k^Tq_k&gt;0$， 则右端的第二个式子大于 $0$, 又因为柯西不等式，则右端的第一个式子大于 $0$. </p>
<p>因此 $H_{k+1}$ 正定。</p>
<p>若 $\alpha_k$ 是通过精确一维线搜索得到的，则 $p_k^Tg_{k+1}=0$ , 因此 $p_k^Tq_k=p^T_kg_{k+1}-p^T_kg_k=-p_k^Tg_k=\alpha_kg_k^TH_kg_k$.</p>
<p>由于 $H_k$ 正定，所以 $p^T_kq_k&gt;0$ .</p>
<p>证毕。</p>
<h3 id="BFGS方法"><a href="#BFGS方法" class="headerlink" title="BFGS方法"></a>BFGS方法</h3><p>类似于 DFP 方法，只是这里不再对 Hessian 矩阵的逆直接进行估计，而是对 Hessian 矩阵进行估计，即 </p>
<blockquote>
<script type="math/tex; mode=display">
q_k=B_{k+1}p_k\tag{32}</script></blockquote>
<p>运用和 DFP 方法类似的分析，可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
B_{k+1}=B_k+\frac{q_kq_k^T}{q_k^Tp_k}-\frac{B_kp_kp_k^TB_k}{p_k^TB_kp_k}\tag{33}</script></blockquote>
<p>对 $B_{k+1}$ 两次运用 Sherman-Morrison 公式，$\left[A+ab^{T}\right]^{-1}=A^{-1}-\frac{A^{-1}ab^{T} A^{-1}}{1+b^{T} A^{-1} a}$ ，可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}^{\mathrm{BFGS}}=H_{k}+\left(1+\frac{q_{k}^{T} H_{k} q_{k}}{q_{k}^{T} q_{k}}\right) \frac{p_{k} p_{k}^{T}}{p_{k}^{T} q_{k}}-\frac{p_{k}q_{k}^{T}H_{k}+H_{k}q_{k} q_{k}^{T}}{q_{k}^{T} p_{k}}\tag{34}</script></blockquote>
<h3 id="Boryden-类算法"><a href="#Boryden-类算法" class="headerlink" title="Boryden 类算法"></a>Boryden 类算法</h3><p>我们记 $H_{k+1}^{\text{DFP}}$ 为 DFP 算法的迭代公式得到的，记 $H_{k+1}^{\text{BFGS}}$ 为 DFP 算法的迭代公式得到的。由于它们都满足 (22) 式。则它们的线性组合满足，我们将</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}^{\alpha}=(1-\alpha) H^{\text{DFP}}_{k+1}+\alpha H_{k+1}^{\text{BFGS}}\tag{35}</script></blockquote>
<p>称为 Boryden 类算法。</p>
<p>将 (35) 展开可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}^{\alpha}=H_{k}+\frac{p_{k} p_{k}^{T}}{p_{k}^{T} q_{k}}-\frac{H_{k} q_{k} q_{k}^{T} H_{k}}{q_{k}^{T} H_{k} q_{k}}+\alpha v_{k} v_{k}^{T}=H_{k+1}^{\mathrm{DFP}}+\alpha v_{k}v_{k}^{T}\tag{36}</script></blockquote>
<p>其中</p>
<blockquote>
<script type="math/tex; mode=display">
v_{k}=\left(q_{k}^{T} H_{k} q_{k}\right)^{1 / 2}\left(\frac{p_{k}}{p_{k}^{T} q_{k}}-\frac{H_{k} q_{k}}{q_{k}^{T} H_{k} q_{k}}\right)</script></blockquote>
<p>当 $\alpha=1$ 时，就是 BFGS 算法，当 $\alpha=0$ 时，就是 DFP 算法。当 $\alpha=\frac{q_k^Tp_k}{q_k^Tp_k-q_k^TH_kq_k}$ 时，就是 SR1 算法。</p>
<h3 id="DFP-算法和BFGS-算法的另一种推导"><a href="#DFP-算法和BFGS-算法的另一种推导" class="headerlink" title="DFP 算法和BFGS 算法的另一种推导"></a>DFP 算法和BFGS 算法的另一种推导</h3><p>再来看我们的目标，我们希望通过 (22) 来计算 $H_k$ , 但是由于 (22) 式只有 $n$ 个方程，而对称矩阵 $H_{k+1}$ 有 $n(n+1)/2$ 个自由度，因此会有无数个矩阵满足 (22) , 即便我们要求 $H_{k+1}$ 正定，也只能额外提供 $n$ 个自由度，仍远远不够，因此我们希望再选择 $H_{k+1}$ 时，与上次迭代的 $H_k$ 尽量接近。即在某种范数的意义下：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=\min_H\|H-H_k\|\tag{37}</script></blockquote>
<p>这里的范数选取很关键，不同的范数可以诱导出不同的拟牛顿法。</p>
<p>当我们把范数选取为加权欧式范数时，就可以得到 BFGS 算法。</p>
<p>其中加权欧式范数指的是 $|A|_W^2:=|W^{1/2}AW^{1/2}|_E^2=tr(WA^TWA)$.</p>
<blockquote>
<p>定理5：把 BFGS 算法的 (34) 写为 $H_{k+1}=H_k+E$, 其中 $H_k$ 对称，则 $E$ 等价于下列等式约束问题：</p>
<p>$\min_E|E|_W$</p>
<p>使得 $E^T=E,Eq_k=\gamma_k$</p>
<p>其中 $\gamma_k=p_k-H_kq_k$, $W$ 为满足 $Wp_k=q_k$ 的任意矩阵。</p>
</blockquote>
<p>证明如下：</p>
<p>该问题是一个凸优化问题，因此我们仅需找 $E$ 的一阶条件即可。</p>
<p>设 Lagrangian 函数为：</p>
<blockquote>
<script type="math/tex; mode=display">
\mathcal L=\frac 14 tr(WE^TWE)+tr(\Lambda^T(E^T-E))-\lambda^TW(Eq_k-\gamma_k)\tag{38}</script></blockquote>
<p>其中 $\Lambda$ 是 $E^T=E$ 的拉格朗日乘子， $\lambda^TW$ 是 $Eq_k=\gamma_k$ 的拉格朗日乘子。</p>
<p>由于 $\partial E/\partial E_{ij} = e_ie_j^T$ , 则：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}\partial\mathcal L/\partial E_{ij}=&\frac 14 (tr(We_ie_j^TWE)+tr(WE^TWe_ie_j^T))\\
\quad&+tr(\Lambda(e_je_i^T-e_ie_j^T))-\lambda^TWe_ie_j^Tq_k\\
=&0
\end{aligned}\tag{39}</script></blockquote>
<p>运用对称性和秩的交换性：</p>
<blockquote>
<script type="math/tex; mode=display">
\frac12[WEW]_{ij}+\Lambda_{ij}-\Lambda_{ji}=[W\lambda q_k^T]\tag{40}</script></blockquote>
<p>因此：</p>
<blockquote>
<script type="math/tex; mode=display">
WEW=W\lambda q_k^T+q_k\lambda^TW\tag{41}</script></blockquote>
<p>由于 $Wp_k=q_k$，并且 $W$ 非奇异：</p>
<blockquote>
<script type="math/tex; mode=display">
E=q_kp_k^T+p_k\lambda^T\tag{42}</script></blockquote>
<p>由于 $E^T=E,Eq_k=\gamma_k$ , 代入 (42) 得：</p>
<blockquote>
<script type="math/tex; mode=display">
\lambda=\frac{\gamma_k-p_k\lambda^Tq_k}{p_k^Tq_k}\tag{43}</script></blockquote>
<p>又因为 $\lambda^Tq_k=\frac{1/2q_k^T\gamma_k}{p_kq_k}$ ，代入 (43) , 并结合 $\gamma_k=p_k-H_kq_k$ 得：</p>
<blockquote>
<script type="math/tex; mode=display">
\lambda=\frac{(H_kq_k-\frac12p_k(q_k^TH_kq_k/p_k^Tq_k))}{p_k^Tq_k}\tag{44}</script></blockquote>
<p>将 (44) 代入 (42) 就得到 BFGS 得表达式。证毕。</p>
<p>同样的，若将 $H_k$ 用 $B_k$ 替代，则将得到 DFP 方法。</p>
<h3 id="L-BFGS方法"><a href="#L-BFGS方法" class="headerlink" title="L-BFGS方法"></a>L-BFGS方法</h3><p>我们再来看 BFGS 算法，$H$ 的迭代关系为：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}^{\mathrm{BFGS}}=H_{k}+\left(1+\frac{q_{k}^{T} H_{k} q_{k}}{q_{k}^{T} q_{k}}\right) \frac{p_{k} p_{k}^{T}}{p_{k}^{T} q_{k}}-\frac{p_{k}q_{k}^{T}H_{k}+H_{k}q_{k} q_{k}^{T}}{q_{k}^{T} p_{k}}\tag{45}</script></blockquote>
<p>显然，在每次迭代需要存储 $H_k$ 用于下一次迭代，即需要存储 $O(n^2)$ 的数据，并且从 (45) 可以看出，每次迭代需要 $O(n^2)$ 的运算量。这在 $n$ 较大时仍是难以接受的。因此在大规模的问题中，还需进行优化。</p>
<p>可以将 (45) 改写为：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}^{\mathrm{BFGS}}=\left(I-\frac{p_kq_k^T}{p_k^Tq_k}\right)H_{k}\left(I-\frac{p_kq_k^T}{p_k^Tq_k}\right)^T+ \frac{p_{k} p_{k}^{T}}{p_{k}^{T} q_{k}}  \tag{46}</script></blockquote>
<p>令 $\rho_k = \frac{1}{p_k^Tq_k}$ , $V_k=I-\rho_kp_kq_k^T$, 则：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=V_k^TH_kV_k+\rho_kp_kp_k^T</script></blockquote>
<p>依次往前递推可以得到由 $H_0$ 和 $p_k,q_k$ 所构成的表达式，这样运行下去的计算量和内存占用量也是相当的高。L-BFGS算法考虑的是进行截断，即我们不从 $H_0$ 开始迭代计算 $H_{k}$ 而是从第 $k-m$ 步开始计算，其中 $m$ 远小于 $n$。</p>
<p>则我们可以将 $H_k$ 写为：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}H_{k}=&\left(V_{k-1}^{T} \cdots V_{k-m}^{T}\right) H_{k-m}\left(V_{k-m} \cdots V_{k-1}\right) \\&+\rho_{k-m}\left(V_{k-1}^{T} \cdots V_{k-m+1}^{T}\right) p_{k-m} p_{k-m}^{T}\left(V_{k-m+1} \cdots V_{k-1}\right) \\&+\rho_{k-m+1}\left(V_{k-1}^{T} \cdots V_{k-m+2}^{T}\right) p_{k-m+1} p_{k-m+1}^{T}\left(V_{k-m+2} \cdots V_{k-1}\right) \\&+\cdots \\&+\rho_{k-1} p_{k-1} p_{k-1}^{T}\end{aligned}\tag{47}</script></blockquote>
<p>其中，我们需要近似 $H_{k-m}$，通常我们可以选择：$H_{k-m}=\gamma I,$ 其中 $\gamma=\frac{q_k^Tp_k}{q_k^Tq_k}$。即 $\gamma$ 可以表示为近似特征值。</p>
<blockquote>
<p>$s=\nabla f_k;$</p>
<p><strong>for</strong> $i=k-1,k-2,\cdots,k-m$</p>
<p>$\quad \alpha_i=\rho_ip_i^Ts$; </p>
<p>$\quad s=s-\alpha_iq_i$;</p>
<p><strong>end for</strong></p>
<p>$ r=H_{k-m}s$;</p>
<p><strong>for</strong> $i=k-m,k-m+1,\cdots,k-1$</p>
<p>$\quad\beta = \rho_iq_i^Tr;$</p>
<p>$\quad r=r+p_i(\alpha_i-\beta);$</p>
<p><strong>end for</strong></p>
<p><strong>stop</strong> with $H_k\nabla f_k=r$</p>
</blockquote>
<p>在该算法中，总共进行了 $4mn$ 次乘法 ，内存方面只需要保存最近 $m$ 次迭代的 $p_k,q_k$，共 $2mn$ 个数，因此大大节约内存和运算量。</p>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Nocedal, Jorge, and Stephen Wright. <em>Numerical optimization</em>. Springer Science &amp; Business Media, 2006.</p>
<p>[2] Boyd, Stephen, Stephen P. Boyd, and Lieven Vandenberghe. <em>Convex optimization</em>. Cambridge university press, 2004.</p>
<p>[3] Luenberger, David G., and Yinyu Ye. <em>Linear and nonlinear programming</em>. Vol. 2. Reading, MA: Addison-wesley, 1984.</p>
<p>[4] Fletcher, Roger. <em>Practical methods of optimization</em>. John Wiley &amp; Sons, 2013.</p>
<p>[5] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. <em>Deep learning</em>. MIT press, 2016.</p>
<p>[6] 李航. 统计学习方法. 清华大学出版社，2019</p>

        </div>
        
            <div style="text-align:center;color: rgb(125,125,125);font-size:14px;">
               -------------本文结束 感谢您的阅读-------------
            </div>
        
        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>文章作者:</span>
                        <span>会飞的猪</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>文章链接:</span>
                        <span><a href="http://shellyandliu.github.io/2020/05/10/%E7%89%9B%E9%A1%BF%E6%B3%95/">http://shellyandliu.github.io/2020/05/10/%E7%89%9B%E9%A1%BF%E6%B3%95/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>版权声明:</span>
                        <span>本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC-BY-NC-4.0</a> ， 转载请注明来自 会飞的猪！</span>
                    </p>
                
            </section>
        
        <section class="post-tags">
            <div>
                <span>标签:</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/"> <i class="iconfont icon-tags"></i> 优化方法</a>
                    
                        <a href="/tags/%E7%89%9B%E9%A1%BF%E6%B3%95/"> <i class="iconfont icon-tags"></i> 牛顿法</a>
                    
                        <a href="/tags/%E9%98%BB%E5%B0%BC%E7%89%9B%E9%A1%BF%E6%B3%95/"> <i class="iconfont icon-tags"></i> 阻尼牛顿法</a>
                    
                        <a href="/tags/%E6%8B%9F%E7%89%9B%E9%A1%BF/"> <i class="iconfont icon-tags"></i> 拟牛顿</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">返回</a>
                <span> /  </span>
                <a href="/">首页</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2020/05/13/Logistic%E5%9B%9E%E5%BD%92/">Logistic回归</a>
            
            
            <a class="next" rel="next" href="/2020/05/09/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/">随机梯度下降法</a>
            
        </section>
        <div class="post-content">
            <section style="font-size: 20px;font-weight: 700;margin-bottom: 10px;margin-top: 20px;">
                <i class="iconfont icon-comments"></i>
                <span>
                    评论
                </span>
            </section>
        </div>
        
            <section id="comments" class="comments">
              <style>
                .comments{margin:0px;padding:10px;background:#fff}
                @media screen and (max-width:800px){.comments{margin:auto;padding:10px;background:#fff}}
              </style>
              <div class="valine_comment"></div>
<!--����js����</body>֮ǰ���뼴��-->
<!--Leancloud ������:-->
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<!--Valine �ĺ��Ĵ����-->
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
  new Valine({
      el: '.valine_comment',
      app_id: 'bCvyxhtL2V59mmqAp81lAGob-gzGzoHsz#Leancloud应用的appId',
      app_key: 'J4zVKQfUjhToGJwBfLcoOHOv',
      placeholder: '欢迎大家评论交流~',
      notify: 'true',
      verify: 'true',
    });
</script>
            </section>
        
        

    </article>
</div>

        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© 会飞的猪 | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>

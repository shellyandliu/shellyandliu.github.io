<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>牛顿法 | 会飞的猪</title><meta name="description" content="牛顿法是一种具有局部二阶收敛率的方法，在牛顿法的基础上增加一个学习步长的方法称为阻尼牛顿法，阻尼牛顿法具有全局收敛性。而牛顿法由于需要求 Hessian 矩阵的逆，计算量巨大，通过对 Hessian 矩阵估计的方法称为拟牛顿法，著名的拟牛顿法有DFP方法和BFGS方法。"><meta name="keywords" content="机器学习,统计,优化方法,二阶收敛,牛顿法,阻尼牛顿法,拟牛顿"><meta name="author" content="会飞的猪"><meta name="copyright" content="会飞的猪"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="twitter:card" content="summary"><meta name="twitter:title" content="牛顿法"><meta name="twitter:description" content="牛顿法是一种具有局部二阶收敛率的方法，在牛顿法的基础上增加一个学习步长的方法称为阻尼牛顿法，阻尼牛顿法具有全局收敛性。而牛顿法由于需要求 Hessian 矩阵的逆，计算量巨大，通过对 Hessian 矩阵估计的方法称为拟牛顿法，著名的拟牛顿法有DFP方法和BFGS方法。"><meta name="twitter:image" content="http://shellyandliu.github.io/img/post.jpg"><meta property="og:type" content="article"><meta property="og:title" content="牛顿法"><meta property="og:url" content="http://shellyandliu.github.io/2020/05/10/%E7%89%9B%E9%A1%BF%E6%B3%95/"><meta property="og:site_name" content="会飞的猪"><meta property="og:description" content="牛顿法是一种具有局部二阶收敛率的方法，在牛顿法的基础上增加一个学习步长的方法称为阻尼牛顿法，阻尼牛顿法具有全局收敛性。而牛顿法由于需要求 Hessian 矩阵的逆，计算量巨大，通过对 Hessian 矩阵估计的方法称为拟牛顿法，著名的拟牛顿法有DFP方法和BFGS方法。"><meta property="og:image" content="http://shellyandliu.github.io/img/post.jpg"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>var autoChangeMode = '1'
var t = Cookies.get("theme")
if (autoChangeMode == '1'){
  var isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
  var isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
  var isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined){
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport){
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour < 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
  }
  } else if (t == 'light') activateLightMode()
  else activateDarkMode()

} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://shellyandliu.github.io/2020/05/10/%E7%89%9B%E9%A1%BF%E6%B3%95/"><link rel="next" title="随机梯度下降法" href="http://shellyandliu.github.io/2020/05/09/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  highlightShrink: 'false',
  isFontAwesomeV5: false,
  isPhotoFigcaption: false
  
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isSidebar: true  
  }</script><noscript><style>
#page-header {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 4.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/longmao.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">11</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">12</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">2</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div></div></div><i class="fa fa-arrow-right on" id="toggle-sidebar" aria-hidden="true">     </i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#牛顿法"><span class="toc-number">1.</span> <span class="toc-text">牛顿法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#收敛性"><span class="toc-number">1.1.</span> <span class="toc-text">收敛性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#阻尼牛顿法"><span class="toc-number">2.</span> <span class="toc-text">阻尼牛顿法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#收敛性-1"><span class="toc-number">2.1.</span> <span class="toc-text">收敛性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#拟牛顿法"><span class="toc-number">3.</span> <span class="toc-text">拟牛顿法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SR1方法"><span class="toc-number">3.1.</span> <span class="toc-text">SR1方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DFP方法"><span class="toc-number">3.2.</span> <span class="toc-text">DFP方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BFGS方法"><span class="toc-number">3.3.</span> <span class="toc-text">BFGS方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Boryden-类算法"><span class="toc-number">3.4.</span> <span class="toc-text">Boryden 类算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DFP-算法和BFGS-算法的另一种推导"><span class="toc-number">3.5.</span> <span class="toc-text">DFP 算法和BFGS 算法的另一种推导</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考文献"><span class="toc-number">4.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div></div><div id="body-wrap"><div class="post-bg" id="nav" style="background-image: url(/img/post.jpg)"><div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">会飞的猪</a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 清单</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div></div><span class="toggle-menu close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span></span></div><div id="post-info"><div id="post-title"><div class="posttitle">牛顿法</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2020-05-10 18:15:26"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2020-05-10</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2020-05-11 16:06:55"><i class="fa fa-history" aria-hidden="true"></i> 更新于 2020-05-11</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/">-优化方法</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="fa fa-eye post-meta__icon" aria-hidden="true"> </i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"><span class="post-meta__separator">|</span><i class="post-meta__icon fa fa-comment-o" aria-hidden="true"></i><span>评论数:</span><a href="/2020/05/10/%E7%89%9B%E9%A1%BF%E6%B3%95/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/2020/05/10/%E7%89%9B%E9%A1%BF%E6%B3%95/" itemprop="commentCount"></span></a></span></div></div></div></div><main class="layout_post" id="content-inner"><article id="post"><div id="article-container"><h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>在梯度下降法中，我们选择的下降方向为 $\nabla f$ , 即 $f$ 的梯度，实际上该算法仅考虑了函数在该点处的下降速度，并没有考虑在下一点处下降速度是增加还是减少。牛顿法则是同时考虑了函数 $f$ 的二阶导数。</p>
<p>牛顿法的具体算法为：</p>
<blockquote>
<p>(1). 选取初始点 $x^{(1)}$。</p>
<p>(2). 在第 $k$ 步迭代时，计算 $\nabla^2f(x^{(k)})$ 和 $\nabla f(x^{(k)})$ .</p>
<p>(3). 置 $x^{(k+1)}=x^{(k)}-(\nabla^2f(x^{(k)}))^{-1}\nabla f(x^{(k)})$.</p>
<p>(4). 直到满足停止准则： $\left|\nabla f\left(x^{(k)}\right)\right|_{2} \leqslant \eta$, 其中 $\eta$ 是预先给定的阈值。</p>
</blockquote>
<p>其中 $(\nabla^2f(x^{(k)}))$ 被称为 Hessian 矩阵，需要它可逆。$\nabla^2f(x^{(k)}))^{-1}\nabla f(x^{(k)})$ 为牛顿法的下降方向，称为牛顿步径。</p>
<p>下面给出为什么取这个下降方向的解释：</p>
<p>考虑在 $x$ 处对 $f$ 进行二阶 Talor 展开：</p>
<blockquote>
<script type="math/tex; mode=display">
f(x+v)\approx f(x)+\nabla f(x)^T v+\frac12v^T\nabla^2f(x)v\tag{1}</script></blockquote>
<p>这是关于 $v$ 的二次函数，在 $v=-(\nabla^2f(x))^{-1}\nabla f(x)$ 处函数达到最小。则若 $f$ 时二次的，那么 $x-(\nabla^2f(x))^{-1}\nabla f(x)$ 是 $f$ 的精确最优解。若 $f$ 是近似二次的，直观上 $x-(\nabla^2f(x))^{-1}\nabla f(x)$ 应该是 $f$ 的最优解的一个很好的近似估计。并且由于 $f(x+v)$ 的二阶展开在 $v=-(\nabla^2f(x))^{-1}\nabla f(x)$ 处取最小，因此，相较于梯度下降法，将提供更加迅速的下降。</p>
<p>另一方面，考虑梯度下降法在强凸条件下的收敛性：</p>
<p> 即 $f$ 在经历最多 $\frac{\log((f(x^{(0)})-p^*)/\epsilon)}{\log(1/c)}$ 次迭代，一定可以得到 $f(x^{(k)})-p^*\le \epsilon$.其中 $c=1-\frac mM$. 也就是说，当 $\frac mM$ 较小时，迭代次数受 $\log (1 / c)=-\log (1-m / M) \approx m / M$ 的影响，将随着 $\frac Mm$ 的增大，线性增长。当 $\frac Mm$ 过于巨大时，将使得迭代速度缓慢到难以进行。</p>
<p>若考虑对 $x$ 做一个坐标变换 $\bar{x}=P^{1/2}x$, 其中 $P$ 为对称正定矩阵。则最优化问题为优化目标函数 $\bar{f}(\bar{x}):=f(P^{-1/2}\bar{x})=f(x)$,此时 $\bar{f}$ 在最优点处的 Hessian 矩阵为： $P^{-1/2}\nabla^2 f(x^*)P^{-1/2}$. 若 $P$ 取为最优点处的 Hessian 矩阵的近似矩阵 $\hat{H}$ 来代替。则 $\bar{f}$ 在最优点处的 Hessian 矩阵近似为 $\hat{H}^{-1/2}\nabla^2\hat{H}^{-1/2}\approx I$。将大大题高迭代效率。而 $\bar{f}(\bar{x})$ 按梯度下降法导出的下降方向为 $-(\nabla^2f(x))^{-1}\nabla f(x)$。</p>
<p>为方便起见，下面将第 $k$ 次迭代的 Hessian 矩阵 $\nabla^2f(x^{(k)})$ 表示为 $H_k$.</p>
<h3 id="收敛性"><a href="#收敛性" class="headerlink" title="收敛性"></a>收敛性</h3><p>下面给出牛顿法的收敛性证明:</p>
<blockquote>
<p>定理1：若初始点 $x^{(1)}$在极值点 $x^*$ 附近，并且 $x^*$ 附近的 Hessian 矩阵正定，并且 Hessian 矩阵满足 Lipschitz 连续条件，即 $|\nabla ^2f(x)-\nabla^2 f(y)|_2\le L|x-y|_2 $，则牛顿法满足二阶收敛.</p>
</blockquote>
<p>证明如下：</p>
<p>由于 $x^{(k+1)}=x^{(k)}-H_k\nabla f(x^{(k)})$:</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}\left\|x_{k+1}-x^{*}\right\|_2 &=\left\|x_{k}-H_{k}^{-1} \nabla f(x^{(k)})-x^{*}\right\|_2 \\ &=\left\|H_{k}^{-1}\right\| _2*\left\|H_{k}\left(x_{k}-x^{*}\right)-\nabla f(x^{(k)})\right\|_2 \\ &=\left\|H_{k}^{-1}\right\| _2*\left\|H_{k}\left(x_{k}-x^{*}\right)-\nabla f(x^{(k)})+\nabla f\left(x^{*}\right)\right\| _2\end{aligned}\tag{2}</script></blockquote>
<p>又因为 $\nabla f(x^{(k)})-g\left(x^{*}\right)=\int_{0}^{1}\left(x^{(k)}-x^{*}\right) \nabla^2f\left(x^{*}+t\left(x^{(k)}-x^{*}\right)\right) d t$.</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}\left\|x^{(k+1)}-x^{*}\right\|_2 & \leq \left\|H_{k}^{-1}\right\| _2*\left\|H_{k}\left(x^{(k)}-x^{*}\right)-g_{k}+g\left(x^{*}\right)\right\|_2 \\ &=\left\|H_{k}^{-1}\right\| _2*\left\|H_{k}\left(x^{(k)}-x^{*}\right)-\int_{0}^{1}\left(x^{(k)}-x^{*}\right) \nabla^2 f\left(x^{*}+t\left(x^{(k)}-x^{*}\right)\right) d t\right\|_2 \\ &=\left\|H_{k}^{-1}\right\| _2*\left\|\int_{0}^{1}\left[H_{k}\left(x^{(k)}-x^{*}\right)-\left(x^{(k)}-x^{*}\right) \nabla^2 f\left(x^{*}+t\left(x^{(k)}-x^{*}\right)\right)\right] d t\right\|_2 \\ &=\left\|H_{k}^{-1}\right\| _2*\left\|x^{(k)}-x^{*}\right\|_2 *\left\|\int_{0}^{1} H_{k}-\nabla^2 f\left(x^{*}+t\left(x^{(k)}-x^{*}\right)\right) d t\right\|_2 \end{aligned}\tag{3}</script></blockquote>
<p>利用 Lipschitz 连续性，得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\left\|x^{(k+1)}-x^{*}\right\|_2 & \leq \left\|H_{k}^{-1}\right\| _2* L\left\|x^{(k)}-x^{*}\right\|_2^{2} \int_{0}^{1}(1-t) d t \\
&=\frac{1}{2} \left\|H_{k}^{-1}\right\| _2* L\left\|x^{(k)}-x^{*}\right\|^{2}_2
\end{aligned}\tag{4}</script></blockquote>
<p>显然，当 $k\to\infty$ 时， $H_k\to H^*$, 于是 $|H_k^{-1}|_2\le2|H^*|_2$. 最终：</p>
<blockquote>
<script type="math/tex; mode=display">
\left\|x^{(k+1)}-x^{*}\right\|_2\le L\|H^*\|_2*\left\|x^{(k)}-x^{*}\right\|^{2}_2\tag{5}</script></blockquote>
<p>证毕。</p>
<p>显然从上面证明可知，牛顿法不是整体收敛的，当初始点远离最优解时，牛顿方向不一定是下降方向，因此作为牛顿法的改进，引入阻尼牛顿法。</p>
<h2 id="阻尼牛顿法"><a href="#阻尼牛顿法" class="headerlink" title="阻尼牛顿法"></a>阻尼牛顿法</h2><p>相较于纯牛顿法，阻尼牛顿法在牛顿迭代过程中增加了迭代步长这样一个参数，而不是统一的取 $1$.</p>
<p>下面先给出牛顿减量的定义：</p>
<blockquote>
<p>$\lambda(x):=(\nabla f(x)^T\nabla^2f(x)^{-1}\nabla f(x))^{1/2}$</p>
</blockquote>
<p>令 $\hat{f}(y)=f(x)+\nabla f(x)^T(y-x)+\frac12(y-x)^T\nabla^2f(x)(y-x)$ 为 $f$ 在 $x$ 附近的二阶泰勒展开。则 $f(x)-\inf_y\hat f(y)=\frac12 \lambda(x)^2$.</p>
<p>因此牛顿减量是基于 $f$ 在 $x$ 处的二阶近似对 $f(x)-f(x^*)$ 做出的估计值，可以用来设计停止准则。</p>
<p>阻尼牛顿法的算法步骤：</p>
<blockquote>
<p>(1). 选取初始点 $x^{(1)}$。</p>
<p>(2). 在第 $k$ 步迭代时，计算 $\nabla^2f(x^{(k)})$ 和 $\nabla f(x^{(k)})$ .</p>
<p>(3). 确定迭代步长 $t_k$.</p>
<p>(4). 置 $x^{(k+1)}=x^{(k)}-t_k(\nabla^2f(x^{(k)}))^{-1}\nabla f(x^{(k)})$.</p>
<p>(5). 直到满足停止准则： $\lambda(x^{(k)})^2/2\le\epsilon$, 其中 $\epsilon$ 是预先给定的阈值。</p>
</blockquote>
<p>下面采用回溯直线算法确定步长 $t_k$ . 并给出在强凸条件下收敛性证明：</p>
<h3 id="收敛性-1"><a href="#收敛性-1" class="headerlink" title="收敛性"></a>收敛性</h3><blockquote>
<p>定理2：假设 $f$ 满足以下条件：</p>
<p>​    a.  $f$ 的梯度满足 Lipschitz 连续条件，即$|\nabla f(x)-\nabla f(y)|_2\le M|x-y|_2 $.</p>
<p>​    b. $f$ 是强凸的，即 $\nabla^2f(x)\ge mI$ 。</p>
<p>​    c. Hessian 矩阵满足 Lipschitz 连续条件，即 $|\nabla ^2f(x)-\nabla^2 f(y)|_2\le L|x-y|_2 $.</p>
<p>当满足以上条件时采用回溯直线的阻尼牛顿法将满足以下的收敛边界：</p>
<script type="math/tex; mode=display">
f\left(x^{(k)}\right)-f^{*} \leq\left\{\begin{array}{ll}
\left(f\left(x^{(0)}\right)-f^{*}\right)-\gamma k & \text { if } k \leq k_{0} \\
\frac{2 m^{3}}{L^{2}}\left(\frac{1}{2}\right)^{2^{k-k_{0}+1}} & \text { if } k>k_{0}
\end{array}\right.\tag{6}</script><p>其中 $\gamma=\alpha\beta\eta^2 m/M$, $\eta=\min\{1,3(1-2\alpha)\}m^2/L,$ $k_0$ 是直到 $|\nabla f(x^{(k_0+1)})|_2\le\eta$ 的迭代数。并且当 $k&gt;k_0$ 时，迭代步长 $t=1$.</p>
</blockquote>
<p>证明如下：</p>
<p>先证明第一阶段，假定 $|\nabla f(x)|_2\ge\eta$ ，由于条件 a ,可以得出 $\nabla^2f(x)\le MI$, 因此：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
f\left(x-t (\nabla^2f(x))^{-1}\nabla f(x)\right) & \leqslant f(x)+t \nabla f(x)^{T} (\nabla^2f(x))^{-1}\nabla f(x)+\frac{M\left\|(\nabla^2f(x))^{-1}\nabla f(x)\right\|_{2}^{2}}{2} t^{2} \\
& \leqslant f(x)-t \lambda(x)^{2}+\frac{M}{2 m} t^{2} \lambda(x)^{2}
\end{aligned}\tag{7}</script></blockquote>
<p>由于步长$\hat{t}=m/M$ 满足：</p>
<blockquote>
<script type="math/tex; mode=display">
f\left(x-\hat t (\nabla^2f(x))^{-1}\nabla f(x)\right) \le f(x)-\frac {m}{2M}\lambda (x)^2\le f(x)-\alpha\hat t\lambda(x)^2\tag{8}</script></blockquote>
<p>符合回溯直线搜索的退出条件，所以回溯直线搜索确定的步长满足 $t\ge \beta m/M$</p>
<p>则目标函数的减少量为：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
f\left(x- t (\nabla^2f(x))^{-1}\nabla f(x)\right) -f(x)&\le-\alpha t \lambda(x)^2\\
&\le-\alpha\beta\frac mM\lambda(x)^2\\
&\le-\alpha\beta\frac{m}{M^2}\|\nabla f(x)\|_2^2\\
&\le-\alpha\beta\eta^2\frac m{M^2}\\
&=-\gamma
\end{aligned}\tag{9}</script></blockquote>
<p>因此 $f\left(x^{(k)}\right)-f^{*} \leq\left(f\left(x^{(0)}\right)-f^{*}\right)-\gamma k\quad\text { if } k \leq k_{0} $. 得证</p>
<p>下面证第二阶段，假定 $|\nabla f(x)|_2&lt;\eta$. 首先说明，若 $\eta\le3(1-2\alpha)m^2/L$，回溯直线搜索的步长就是单位步长。</p>
<p>由 Hessian 矩阵的 Lipschitz 条件，对于 $t\ge 0$, 我们有 $|\nabla^2f(x-t (\nabla^2f(x))^{-1}\nabla f(x))-\nabla^2 f(x)|_2\le tL|(\nabla^2f(x))^{-1}\nabla f(x)|_2$.</p>
<p>若令 $\hat{f}(t)=f(x-t (\nabla^2f(x))^{-1}\nabla f(x))$, 则有：</p>
<blockquote>
<script type="math/tex; mode=display">
\|\hat{f}''(t)-\hat{f}''(0)\|_2\le tL\|t (\nabla^2f(x))^{-1}\nabla f(x)\|_2^3\tag{10}</script></blockquote>
<p>由于 $\hat{f}’’(0)=\lambda(x)^2$：</p>
<blockquote>
<script type="math/tex; mode=display">
\hat f''(t)\le\hat f''(0)+tL\|t (\nabla^2f(x))^{-1}\nabla f(x)\|_2^3\le\lambda(x)^2+t\frac{L}{m^{3/2}}\lambda(x)^3\tag{11}</script></blockquote>
<p>由于 $\hat f’’(0)=-\lambda(x)^2$, 对 (11) 积分可得：</p>
<blockquote>
<script type="math/tex; mode=display">
\hat{f}'(t)\le\hat f'(0)+t\lambda(x)^2+t^2\frac{L}{2m^{3/2}}\lambda(x)^3=-\lambda(x)^2+t\lambda(x)^2+t^2\frac L{2m^{3/2}}\lambda(x)^3\tag{12}</script></blockquote>
<p>再次积分得：</p>
<blockquote>
<script type="math/tex; mode=display">
\hat f(t)\le \hat f(0)-t\lambda(x)^2+t^2\frac 12\lambda(x)^2+t^3\frac{L}{6m^{3/2}}\lambda(x)^3\tag{13}</script></blockquote>
<p>在 (13) 中取 $t=1$ 得：</p>
<blockquote>
<script type="math/tex; mode=display">
f(x-(\nabla^2f(x))^{-1}\nabla f(x))\le f(x)-\frac 12 \lambda(x)^2+\frac L{6m^{3/2}}\lambda(x)^3\tag{14}</script></blockquote>
<p>由于 $|\nabla f(x)|_2\le \eta\le 3(1-2\alpha)m^2/L$, 由强凸性可知：</p>
<blockquote>
<script type="math/tex; mode=display">
\lambda(x)\le3(1-2\alpha)m^{3/2}/L\tag{15}</script></blockquote>
<p>则由 (14)(15) 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
f(x-(\nabla^2f(x))^{-1}\nabla f(x))&\le f(x)- \lambda(x)^2\left(\frac 12-\frac {L\lambda(x)}{6m^{3/2}}\right)\\
&\le f(x)-\alpha\lambda(x)^2\\
&=f(x)+\alpha\nabla f(x)^T(\nabla^2f(x))^{-1}\nabla f(x)
\end{aligned}\tag{16}</script></blockquote>
<p>该式表明 $t=1$ 满足回溯直线搜索得退出条件。</p>
<p>现在分析其收敛速度：令 $d(x)=(\nabla^2f(x))^{-1}\nabla f(x)$ , 应用 Hessian 矩阵得 Lipschitz 条件，我们有：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\left\|\nabla f\left(x-d(x)\right)\right\|_{2} &=\left\|\nabla f\left(x+d(x)\right)-\nabla f(x)-\nabla^{2} f(x) d(x)\right\|_{2} \\
&=\left\|\int_{0}^{1}\left(\nabla^{2} f\left(x+td(x)\right)-\nabla^{2} f(x)\right)d(x) d t\right\|_{2} \\
& \leqslant \frac{L}{2}\left\|d(x)\right\|_{2}^{2} \\
&=\frac{L}{2}\left\|\nabla^{2} f(x)^{-1} \nabla f(x)\right\|_{2}^{2} \\
& \leqslant \frac{L}{2 m^{2}}\|\nabla f(x)\|_{2}^{2}
\end{aligned}\tag{17}</script></blockquote>
<p>因此 $f\left(x^{(k)}\right)-f^{*} \leq\frac{2 m^{3}}{L^{2}}\left(\frac{1}{2}\right)^{2^{k-k_{0}+1}} \quad\text { if } k&gt;k_{0}$. 得证</p>
<p>因此由上述分析，拟牛顿法若要满足 $f\left(x^{(k)}\right)-f^{*}\le\epsilon$, 最多需要 $\frac{f\left(x^{(0)}\right)-f^{\star}}{\gamma}+\log \log \left(\epsilon_{0} / \epsilon\right)$ 次迭代，其中 $\epsilon_0=2m^3/L$。</p>
<p>上面通过增加学习步长，使得牛顿法得到了全局收敛性，但是相较于梯度下降法，牛顿法仍有它的缺点：</p>
<p>(1). 牛顿法在每次迭代需要用到 $O(n^2)$ 得内存来存储 Hessian 矩阵，而梯度下降法只用 $O(n)$。</p>
<p>(2). 牛顿法要求函数必须是强凸得，即 Hessian 矩阵得逆存在，而梯度下降法不需要。</p>
<p>(3). 牛顿法需要 $O(n^3)$ 来计算 Hessian 矩阵得逆。这在 $n$ 较大时，计算量是相当大得。</p>
<p>其中第 (2)(3) 条缺点，我们可以通过对 Hessian 矩阵近似来克服它。</p>
<h2 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h2><p>假设我们用矩阵 $B_k$ 来替代在 $x^{(k)}$ 处的 Hessian 矩阵，则迭代表达式为：</p>
<blockquote>
<script type="math/tex; mode=display">
x^{(k
+1)}=x^{(k)}-\alpha_kB_k^{-1}\nabla f(x^{(k)})\tag{18}</script></blockquote>
<p>由于是用 $B_k$ 来近似 $\nabla ^2f(x^{(k)})$. 因此怎样的 $B_k$ 才算合理的非常重要，因此给出下面定理</p>
<blockquote>
<p>定理3：假设 $f$ 是二次连续可微的，考虑迭代关系式为 (18) ，假设 $\alpha_k=1$, 若 $\{x^{(k)}\}$ 收敛到 $x^*$ 使得 $\nabla f(x^*)=0$, 且 $\nabla^2 f(x^*) $ 是正定的，那么 $\{x^{(k)}\}$ 将超过线性收敛的速度收敛，当且仅当 $\lim _{k \rightarrow \infty} \frac{\left|\left(B_{k}-\nabla^{2} f\left(x^{*}\right)\right) B_k^{-1}\nabla f(x^{(k)})\right|_2}{\left|B_k^{-1}\nabla f(x^{(k)})\right|_2}=0$</p>
</blockquote>
<p> 证明如下：令 $p_k=B_k^{-1}\nabla f(x^{(k)})$, $p_k^N=-(\nabla^2f(x^{(k)}))^{-1}\nabla f(x^{(k)})$. 则可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
p_k-p_k^N&=(\nabla^2f(x^{(k)}))^{-1}(\nabla^2f(x^{(k)})p_k+\nabla f(x^{(k)}))\\
&=(\nabla^2f(x^{(k)}))^{-1}(\nabla^2 f(x^{(k)})-B_k)p_k\\
&=O(\|(\nabla^2 f(x^{(k)})-B_k)p_k\|_2)\\
&=o(\|p_k\|_2)
\end{aligned}\tag{19}</script></blockquote>
<p>其中导数第二个等式是由于当 $x^{(k)}$ 充分靠近 $x^*$ 时，$|\nabla^2f(x^{(k)})|_2$ 有界。因此：</p>
<blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\left\|x^{(k+1)}-x^{*}\right\|_2 &=\left\|x^{(k)}+p_{k}-x^{*}\right\|_2 \\
&=\left\|x^{(k)}+p_{k}^{N}-x^{*}+p_{k}-p_{k}^{N}\right\| \\
& \leq\left\|x^{(k)}+p_{k}^{N}-x^{*}\right\|+\left\|p_{k}-p_{k}^{N}\right\| \\
&=O\left(\left\|x^{(k)}-x^{*}\right\|^{2}\right)+o\left(\left\|p_{k}\right\|\right) \\
&=O\left(\left\|x^{(k)}-x^{*}\right\|^{2}\right)+o\left(\left\|x_{k}-x^{*}\right\|\right) \\
&=o\left(\left\|x^{(k)}-x^{*}\right\|\right)
\end{aligned}\tag{20}</script></blockquote>
<p>证毕。</p>
<p>下面介绍如何确定 $B_k$ 。</p>
<p>令 $g_{k}=\nabla f(x^{(k)})$, $p_k=x^{(k+1)}-x^{(k)}$, $F_k=\nabla ^2f(x^{(k)})$ , $q_k=g_{k+1}-g_k$. 则由泰勒展开有：</p>
<blockquote>
<script type="math/tex; mode=display">
q_k=g_{k+1}-g_k\approx F_kp_k\tag{21}</script></blockquote>
<p>显然当优化函数为二次函数时，上式取等号。若希望用 $H_k$ 来近似 $F_k$，则它满足：$H_{k}q_k\approx p_k$. 因此我们可以取 $H_{k+1}$ 满足：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}q_k=p_k\tag{22}</script></blockquote>
<p>因此 $H_{k+1}$ 的不同选取方法，对应了不同的拟牛顿法。</p>
<h3 id="SR1方法"><a href="#SR1方法" class="headerlink" title="SR1方法"></a>SR1方法</h3><p>最简单的一种迭代方法是，考虑：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=H_k+E_k=H_k+a_kz_kz_k^T\tag{23}</script></blockquote>
<p>其中 $a_k$ 为常数，$z_k$ 为向量。并且只要初始选择的 $H_0$ 为对称矩阵，则 $H_{k+1}$ 为对称的。</p>
<p>将 (23) 代入 (22) 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
p_k=H_{k+1}q_k=H_kq_k+a_kz_kz_k^Tq_k\tag{24}</script></blockquote>
<p>与 $q_k$ 取内积得：</p>
<blockquote>
<script type="math/tex; mode=display">
q_k^Tp_k-q_k^TH_kq_k=a_k(z_k^Tq_k)^2\tag{25}</script></blockquote>
<p>另一方面，将 (24) 代入 (23) 得：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=H_k+\frac{(p_k-H_kq_k)(p_k-H_kq_k)^T}{a_k(z_k^Tq_k)^2}</script></blockquote>
<p>将 (25) 代入得：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=H_k+\frac{(p_k-H_kq_k)(p_k-H_kq_k)^T}{q_k^T(p_k-H_kq_k)}\tag{26}</script></blockquote>
<p>上面得到的 $H_{k+1}$ 不一定正定。且分母可能为很小，这将导致数值上的困难。</p>
<h3 id="DFP方法"><a href="#DFP方法" class="headerlink" title="DFP方法"></a>DFP方法</h3><p>上述的SR1方法的更新量为秩为 $1$ 的矩阵，现在考虑更新量为秩为2矩阵：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=H_k+a_ku_ku_k^T+b_kv_kv_k^T\tag{27}</script></blockquote>
<p>因此，将 (27) 代入 (22) 可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
p_k=H_{k+1}q_k=H_kq_k+a_ku_ku_k^Tq_k+b_kv_kv_k^Tq_k\tag{28}</script></blockquote>
<p>显然，这个时候，$u_k,v_k$ 不能够被唯一的确定。一个显然的选择是 $u_k=p_k,v_k=H_kq_k$. 取 使 $au_k^Tq_k=1,bv_k^Tq_k=-1$ 成立的 $a,b$.</p>
<p>因此：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=H_k+\frac{p_kp_k^T}{p_k^Tq_k}-\frac{H_kq_kq_k^TH_k}{q_k^TH_kq_k}\tag{29}</script></blockquote>
<p>使用上述对 $H_k$ 估计的方法称为DFP方法：</p>
<blockquote>
<p>(1). 选定初始的对称正定矩阵 $H_0$, 和初始迭代点 $x^{(0)}$.</p>
<p>(2). 令 $d_k=-H_kg_k$. 选取步长 $\alpha_k$ . 则可以得到 $x^{(k+1)}=x^{(k)}+\alpha_kd_k$ .</p>
<p>(3). 按照 (29) 迭代得到 $H_{k+1}$.</p>
<p>(4). 重复上述步骤直到满足停止条件。</p>
</blockquote>
<p>给出下列定理：</p>
<blockquote>
<p>定理4：若 $p_k^Tq_k&gt;0$, 对任意的 $k$ 成立，则 DFP 方法得到的 $H_{k}$ 矩阵为正定矩阵。特别的，若步长 $\alpha_k$ 是通过精确一维线性搜索得到的，即 $\alpha_k=\arg\min_\alpha f(x^{(k)}+\alpha d_k)$. 则 DFP 方法得到的 $H_k$ 为正定矩阵。</p>
</blockquote>
<p>证明如下：</p>
<p>由 (29) 式，对任意 $x\in R^n$, 我们有：</p>
<blockquote>
<script type="math/tex; mode=display">
x^TH_{k+1}x=x^TH_kx+\frac{(x^Tp_k)^2}{p_k^Tq_k}-\frac{(x^TH_kq_k)^2}{q_k^TH_kq_k}\tag{30}</script></blockquote>
<p>定义 $a=H_k^{1/2}x,b=H_k^{1/2}q_k$, 可以将 (30) 重写为：</p>
<blockquote>
<script type="math/tex; mode=display">
x^TH_{k+1}x=\frac{(a^Ta)(b^Tb)-(a^Tb)^2}{(b^Tb)}+\frac{(x^Tp_k)^2}{p_k^Tq_k}\tag{31}</script></blockquote>
<p>由于 $p_k^Tq_k&gt;0$， 则右端的第二个式子大于 $0$, 又因为柯西不等式，则右端的第一个式子大于 $0$. </p>
<p>因此 $H_{k+1}$ 正定。</p>
<p>若 $\alpha_k$ 是通过精确一维线搜索得到的，则 $p_k^Tg_{k+1}=0$ , 因此 $p_k^Tq_k=p^T_kg_{k+1}-p^T_kg_k=-p_k^Tg_k=\alpha_kg_k^TH_kg_k$.</p>
<p>由于 $H_k$ 正定，所以 $p^T_kq_k&gt;0$ .</p>
<p>证毕。</p>
<h3 id="BFGS方法"><a href="#BFGS方法" class="headerlink" title="BFGS方法"></a>BFGS方法</h3><p>类似于 DFP 方法，只是这里不再对 Hessian 矩阵的逆直接进行估计，而是对 Hessian 矩阵进行估计，即 </p>
<blockquote>
<script type="math/tex; mode=display">
q_k=B_{k+1}p_k\tag{32}</script></blockquote>
<p>运用和 DFP 方法类似的分析，可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
B_{k+1}=B_k+\frac{q_kq_k^T}{q_k^Tp_k}-\frac{B_kp_kp_k^TB_k}{p_k^TB_kp_k}\tag{33}</script></blockquote>
<p>对 $B_{k+1}$ 两次运用 Sherman-Morrison 公式，$\left[A+ab^{T}\right]^{-1}=A^{-1}-\frac{A^{-1}ab^{T} A^{-1}}{1+b^{T} A^{-1} a}$ ，可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}^{\mathrm{BFGS}}=H_{k}+\left(\frac{1+q_{k}^{T} H_{k} q_{k}}{q_{k}^{T} q_{k}}\right) \frac{p_{k} p_{k}^{T}}{p_{k}^{T} q_{k}}-\frac{p_{k}q_{k}^{T}H_{k}+H_{k}q_{k} q_{k}^{T}}{q_{k}^{T} p_{k}}\tag{34}</script></blockquote>
<h3 id="Boryden-类算法"><a href="#Boryden-类算法" class="headerlink" title="Boryden 类算法"></a>Boryden 类算法</h3><p>我们记 $H_{k+1}^{\text{DFP}}$ 为 DFP 算法的迭代公式得到的，记 $H_{k+1}^{\text{BFGS}}$ 为 DFP 算法的迭代公式得到的。由于它们都满足 (22) 式。则它们的线性组合满足，我们将</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}^{\alpha}=(1-\alpha) H^{\text{DFP}}_{k+1}+\alpha H_{k+1}^{\text{BFGS}}\tag{35}</script></blockquote>
<p>称为 Boryden 类算法。</p>
<p>将 (35) 展开可以得到：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}^{\alpha}=H_{k}+\frac{p_{k} p_{k}^{T}}{p_{k}^{T} q_{k}}-\frac{H_{k} q_{k} q_{k}^{T} H_{k}}{q_{k}^{T} H_{k} q_{k}}+\alpha v_{k} v_{k}^{T}=H_{k+1}^{\mathrm{DFP}}+\alpha v_{k}v_{k}^{T}\tag{36}</script></blockquote>
<p>其中</p>
<blockquote>
<script type="math/tex; mode=display">
v_{k}=\left(q_{k}^{T} H_{k} q_{k}\right)^{1 / 2}\left(\frac{p_{k}}{p_{k}^{T} q_{k}}-\frac{H_{k} q_{k}}{q_{k}^{T} H_{k} q_{k}}\right)</script></blockquote>
<p>当 $\alpha=1$ 时，就是 BFGS 算法，当 $\alpha=0$ 时，就是 DFP 算法。当 $\alpha=\frac{q_k^Tp_k}{q_k^Tp_k-q_k^TH_kq_k}$ 时，就是 SR1 算法。</p>
<h3 id="DFP-算法和BFGS-算法的另一种推导"><a href="#DFP-算法和BFGS-算法的另一种推导" class="headerlink" title="DFP 算法和BFGS 算法的另一种推导"></a>DFP 算法和BFGS 算法的另一种推导</h3><p>再来看我们的目标，我们希望通过 (22) 来计算 $H_k$ , 但是由于 (22) 式只有 $n$ 个方程，而对称矩阵 $H_{k+1}$ 有 $n(n+1)/2$ 个自由度，因此会有无数个矩阵满足 (22) , 即便我们要求 $H_{k+1}$ 正定，也只能额外提供 $n$ 个自由度，仍远远不够，因此我们希望再选择 $H_{k+1}$ 时，与上次迭代的 $H_k$ 尽量接近。即在某种范数的意义下：</p>
<blockquote>
<script type="math/tex; mode=display">
H_{k+1}=\min_H\|H-H_k\|\tag{37}</script></blockquote>
<p>这里的范数选取很关键，不同的范数可以诱导出不同的拟牛顿法。</p>
<p>当我们把范数选取为加权欧式范数时，就可以得到 BFGS 算法。</p>
<p>其中加权欧式范数指的是 $|A|_W^2:=|W^{1/2}AW^{1/2}|_E^2=tr(WA^TWA)$.</p>
<blockquote>
<p>定理5：把 BFGS 算法的 (34) 写为 $H_{k+1}=H_k+E$, 其中 $H_k$ 对称，则 $E$ 等价于下列等式约束问题：</p>
<p>$\min_E|E|_W$</p>
<p>使得 $E^T=E,Eq_k=\gamma_k$</p>
<p>其中 $\gamma_k=p_k-H_kq_k$, $W$ 为满足 $Wp_k=q_k$ 的任意矩阵。</p>
</blockquote>
<p>证明如下：</p>
<p>该问题是一个凸优化问题，因此我们仅需找 $E$ 的一阶条件即可。</p>
<p>设 Lagrangian 函数为：</p>
<blockquote>
<script type="math/tex; mode=display">
\mathcal L=\frac 14 tr(WE^TWE)+tr(\Lambda^T(E^T-E))-\lambda^TW(Eq_k-\gamma_k)\tag{38}</script></blockquote>
<p>其中 $\Lambda$ 是 $E^T=E$ 的拉格朗日乘子， $\lambda^TW$ 是 $Eq_k=\gamma_k$ 的拉格朗日乘子。</p>
<p>由于 $\partial E/\partial E_{ij} = e_ie_j^T$ , 则：</p>
<blockquote>
<script type="math/tex; mode=display">
\partial\mathcal L/\partial E_{ij}=\frac 14 (tr(We_ie_j^TWE)+tr(WE^TWe_ie_j^T))+tr(\Lambda(e_je_i^T-e_ie_j^T))-\lambda^TWe_ie_j^Tq_k=0\tag{39}</script></blockquote>
<p>运用对称性和秩的交换性：</p>
<blockquote>
<script type="math/tex; mode=display">
\frac12[WEW]_{ij}+\Lambda_{ij}-\Lambda_{ji}=[W\lambda q_k^T]\tag{40}</script></blockquote>
<p>因此：</p>
<blockquote>
<script type="math/tex; mode=display">
WEW=W\lambda q_k^T+q_k\lambda^TW\tag{41}</script></blockquote>
<p>由于 $Wp_k=q_k$，并且 $W$ 非奇异：</p>
<blockquote>
<script type="math/tex; mode=display">
E=q_kp_k^T+p_k\lambda^T\tag{42}</script></blockquote>
<p>由于 $E^T=E,Eq_k=\gamma_k$ , 代入 (42) 得：</p>
<blockquote>
<script type="math/tex; mode=display">
\lambda=\frac{\gamma_k-p_k\lambda^Tq_k}{p_k^Tq_k}\tag{43}</script></blockquote>
<p>又因为 $\lambda^Tq_k=\frac{1/2q_k^T\gamma_k}{p_kq_k}$ ，代入 (43) , 并结合 $\gamma_k=p_k-H_kq_k$ 得：</p>
<blockquote>
<script type="math/tex; mode=display">
\lambda=\frac{(H_kq_k-\frac12p_k(q_k^TH_kq_k/p_k^Tq_k))}{p_k^Tq_k}\tag{44}</script></blockquote>
<p>将 (44) 代入 (42) 就得到 BFGS 得表达式。证毕。</p>
<p>同样的，若将 $H_k$ 用 $B_k$ 替代，则将得到 DFP 方法。</p>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Nocedal, Jorge, and Stephen Wright. <em>Numerical optimization</em>. Springer Science &amp; Business Media, 2006.</p>
<p>[2] Boyd, Stephen, Stephen P. Boyd, and Lieven Vandenberghe. <em>Convex optimization</em>. Cambridge university press, 2004.</p>
<p>[3] Luenberger, David G., and Yinyu Ye. <em>Linear and nonlinear programming</em>. Vol. 2. Reading, MA: Addison-wesley, 1984.</p>
<p>[4] Fletcher, Roger. <em>Practical methods of optimization</em>. John Wiley &amp; Sons, 2013.</p>
<p>[5] Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. <em>Deep learning</em>. MIT press, 2016.</p>
<p>[6] 李航. 统计学习方法. 清华大学出版社，2019</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">会飞的猪</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://shellyandliu.github.io/2020/05/10/%E7%89%9B%E9%A1%BF%E6%B3%95/">http://shellyandliu.github.io/2020/05/10/%E7%89%9B%E9%A1%BF%E6%B3%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://shellyandliu.github.io" target="_blank">会飞的猪</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E7%BB%9F%E8%AE%A1/">统计</a><a class="post-meta__tags" href="/tags/%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/">优化方法</a><a class="post-meta__tags" href="/tags/%E4%BA%8C%E9%98%B6%E6%94%B6%E6%95%9B/">二阶收敛</a><a class="post-meta__tags" href="/tags/%E7%89%9B%E9%A1%BF%E6%B3%95/">牛顿法</a><a class="post-meta__tags" href="/tags/%E9%98%BB%E5%B0%BC%E7%89%9B%E9%A1%BF%E6%B3%95/">阻尼牛顿法</a><a class="post-meta__tags" href="/tags/%E6%8B%9F%E7%89%9B%E9%A1%BF/">拟牛顿</a></div><div class="post_share"><div class="social-share" data-image="/img/post.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/weixin.png" alt="微信打赏"/><div class="post-qr-code__desc">微信打赏</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/zhifubao.png" alt="支付宝打赏"/><div class="post-qr-code__desc">支付宝打赏</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2020/05/09/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"><img class="next_cover lazyload" data-src="/img/post.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">随机梯度下降法</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/05/08/梯度下降/" title="梯度下降法"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-08</div><div class="relatedPosts_title">梯度下降法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/05/09/随机梯度下降法/" title="随机梯度下降法"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-05-09</div><div class="relatedPosts_title">随机梯度下降法</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/Lasso回归/" title="Lasso回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">Lasso回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/一元线性回归/" title="一元线性回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">一元线性回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/岭回归和广义岭回归/" title="岭回归和广义岭回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">岭回归和广义岭回归</div></div></a></div><div class="relatedPosts_item"><a href="/2020/04/16/主成分回归和偏最小二乘回归/" title="主成分回归和偏最小二乘回归"><img class="relatedPosts_cover lazyload"data-src="/img/post.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-04-16</div><div class="relatedPosts_title">主成分回归和偏最小二乘回归</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify: false,
  verify: false,
  appId: 'bCvyxhtL2V59mmqAp81lAGob-gzGzoHsz',
  appKey: 'J4zVKQfUjhToGJwBfLcoOHOv',
  placeholder: '留下邮箱可以收到回复提醒哦~~',
  avatar: 'monsterid',
  meta: guest_info,
  pageSize: '10',
  lang: 'zh-cn',
  recordIP: false,
  serverURLs: ''
});</script></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By 会飞的猪</div><div class="framework-info"><span>驱动 </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">欢迎来到我的博客</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">简</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>